{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#openmcmc","title":"openMCMC","text":"<p>openMCMC is a package for constructing Bayesian models from distributional components, and then doing parameter  estimation using Markov Chain Monte Carlo (MCMC) methods. The package supports a number of standard distributions used  in Bayesian modelling (e.g. Normal, gamma, uniform), and a number of simple functional forms for the parameters of  these distributions. For a model constructed in the toolbox, a number of different MCMC algorithms are available,  including simple random walk Metropolis-Hastings, manifold MALA, exact samplers for conjugate distribution choices,  and reversible-jump MCMC for parameters with an unknown dimensionality.</p>"},{"location":"#installing-openmcmc-as-a-package","title":"Installing openMCMC as a package","text":"<p>Suppose you want to use this openMCMC package in a different project. You can install it from PyPi through pip  <code>pip install openmcmc</code>. Or you could clone the repository and install it from the source code. After activating the environment you want to  install openMCMC in, open a terminal, move to the main openMCMC folder where pyproject.toml is located and run <code>pip install .</code>, optionally you can pass the <code>-e</code> flag is for editable mode. All the main options, info and settings for the package are found in the pyproject.toml file which sits in this repo as well.</p>"},{"location":"#examples","title":"Examples","text":"<p>For some examples on how to use this package please check out these Examples</p>"},{"location":"#contribution","title":"Contribution","text":"<p>This project welcomes contributions and suggestions. If you have a suggestion that would make this better you can  simply open an issue with a relevant title. Don't forget to give the project a star! Thanks again!</p> <p>For more details on contributing to this repository, see the Contributing guide.</p>"},{"location":"#licensing","title":"Licensing","text":"<p>Distributed under the Apache License Version 2.0. See the license file for more information.</p>"},{"location":"openmcmc/gmrf/","title":"GMRF","text":""},{"location":"openmcmc/gmrf/#gmrf","title":"GMRF","text":"<p>Reference: Rue, Held 2005 Gaussian Markov Random Fields</p> <p>Helper functions for sampling and dealing with Multivariate normal distributions defined by precision matrices which avoid the need for direct inversion and efficiently reuse cholesky factorisations with sparse implementations</p> <p>Notation: b: conditional mean Q: precision matrix L: lower triangle cholesky factorisation of a precision matrix Q</p>"},{"location":"openmcmc/gmrf/#openmcmc.gmrf.sample_normal","title":"<code>sample_normal(mu, Q=None, L=None, n=1)</code>","text":"<p>Generate multivariate random variables from a precision matrix Q using lower cholesky factorisation to get L.</p> <p>Note: sparse_linalg.spsolve_triangular compared to sparse_linalg.spsolve, and it appears to be much slower. Algorithm 2.4 from Rue, Held 2005 Gaussian Markov Random Fields  Sampling x ~ N(mu , Q^-1) 1: Compute the lower Cholesky factorisation, Q = L @ L' 2: Sample z ~ N(0,  I) 3: Solve L' v = z 4: Compute x = z + v 5: Return x Args:     mu (np.array): p x 1 mean     Q (np.array, optional): p x p for precision matrix. Defaults to None.     L (np.array, optional): p x p for lower triangular cholesky factorisation of                                  precision matrix. Defaults to None.     n (int, optional): number of samples. Defaults to 1.</p> <p>Returns:</p> Type Description <code>array</code> <p>p x n random normal values</p> Source code in <code>src/openmcmc/gmrf.py</code> <pre><code>def sample_normal(\n    mu: np.ndarray, Q: Union[np.ndarray, sparse.csc_matrix] = None, L: np.ndarray = None, n: int = 1\n) -&gt; np.ndarray:\n    \"\"\"Generate multivariate random variables from a precision matrix Q using lower cholesky factorisation to get L.\n\n    Note: sparse_linalg.spsolve_triangular compared to sparse_linalg.spsolve, and\n    it appears to be much slower.\n    Algorithm 2.4 from Rue, Held 2005 Gaussian Markov Random Fields\n     Sampling x ~ N(mu , Q^-1)\n    1: Compute the lower Cholesky factorisation, Q = L @ L'\n    2: Sample z ~ N(0,  I)\n    3: Solve L' v = z\n    4: Compute x = z + v\n    5: Return x\n    Args:\n        mu (np.array): p x 1 mean\n        Q (np.array, optional): p x p for precision matrix. Defaults to None.\n        L (np.array, optional): p x p for lower triangular cholesky factorisation of\n                                     precision matrix. Defaults to None.\n        n (int, optional): number of samples. Defaults to 1.\n\n    Returns:\n        (np.array): p x n random normal values\n\n    \"\"\"\n    size = [np.size(mu), n]\n\n    z = np.random.standard_normal(size=size)\n\n    if L is None:\n        L = cholesky(Q)\n\n    return solve(L.T, z).reshape(z.shape) + mu\n</code></pre>"},{"location":"openmcmc/gmrf/#openmcmc.gmrf.sample_truncated_normal","title":"<code>sample_truncated_normal(mu, Q=None, L=None, lower=None, upper=None, n=1, method='Gibbs')</code>","text":"<p>Sample from multivariate truncated normal using either rejection sampling or Gibbs sampling.</p> <p>Gibbs sampling should be faster but is generated through a markov chain so samples may not be completely independent The Markov chain is set up for sampling from gibbs_canonical_truncated_normal which is thinned by every 10 observations to get more i.i.d. samples</p> <p>Rejection sampling will work well for low dimensions and low amounts of truncated but will scale very poorly.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>array</code> <p>p x 1 mean</p> required <code>Q</code> <code>array</code> <p>p x p for precision matrix. Defaults to None.</p> <code>None</code> <code>L</code> <code>array</code> <p>p x p for lower triangular cholesky factorisation of                          precision matrix. Defaults to None.</p> <code>None</code> <code>lower</code> <code>array</code> <p>lower bound</p> <code>None</code> <code>upper</code> <code>array</code> <p>upper bound</p> <code>None</code> <code>n</code> <code>int</code> <p>number of samples. Defaults to 1.</p> <code>1</code> <code>method</code> <code>str</code> <p>defines method to use for TN sampling Either 'Gibbs' or 'Rejection' Defaults to 'Gibbs'.</p> <code>'Gibbs'</code> <p>Returns:</p> Type Description <code>array</code> <p>p x n random truncated normal values</p> Source code in <code>src/openmcmc/gmrf.py</code> <pre><code>def sample_truncated_normal(\n    mu: np.ndarray,\n    Q: Union[np.ndarray, sparse.csc_matrix] = None,\n    L: np.ndarray = None,\n    lower: np.ndarray = None,\n    upper: np.ndarray = None,\n    n: np.array = 1,\n    method=\"Gibbs\",\n) -&gt; np.ndarray:\n    \"\"\"Sample from multivariate truncated normal using either rejection sampling or Gibbs sampling.\n\n    Gibbs sampling should be faster but is generated through a markov chain so samples may not be completely independent\n    The Markov chain is set up for sampling from gibbs_canonical_truncated_normal which is thinned by every 10\n    observations to get more i.i.d. samples\n\n    Rejection sampling will work well for low dimensions and low amounts of truncated but will scale very poorly.\n\n    Args:\n        mu (np.array): p x 1 mean\n        Q (np.array, optional): p x p for precision matrix. Defaults to None.\n        L (np.array, optional): p x p for lower triangular cholesky factorisation of\n                                     precision matrix. Defaults to None.\n        lower (np.array, optional): lower bound\n        upper (np.array, optional): upper bound\n        n (int, optional): number of samples. Defaults to 1.\n        method (str, optional): defines method to use for TN sampling Either 'Gibbs' or 'Rejection' Defaults to 'Gibbs'.\n\n    Returns:\n        (np.array): p x n random truncated normal values\n\n    \"\"\"\n    if method == \"Gibbs\":\n        d = mu.shape[0]\n        b = Q @ mu\n        Z = np.empty(shape=(d, n))\n        Z[:, 0] = sample_truncated_normal_rejection(mu=mu, Q=Q, L=L, lower=lower, upper=upper, n=1).flatten()\n        thin = 10\n        for i in range(n - 1):\n            x = Z[:, i].reshape(d, 1)\n            for _ in range(thin):\n                x = gibbs_canonical_truncated_normal(b=b, Q=Q, x=x, lower=lower, upper=upper)\n                Z[:, i + 1] = x.flatten()\n        return Z\n    if method == \"Rejection\":\n        return sample_truncated_normal_rejection(mu=mu, Q=Q, L=L, lower=lower, upper=upper, n=n)\n\n    raise TypeError(\"method should be either Gibbs or Rejection\")\n</code></pre>"},{"location":"openmcmc/gmrf/#openmcmc.gmrf.sample_truncated_normal_rejection","title":"<code>sample_truncated_normal_rejection(mu, Q=None, L=None, lower=None, upper=None, n=1)</code>","text":"<p>Sample from multivariate truncated normal using rejection sampling.</p> <p>Rejection sampling will work well for low dimensions and low amounts of truncated but will scale very poorly.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>array</code> <p>p x 1 mean</p> required <code>Q</code> <code>array</code> <p>p x p for precision matrix. Defaults to None.</p> <code>None</code> <code>L</code> <code>array</code> <p>p x p for lower triangular cholesky factorisation of                          precision matrix. Defaults to None.</p> <code>None</code> <code>lower</code> <code>array</code> <p>lower bound</p> <code>None</code> <code>upper</code> <code>array</code> <p>upper bound</p> <code>None</code> <code>n</code> <code>int</code> <p>number of samples. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>array</code> <p>p x n random truncated normal values</p> Source code in <code>src/openmcmc/gmrf.py</code> <pre><code>def sample_truncated_normal_rejection(\n    mu: np.ndarray,\n    Q: Union[np.ndarray, sparse.csc_matrix] = None,\n    L: np.array = None,\n    lower: np.ndarray = None,\n    upper: np.ndarray = None,\n    n: np.array = 1,\n) -&gt; np.array:\n    \"\"\"Sample from multivariate truncated normal using rejection sampling.\n\n    Rejection sampling will work well for low dimensions and low amounts of truncated but will scale very poorly.\n\n    Args:\n        mu (np.array): p x 1 mean\n        Q (np.array, optional): p x p for precision matrix. Defaults to None.\n        L (np.array, optional): p x p for lower triangular cholesky factorisation of\n                                     precision matrix. Defaults to None.\n        lower (np.array, optional): lower bound\n        upper (np.array, optional): upper bound\n        n (int, optional): number of samples. Defaults to 1.\n\n    Returns:\n        (np.array): p x n random truncated normal values\n\n    \"\"\"\n    if L is None:\n        L = cholesky(Q)\n\n    n_bad = n\n\n    if lower is None:\n        lower = -np.inf\n\n    if upper is None:\n        upper = np.inf\n\n    if np.any(lower &gt;= upper):\n        raise ValueError(\"Error lower bound must be strictly less than upper bound\")\n\n    samples = sample_normal(mu, L=L, n=n_bad)\n    ind_bad = np.any(np.bitwise_or(samples &lt; lower, samples &gt; upper), axis=0)\n    n_bad = np.sum(ind_bad)\n\n    while n_bad &gt; 0:\n        sample_temp = sample_normal(mu, L=L, n=n_bad)\n\n        samples[:, ind_bad] = sample_temp\n        ind_bad = np.any(np.bitwise_or(samples &lt; lower, samples &gt; upper), axis=0)\n\n        n_bad = np.sum(ind_bad)\n\n    return samples\n</code></pre>"},{"location":"openmcmc/gmrf/#openmcmc.gmrf.sample_normal_canonical","title":"<code>sample_normal_canonical(b, Q=None, L=None)</code>","text":"<p>Generate multivariate random variables canonical representation precision matrix using cholesky factorisation.</p> <p>Algorithm 2.5 from Rue, Held 2005 Gaussian Markov Random Fields: Sampling x ~ N( Q^-1 b, Q^-1)     1: Compute the Cholesky factorisation, Q = L @ L'     2: Solve L w = b     3: Solve L' mu = w     4: Sample z ~ N(0; I)     5: Solve L' v = z     6: Compute x = mu + v     7: Return x</p> <p>Steps 2 and 3 are done in the function cho_solve and the output is thus mu. Steps 4, 5 and 6 are the algorithm 2.5 implemented in the function sample_normal</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>ndarray</code> <p>p x 1 conditional mean</p> required <code>Q</code> <code>ndarray</code> <p>p x p for precision matrix. Defaults to None.</p> <code>None</code> <code>L</code> <code>ndarray</code> <p>p x p for lower triangular cholesky factorisation                         of precision matrix. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>p x 1 random normal values</p> Source code in <code>src/openmcmc/gmrf.py</code> <pre><code>def sample_normal_canonical(b: np.ndarray, Q: np.ndarray = None, L: np.ndarray = None) -&gt; np.ndarray:\n    \"\"\"Generate multivariate random variables canonical representation precision matrix using cholesky factorisation.\n\n    Algorithm 2.5 from Rue, Held 2005 Gaussian Markov Random Fields:\n    Sampling x ~ N( Q^-1 b, Q^-1)\n        1: Compute the Cholesky factorisation, Q = L @ L'\n        2: Solve L w = b\n        3: Solve L' mu = w\n        4: Sample z ~ N(0; I)\n        5: Solve L' v = z\n        6: Compute x = mu + v\n        7: Return x\n\n    Steps 2 and 3 are done in the function cho_solve and the output is thus mu.\n    Steps 4, 5 and 6 are the algorithm 2.5 implemented in the function sample_normal\n\n    Args:\n        b (np.ndarray): p x 1 conditional mean\n        Q (np.ndarray, optional): p x p for precision matrix. Defaults to None.\n        L (np.ndarray, optional): p x p for lower triangular cholesky factorisation\n                                    of precision matrix. Defaults to None.\n\n    Returns:\n        (np.ndarray): p x 1 random normal values\n\n    \"\"\"\n    if L is None:\n        L = sparse_cholesky(Q)\n\n    mu = cho_solve((L, True), b).reshape(b.shape)\n\n    return sample_normal(mu, L=L)\n</code></pre>"},{"location":"openmcmc/gmrf/#openmcmc.gmrf.gibbs_canonical_truncated_normal","title":"<code>gibbs_canonical_truncated_normal(b, Q, x, lower=-np.inf, upper=np.inf)</code>","text":"<p>Generate truncated multivariate random variables from a precision matrix Q using lower cholesky factorisation to get L based on current state x using Gibbs sampling.</p> <p>subject to linear inequality constraints lower &lt; X &lt; upper</p> <p>Lemma 2.1 from Rue, Held 2005 Gaussian Markov Random Fields  Sampling x ~ N_c( Q^-1 b , Q^-1)  x_a | x_b ~ N_c( b_a - Q_ab x_b, Q_aa)</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>array</code> <p>p x 1 mean</p> required <code>Q</code> <code>array</code> <p>p x p for precision matrix. Defaults to None.</p> required <code>x</code> <code>array</code> <p>p x 1 current state.</p> required <code>lower</code> <code>array</code> <p>p x 1 lower bound for each dimension</p> <code>-inf</code> <code>upper</code> <code>array</code> <p>p x 1 upper bound for each dimension</p> <code>inf</code> <p>Returns:</p> Type Description <code>array</code> <p>p x 1 random normal values</p> Source code in <code>src/openmcmc/gmrf.py</code> <pre><code>def gibbs_canonical_truncated_normal(\n    b: np.ndarray,\n    Q: Union[np.ndarray, sparse.csc_matrix],\n    x: np.ndarray,\n    lower: np.ndarray = -np.inf,\n    upper: np.ndarray = np.inf,\n) -&gt; np.ndarray:\n    \"\"\"Generate truncated multivariate random variables from a precision matrix Q using lower cholesky factorisation to get L based on current state x using Gibbs sampling.\n\n    subject to linear inequality constraints\n    lower &lt; X &lt; upper\n\n    Lemma 2.1 from Rue, Held 2005 Gaussian Markov Random Fields\n     Sampling x ~ N_c( Q^-1 b , Q^-1)\n     x_a | x_b ~ N_c( b_a - Q_ab x_b, Q_aa)\n\n    Args:\n        b (np.array): p x 1 mean\n        Q (np.array): p x p for precision matrix. Defaults to None.\n        x (np.array): p x 1 current state.\n        lower (np.array, optional): p x 1 lower bound for each dimension\n        upper (np.array, optional): p x 1 upper bound for each dimension\n\n    Returns:\n        (np.array): p x 1 random normal values\n\n    \"\"\"\n    if (lower == -np.inf or lower is None) and (upper == np.inf or upper is None):\n        return sample_normal_canonical(b, Q)\n\n    if lower is None:\n        lower = -np.inf\n    if upper is None:\n        upper = np.inf\n\n    p = np.size(x)\n    temp_limit = np.full(shape=(p, 1), fill_value=np.inf)\n    lower = np.maximum(lower, -temp_limit)\n    upper = np.minimum(upper, temp_limit)\n\n    if p == 1:\n        if sparse.issparse(Q):\n            Q = Q.toarray()\n        return np.array(truncated_normal_rv(mean=b / Q, scale=1 / np.sqrt(Q), lower=lower, upper=upper), ndmin=2)\n\n    if sparse.issparse(Q):\n        Q_diag = Q.diagonal()\n    else:\n        Q_diag = np.diag(Q)\n\n    for i in range(p):\n        Q_ii = Q_diag[i]\n        v_i = 1 / Q_ii\n        scale_i = np.sqrt(v_i)\n\n        if sparse.issparse(Q):\n            cond_mean_i = v_i * (b[i] - Q.getrow(i) @ x + Q_ii * x[i])\n        else:\n            cond_mean_i = v_i * (b[i] - Q[i, :] @ x + Q_ii * x[i])\n\n        x[i] = truncated_normal_rv(mean=cond_mean_i, scale=scale_i, lower=lower[i], upper=upper[i])\n\n    return x\n</code></pre>"},{"location":"openmcmc/gmrf/#openmcmc.gmrf.truncated_normal_rv","title":"<code>truncated_normal_rv(mean, scale, lower, upper, size=1)</code>","text":"<p>Wrapper for scipy.stats.truncnorm.rvs handles cases a, b not standard form.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>array</code> <p>p x 1 mean for each dimension</p> required <code>scale</code> <code>array</code> <p>p x 1 standard deviation for each dimension</p> required <code>lower</code> <code>array</code> <p>p x 1 lower bound for each dimension</p> required <code>upper</code> <code>array</code> <p>p x 1 upper bound for each dimension</p> required <code>size</code> <code>int</code> <p>size of output array default = 1</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>size x 1  truncated normal samples</p> Source code in <code>src/openmcmc/gmrf.py</code> <pre><code>def truncated_normal_rv(\n    mean: np.ndarray, scale: np.ndarray, lower: np.ndarray, upper: np.ndarray, size=1\n) -&gt; np.ndarray:\n    \"\"\"Wrapper for scipy.stats.truncnorm.rvs handles cases a, b not standard form.\n\n    Args:\n        mean (np.array): p x 1 mean for each dimension\n        scale (np.array): p x 1 standard deviation for each dimension\n        lower (np.array): p x 1 lower bound for each dimension\n        upper (np.array): p x 1 upper bound for each dimension\n        size (int): size of output array default = 1\n\n    Returns:\n        (np.ndarray): size x 1  truncated normal samples\n\n    \"\"\"\n    if lower is None:\n        lower = -np.inf\n\n    if upper is None:\n        upper = np.inf\n\n    a, b = (lower - mean) / scale, (upper - mean) / scale\n    return truncnorm.rvs(a, b, loc=mean, scale=scale, size=size)\n</code></pre>"},{"location":"openmcmc/gmrf/#openmcmc.gmrf.truncated_normal_log_pdf","title":"<code>truncated_normal_log_pdf(x, mean, scale, lower, upper)</code>","text":"<p>Wrapper for scipy.stats.truncnorm.logpdf handles cases a, b not standard form.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>values</p> required <code>mean</code> <code>ndarray</code> <p>mean</p> required <code>scale</code> <code>ndarray</code> <p>standard deviation</p> required <code>lower</code> <code>ndarray</code> <p>lower bound</p> required <code>upper</code> <code>ndarray</code> <p>upper bound</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>truncated normal sample</p> Source code in <code>src/openmcmc/gmrf.py</code> <pre><code>def truncated_normal_log_pdf(\n    x: np.ndarray, mean: np.ndarray, scale: np.ndarray, lower: np.ndarray, upper: np.ndarray\n) -&gt; np.ndarray:\n    \"\"\"Wrapper for scipy.stats.truncnorm.logpdf handles cases a, b not standard form.\n\n    Args:\n        x (np.ndarray): values\n        mean (np.ndarray): mean\n        scale (np.ndarray): standard deviation\n        lower (np.ndarray): lower bound\n        upper (np.ndarray): upper bound\n\n    Returns:\n        (np.ndarray): truncated normal sample\n\n    \"\"\"\n    if lower is None:\n        lower = -np.inf\n\n    if upper is None:\n        upper = np.inf\n\n    a, b = (lower - mean) / scale, (upper - mean) / scale\n    return truncnorm.logpdf(x, a, b, loc=mean, scale=scale)\n</code></pre>"},{"location":"openmcmc/gmrf/#openmcmc.gmrf.multivariate_normal_pdf","title":"<code>multivariate_normal_pdf(x, mu, Q, by_observation=False)</code>","text":"<p>Compute diagonalized log-pdf of a multivariate Gaussian distribution in terms of the precision matrix, can take sparse precision matrix inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>dim  x n value for the distribution response. where dim is the number of dimensions and n             is the number of observations</p> required <code>mu</code> <code>ndarray</code> <p>dim x 1 distribution mean vector.</p> required <code>Q</code> <code>(ndarray, csc_matrix)</code> <p>dim x dim distribution precision matrix can be sparse or np.array</p> required <code>by_observation</code> <code>bool</code> <p>indicates whether we should sum over observations default= False</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>log-pdf of the Gaussian distribution either:                         (1,) if by_observation = False or                         (n,) if by_observation = True</p> Source code in <code>src/openmcmc/gmrf.py</code> <pre><code>def multivariate_normal_pdf(\n    x: np.ndarray, mu: np.ndarray, Q: Union[np.ndarray, sparse.csc_matrix], by_observation: bool = False\n) -&gt; Union[np.ndarray, float]:\n    \"\"\"Compute diagonalized log-pdf of a multivariate Gaussian distribution in terms of the precision matrix, can take sparse precision matrix inputs.\n\n    Args:\n        x (np.ndarray): dim  x n value for the distribution response. where dim is the number of dimensions and n\n                        is the number of observations\n        mu (np.ndarray): dim x 1 distribution mean vector.\n        Q (np.ndarray, sparse.csc_matrix): dim x dim distribution precision matrix can be sparse or np.array\n        by_observation (bool, optional): indicates whether we should sum over observations default= False\n\n    Returns:\n        (np.ndarray): log-pdf of the Gaussian distribution either:\n                                    (1,) if by_observation = False or\n                                    (n,) if by_observation = True\n\n    \"\"\"\n    L = cholesky(Q)\n    dim = L.shape[0]\n\n    log_det_precision = 2 * np.sum(np.log(L.diagonal()))\n    Q_residual = L.T @ (x - mu)\n    log_p = (1 / 2) * (log_det_precision - dim * np.log(2 * np.pi) - np.sum(np.power(Q_residual, 2), axis=0))\n\n    if not by_observation:\n        log_p = np.sum(log_p)\n    return log_p\n</code></pre>"},{"location":"openmcmc/gmrf/#openmcmc.gmrf.precision_temporal","title":"<code>precision_temporal(time, unit_length=1.0, is_sparse=True)</code>","text":"<p>Generate temporal difference penalty matrix.</p> <p>Details can be found on pages 97-99 of 'Gaussian Markov Random Fields' [Rue, Held 2005], 'The first-order random walk for irregular locations'.</p> <p>Converts time to number of seconds then call precision_irregular</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>DatetimeArray</code> <p>vector of times</p> required <code>unit_length</code> <code>float</code> <p>numbers seconds to define unit difference Defaults to 1 second</p> <code>1.0</code> <code>is_sparse</code> <code>bool</code> <p>Flag if generated as sparse. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>P</code> <code>Union[ndarray, csc_matrix]</code> <p>un-scaled precision matrix</p> Source code in <code>src/openmcmc/gmrf.py</code> <pre><code>def precision_temporal(\n    time: DatetimeArray, unit_length: float = 1.0, is_sparse: bool = True\n) -&gt; Union[np.ndarray, sparse.csc_matrix]:\n    \"\"\"Generate temporal difference penalty matrix.\n\n    Details can be found on pages 97-99 of 'Gaussian Markov Random Fields'\n    [Rue, Held 2005], 'The first-order random walk for irregular locations'.\n\n    Converts time to number of seconds then call precision_irregular\n\n    Args:\n        time (DatetimeArray): vector of times\n        unit_length (float, optional): numbers seconds to define unit difference Defaults to 1 second\n        is_sparse (bool, optional): Flag if generated as sparse. Defaults to True.\n\n    Returns:\n        P (Union[np.ndarray, sparse.csc_matrix]): un-scaled precision matrix\n\n    \"\"\"\n    s = (time - time.min()).total_seconds() / unit_length\n\n    return precision_irregular(s, is_sparse=is_sparse)\n</code></pre>"},{"location":"openmcmc/gmrf/#openmcmc.gmrf.precision_irregular","title":"<code>precision_irregular(s, is_sparse=True)</code>","text":"<p>Generate penalty matrix from irregular observations using first order random walk.</p> <p>Details can be found on pages 97-99 of 'Gaussian Markov Random Fields' [Rue, Held 2005], 'The first-order random walk for irregular locations'.</p> Diagonal and off-diagonal elements of the precision found as follows <p>1/del_{i-1} + 1/del_{i},    j = i,</p> <p>Q_{ij} =    -1/del_{i},                 j = i+1,             0,                          else. where del = [t_{i+1} - t_{i}]</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>ndarray</code> <p>vector of locations.</p> required <code>is_sparse</code> <code>bool</code> <p>Flag if generated as sparse. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>P</code> <code> Union[np.ndarray, sparse.csc_matrix]</code> <p>un-scaled precision matrix</p> Source code in <code>src/openmcmc/gmrf.py</code> <pre><code>def precision_irregular(s: np.ndarray, is_sparse: bool = True) -&gt; Union[np.ndarray, sparse.csc_matrix]:\n    \"\"\"Generate penalty matrix from irregular observations using first order random walk.\n\n    Details can be found on pages 97-99 of 'Gaussian Markov Random Fields'\n    [Rue, Held 2005], 'The first-order random walk for irregular locations'.\n\n    Diagonal and off-diagonal elements of the precision found as follows:\n                1/del_{i-1} + 1/del_{i},    j = i,\n    Q_{ij} =    -1/del_{i},                 j = i+1,\n                0,                          else.\n    where del = [t_{i+1} - t_{i}]\n\n    Args:\n        s (np.ndarray): vector of locations.\n        is_sparse (bool, optional): Flag if generated as sparse. Defaults to True.\n\n    Returns:\n        P ( Union[np.ndarray, sparse.csc_matrix]): un-scaled precision matrix\n\n    \"\"\"\n    if s.ndim &gt; 1:\n        s = np.squeeze(s)\n\n    if s.size &gt; 1:\n        delta_reciprocal = 1.0 / np.diff(s)\n\n        d_0 = np.append(\n            np.append(delta_reciprocal[0], delta_reciprocal[:-1] + delta_reciprocal[1:]), delta_reciprocal[-1]\n        )\n        if is_sparse:\n            P = sparse.diags(diagonals=(-delta_reciprocal, d_0, -delta_reciprocal), offsets=[-1, 0, 1], format=\"csc\")\n        else:\n            P = np.diag(d_0, k=0) - np.diag(delta_reciprocal, k=-1) - np.diag(delta_reciprocal, k=1)\n    else:\n        P = np.array(1, ndmin=2)\n\n    return P\n</code></pre>"},{"location":"openmcmc/gmrf/#openmcmc.gmrf.solve","title":"<code>solve(a, b)</code>","text":"<p>Solve a linear matrix equation, or system of linear scalar equations.</p> <p>Computes the \u201cexact\u201d solution, x, of the well-determined, i.e., full rank, linear matrix equation ax = b.</p> <p>If inputs are sparse calls scipy.linalg.spsolve else calls np.linalg.solve</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[ndarray, csc_matrix]</code> <p>description</p> required <code>b</code> <code>Union[ndarray, csc_matrix]</code> <p>description</p> required <p>Returns     Union(np.ndarray, sparse.csc_matrix) solution to the system in same format as input</p> Source code in <code>src/openmcmc/gmrf.py</code> <pre><code>def solve(\n    a: Union[np.ndarray, sparse.csc_matrix], b: Union[np.ndarray, sparse.csc_matrix]\n) -&gt; Union[np.ndarray, sparse.csc_matrix]:\n    \"\"\"Solve a linear matrix equation, or system of linear scalar equations.\n\n    Computes the \u201cexact\u201d solution, x, of the well-determined, i.e., full rank, linear matrix equation ax = b.\n\n    If inputs are sparse calls scipy.linalg.spsolve else calls np.linalg.solve\n\n    Args:\n        a (Union[np.ndarray, sparse.csc_matrix]): _description_\n        b (Union[np.ndarray, sparse.csc_matrix]): _description_\n\n    Returns\n        Union(np.ndarray, sparse.csc_matrix) solution to the system in same format as input\n\n    \"\"\"\n    if sparse.issparse(a) or sparse.issparse(b):\n        return sparse_linalg.spsolve(a, b)\n\n    return np.linalg.solve(a, b)\n</code></pre>"},{"location":"openmcmc/gmrf/#openmcmc.gmrf.cho_solve","title":"<code>cho_solve(c_and_lower, b)</code>","text":"<p>Solve the linear equations A x = b, given the Cholesky factorization of A.</p> <p>If inputs are sparse calls sparse solvers otherwise uses scipy.linalg.cho_solve</p> <p>Parameters:</p> Name Type Description Default <code>c_and_lower</code> <code> tuple(Union(np.ndarray, sparse.csc_matrix), bool</code> <p>Cholesky factorization of A         and flag for if it is a lower Cholesky</p> required <code>b</code> <code>Union(np.ndarray, sparse.csc_matrix</code> <p>Right-hand side</p> required <p>Returns     (Union(np.ndarray, sparse.csc_matrix)) The solution to the system A x = b</p> Source code in <code>src/openmcmc/gmrf.py</code> <pre><code>def cho_solve(c_and_lower: tuple, b: Union[np.ndarray, sparse.csc_matrix]) -&gt; Union[np.ndarray, sparse.csc_matrix]:\n    \"\"\"Solve the linear equations A x = b, given the Cholesky factorization of A.\n\n    If inputs are sparse calls sparse solvers otherwise uses scipy.linalg.cho_solve\n\n    Args:\n        c_and_lower ( tuple(Union(np.ndarray, sparse.csc_matrix), bool)): Cholesky factorization of A\n                    and flag for if it is a lower Cholesky\n        b (Union(np.ndarray, sparse.csc_matrix)): Right-hand side\n\n    Returns\n        (Union(np.ndarray, sparse.csc_matrix)) The solution to the system A x = b\n\n    \"\"\"\n    if sparse.issparse(c_and_lower[0]) or sparse.issparse(b):\n        if c_and_lower[1]:\n            L = c_and_lower[0]\n            U = c_and_lower[0].T\n        else:\n            L = c_and_lower[0].T\n            U = c_and_lower[0]\n\n        w = sparse_linalg.spsolve(L, b)\n        return sparse_linalg.spsolve(U, w)\n\n    return linalg.cho_solve(c_and_lower, b)\n</code></pre>"},{"location":"openmcmc/gmrf/#openmcmc.gmrf.cholesky","title":"<code>cholesky(Q, lower=True)</code>","text":"<p>Compute Cholesky factorization of input matrix.</p> <p>If it is sparse will use gmf.sparse_cholesky otherwise will use linalg.cholesky</p> <p>Parameters:</p> Name Type Description Default <code>Q</code> <code>Union[ndarray, csc_matrix]</code> <p>precision matrix, for factorization</p> required <code>lower</code> <code>bool</code> <p>flag for lower triangular matrix, default is true</p> <code>True</code> <p>Returns    (Union[np.ndarray, sparse.csc_matrix]: Cholesky factorization of the input in the same format as the input</p> Source code in <code>src/openmcmc/gmrf.py</code> <pre><code>def cholesky(Q: Union[np.ndarray, sparse.csc_matrix], lower: bool = True) -&gt; Union[np.ndarray, sparse.csc_matrix]:\n    \"\"\"Compute Cholesky factorization of input matrix.\n\n    If it is sparse will use gmf.sparse_cholesky otherwise will use linalg.cholesky\n\n    Args:\n        Q (Union[np.ndarray, sparse.csc_matrix]):  precision matrix, for factorization\n        lower (bool, optional): flag for lower triangular matrix, default is true\n\n    Returns\n       (Union[np.ndarray, sparse.csc_matrix]: Cholesky factorization of the input in the same format as the input\n\n    \"\"\"\n    if sparse.issparse(Q):\n        L = sparse_cholesky(Q)\n    else:\n        L = np.linalg.cholesky(Q)\n\n    if lower:\n        return L\n\n    return L.T\n</code></pre>"},{"location":"openmcmc/gmrf/#openmcmc.gmrf.sparse_cholesky","title":"<code>sparse_cholesky(Q)</code>","text":"<p>Compute sparse Cholesky factorization of input matrix.</p> <p>Uses the scipy.sparse functionality for LU decomposition, and converts to Cholesky factorization. Approach taken from: https://gist.github.com/omitakahiro/c49e5168d04438c5b20c921b928f1f5d</p> <p>If the sparse matrix is identified as unsuitable for Cholesky factorization, the function attempts to compute the Chol of the dense matrix instead.</p> <p>Parameters:</p> Name Type Description Default <code>Q</code> <code>csc_matrix</code> <p>sparse precision matrix, for factorization</p> required <p>Returns:</p> Type Description <code>csc_matrix</code> <p>Cholesky factorization of the input</p> Source code in <code>src/openmcmc/gmrf.py</code> <pre><code>def sparse_cholesky(Q: sparse.csc_matrix) -&gt; sparse.csc_matrix:\n    \"\"\"Compute sparse Cholesky factorization of input matrix.\n\n    Uses the scipy.sparse functionality for LU decomposition, and converts\n    to Cholesky factorization. Approach taken from:\n    https://gist.github.com/omitakahiro/c49e5168d04438c5b20c921b928f1f5d\n\n    If the sparse matrix is identified as unsuitable for Cholesky factorization,\n    the function attempts to compute the Chol of the dense matrix instead.\n\n    Args:\n        Q (sparse.csc_matrix): sparse precision matrix, for factorization\n\n    Returns:\n        (sparse.csc_matrix): Cholesky factorization of the input\n\n    \"\"\"\n    m = Q.shape[0]\n    n = Q.shape[1]\n    if m != n:\n        raise ValueError(\"Matrix is not square\")\n\n    if sparse.issparse(Q):\n        if not isinstance(Q, sparse.csc_matrix):\n            Q = Q.tocsc()\n        fact_lu = sparse_linalg.splu(Q, diag_pivot_thresh=0, options={\"RowPerm\": False, \"ColPerm\": False})\n        if (fact_lu.U.diagonal() &gt; 0).all():\n            return fact_lu.L.dot(sparse.diags(fact_lu.U.diagonal() ** 0.5))\n\n        return np.linalg.cholesky(Q.toarray())\n\n    return np.linalg.cholesky(Q)\n</code></pre>"},{"location":"openmcmc/mcmc/","title":"MCMC","text":""},{"location":"openmcmc/mcmc/#mcmc","title":"MCMC","text":"<p>Main MCMC class for mcmc setup.</p>"},{"location":"openmcmc/mcmc/#openmcmc.mcmc.MCMC","title":"<code>MCMC</code>  <code>dataclass</code>","text":"<p>Class for running Markov Chain Monte Carlo on a Model object to do parameter inference.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>initial state of sampler. Any parameters not specified will be sampled from prior distributions.</p> required <code>samplers</code> <code>list</code> <p>list of the samplers to be used for each parameter to be estimated.</p> required <code>n_burn</code> <code>int</code> <p>number of initial burn in these iterations are not stored, default 5000.</p> <code>5000</code> <code>n_iter</code> <code>int</code> <p>number of iterations which are stored in store, default 5000</p> <code>5000</code> <code>n_thin</code> <code>int</code> <p>number of iterations to thin by, default 1.</p> <code>1</code> <p>Attributes:</p> Name Type Description <code>state</code> <code>dict</code> <p>initial state of sampler any parameters not          specified will be sampler from prior distributions</p> <code>samplers</code> <code>list</code> <p>list of the samplers to be used for each parameter to be estimated.</p> <code>n_burn</code> <code>int</code> <p>number of initial burn in these iterations are not stored.</p> <code>n_iter</code> <code>int</code> <p>number of iterations which are stored in store.</p> <code>n_thin</code> <code>int</code> <p>number of iterations to thin by.</p> <code>store</code> <code>dict</code> <p>dictionary storing MCMC output as np.array for each inference parameter.</p> Source code in <code>src/openmcmc/mcmc.py</code> <pre><code>@dataclass\nclass MCMC:\n    \"\"\"Class for running Markov Chain Monte Carlo on a Model object to do parameter inference.\n\n    Args:\n        state (dict): initial state of sampler. Any parameters not specified will be sampled from prior distributions.\n        samplers (list): list of the samplers to be used for each parameter to be estimated.\n        n_burn (int, optional): number of initial burn in these iterations are not stored, default 5000.\n        n_iter (int, optional): number of iterations which are stored in store, default 5000\n        n_thin (int, optional): number of iterations to thin by, default 1.\n\n    Attributes:\n        state (dict): initial state of sampler any parameters not\n                     specified will be sampler from prior distributions\n        samplers (list): list of the samplers to be used for each parameter to be estimated.\n        n_burn (int): number of initial burn in these iterations are not stored.\n        n_iter (int): number of iterations which are stored in store.\n        n_thin (int): number of iterations to thin by.\n        store (dict): dictionary storing MCMC output as np.array for each inference parameter.\n\n    \"\"\"\n\n    state: dict\n    samplers: list[MCMCSampler]\n    model: Model\n    n_burn: int = 5000\n    n_iter: int = 5000\n    n_thin: int = 1\n    store: dict = field(default_factory=dict, init=False)\n\n    def __post_init__(self):\n        \"\"\"Convert any state values to at least 2D np.arrays and sample any missing states from the prior distributions and set up storage arrays for the sampled values.\n\n        Ensures that all elements of the initial state are in an appropriate format for running\n        the sampler:\n            - sparse matrices are left unchanged.\n            - all other data types are coerced (if possible) to np.ndarray.\n            - any scalars or existing np.ndarray with only one dimension are forced to be at\n                least 2D.\n\n        Also initialises an item in the storage dictionary for each of the sampled values,\n        for any data fitted values, and for the log-posterior value.\n\n        \"\"\"\n        self.state = copy(self.state)\n\n        for key, term in self.state.items():\n            if sparse.issparse(term):\n                continue\n\n            if not isinstance(term, np.ndarray):\n                term = np.array(term, ndmin=2, dtype=np.float64)\n                if np.shape(term)[0] == 1:\n                    term = term.T\n            elif term.ndim &lt; 2:\n                term = np.atleast_2d(term).T\n\n            self.state[key] = term\n\n        for sampler in self.samplers:\n            if sampler.param not in self.state:\n                self.state[sampler.param] = sampler.model[sampler.param].rvs(self.state)\n            self.store = sampler.init_store(current_state=self.state, store=self.store, n_iterations=self.n_iter)\n        if self.model.response is not None:\n            for response in self.model.response.keys():\n                self.store[response] = np.full(shape=(self.state[response].size, self.n_iter), fill_value=np.nan)\n        self.store[\"log_post\"] = np.full(shape=(self.n_iter, 1), fill_value=np.nan)\n\n    def run_mcmc(self):\n        \"\"\"Runs MCMC routine for the given model specification.\n\n        Numbers the iteratins of the sampler from -self.n_burn to self.n_iter, sotring every self.n_thin samples.\n\n        Runs a first loop over samplers, and generates a sample for all corresponding variables in the state. Then\n        stores the value of each of the sampled parameters in the self.store dictionary, as well as the data fitted\n        values and the log-posterior value.\n\n        \"\"\"\n        for i_it in tqdm(range(-self.n_burn, self.n_iter)):\n            for _ in range(self.n_thin):\n                for sampler in self.samplers:\n                    self.state = sampler.sample(self.state)\n\n            if i_it &lt; 0:\n                continue\n\n            for sampler in self.samplers:\n                self.store = sampler.store(current_state=self.state, store=self.store, iteration=i_it)\n\n            self.store[\"log_post\"][i_it] = self.model.log_p(self.state)\n            if self.model.response is not None:\n                for response, predictor in self.model.response.items():\n                    self.store[response][:, [i_it]] = getattr(self.model[response], predictor).predictor(self.state)\n\n        for sampler in self.samplers:\n            if isinstance(sampler, MetropolisHastings):\n                print(f\"{sampler.param}: {sampler.accept_rate.get_acceptance_rate()}\")\n</code></pre>"},{"location":"openmcmc/mcmc/#openmcmc.mcmc.MCMC.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Convert any state values to at least 2D np.arrays and sample any missing states from the prior distributions and set up storage arrays for the sampled values.</p> <p>Ensures that all elements of the initial state are in an appropriate format for running the sampler:     - sparse matrices are left unchanged.     - all other data types are coerced (if possible) to np.ndarray.     - any scalars or existing np.ndarray with only one dimension are forced to be at         least 2D.</p> <p>Also initialises an item in the storage dictionary for each of the sampled values, for any data fitted values, and for the log-posterior value.</p> Source code in <code>src/openmcmc/mcmc.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Convert any state values to at least 2D np.arrays and sample any missing states from the prior distributions and set up storage arrays for the sampled values.\n\n    Ensures that all elements of the initial state are in an appropriate format for running\n    the sampler:\n        - sparse matrices are left unchanged.\n        - all other data types are coerced (if possible) to np.ndarray.\n        - any scalars or existing np.ndarray with only one dimension are forced to be at\n            least 2D.\n\n    Also initialises an item in the storage dictionary for each of the sampled values,\n    for any data fitted values, and for the log-posterior value.\n\n    \"\"\"\n    self.state = copy(self.state)\n\n    for key, term in self.state.items():\n        if sparse.issparse(term):\n            continue\n\n        if not isinstance(term, np.ndarray):\n            term = np.array(term, ndmin=2, dtype=np.float64)\n            if np.shape(term)[0] == 1:\n                term = term.T\n        elif term.ndim &lt; 2:\n            term = np.atleast_2d(term).T\n\n        self.state[key] = term\n\n    for sampler in self.samplers:\n        if sampler.param not in self.state:\n            self.state[sampler.param] = sampler.model[sampler.param].rvs(self.state)\n        self.store = sampler.init_store(current_state=self.state, store=self.store, n_iterations=self.n_iter)\n    if self.model.response is not None:\n        for response in self.model.response.keys():\n            self.store[response] = np.full(shape=(self.state[response].size, self.n_iter), fill_value=np.nan)\n    self.store[\"log_post\"] = np.full(shape=(self.n_iter, 1), fill_value=np.nan)\n</code></pre>"},{"location":"openmcmc/mcmc/#openmcmc.mcmc.MCMC.run_mcmc","title":"<code>run_mcmc()</code>","text":"<p>Runs MCMC routine for the given model specification.</p> <p>Numbers the iteratins of the sampler from -self.n_burn to self.n_iter, sotring every self.n_thin samples.</p> <p>Runs a first loop over samplers, and generates a sample for all corresponding variables in the state. Then stores the value of each of the sampled parameters in the self.store dictionary, as well as the data fitted values and the log-posterior value.</p> Source code in <code>src/openmcmc/mcmc.py</code> <pre><code>def run_mcmc(self):\n    \"\"\"Runs MCMC routine for the given model specification.\n\n    Numbers the iteratins of the sampler from -self.n_burn to self.n_iter, sotring every self.n_thin samples.\n\n    Runs a first loop over samplers, and generates a sample for all corresponding variables in the state. Then\n    stores the value of each of the sampled parameters in the self.store dictionary, as well as the data fitted\n    values and the log-posterior value.\n\n    \"\"\"\n    for i_it in tqdm(range(-self.n_burn, self.n_iter)):\n        for _ in range(self.n_thin):\n            for sampler in self.samplers:\n                self.state = sampler.sample(self.state)\n\n        if i_it &lt; 0:\n            continue\n\n        for sampler in self.samplers:\n            self.store = sampler.store(current_state=self.state, store=self.store, iteration=i_it)\n\n        self.store[\"log_post\"][i_it] = self.model.log_p(self.state)\n        if self.model.response is not None:\n            for response, predictor in self.model.response.items():\n                self.store[response][:, [i_it]] = getattr(self.model[response], predictor).predictor(self.state)\n\n    for sampler in self.samplers:\n        if isinstance(sampler, MetropolisHastings):\n            print(f\"{sampler.param}: {sampler.accept_rate.get_acceptance_rate()}\")\n</code></pre>"},{"location":"openmcmc/model/","title":"Model","text":""},{"location":"openmcmc/model/#model","title":"Model","text":"<p>Model module.</p> <p>This module provides the class definition of the Model class, a dictionary-like collection of distributions to form a model.</p>"},{"location":"openmcmc/model/#openmcmc.model.Model","title":"<code>Model</code>  <code>dataclass</code>","text":"<p>               Bases: <code>dict</code></p> <p>Dictionary-like collection of distributions to form a model.</p> <p>self.keys() indexes the responses of the distributions in the collection; self.values() contain the individual distribution objects in the model, of type Distribution.</p> <p>Attributes:</p> Name Type Description <code>response</code> <code>dict</code> <p>dictionary with keys corresponding to the data values within state, and values corresponding to the desired predictor values within the data distributions (for storing fitted values).</p> Source code in <code>src/openmcmc/model.py</code> <pre><code>@dataclass\nclass Model(dict):\n    \"\"\"Dictionary-like collection of distributions to form a model.\n\n    self.keys() indexes the responses of the distributions in the collection; self.values() contain the individual\n    distribution objects in the model, of type Distribution.\n\n    Attributes:\n        response (dict): dictionary with keys corresponding to the data values within state, and values corresponding\n            to the desired predictor values within the data distributions (for storing fitted values).\n\n    \"\"\"\n\n    def __init__(self, distributions: list[Distribution], response: dict = None):\n        dist_dict = {}\n        for dist in distributions:\n            dist_dict[dist.response] = dist\n        super().__init__(dist_dict)\n        self.response = response\n\n    def conditional(self, param: str):\n        \"\"\"Return sub-model which consists of the subset of distributions dependent on the supplied parameter.\n\n        Args:\n            param (str): parameter to find within the model distributions.\n\n        Returns:\n            (Model): model object containing only distributions which have a dependence on param.\n\n        \"\"\"\n        conditional_dist = []\n        for dst in self.values():\n            if param in dst.param_list:\n                conditional_dist.append(dst)\n        return Model(conditional_dist)\n\n    def log_p(self, state: dict) -&gt; Union[float, np.ndarray]:\n        \"\"\"Compute the log-probability density for the full model.\n\n        Args:\n            state (dict): dictionary with current state information.\n\n        Returns:\n            (Union[float, np.ndarray]): POSITIVE log-probability density evaluated using the information in state.\n\n        \"\"\"\n        log_prob = 0\n        for dst in self.values():\n            log_prob += dst.log_p(state)\n        return log_prob\n\n    def grad_log_p(\n        self, state: dict, param: str, hessian_required: bool = True\n    ) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"Generate vector of derivatives of the log-pdf with respect to a given parameter, as the sum of the derivatives of all the individual components of the model. If required, also generate the Hessian.\n\n        Function only defined for scalar- and vector-valued parameters param. If hessian_required=True, this function\n        returns a tuple of (gradient, Hessian). If hessian_required=False, this function returns a np.ndarray (just\n        the gradient of the log-density).\n\n        Args:\n            state (dict): current state information.\n            param (str): name of the parameter for which we compute derivatives.\n            hessian_required (bool): flag for whether the Hessian should be calculated and supplied as an output.\n\n        Returns:\n            (Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]): if hessian_required=True, then a tuple of (gradient,\n                hessian) is returned. If hessian_required=False, then just a gradient vector is returned. The returned\n                values are as follows:\n                grad (np.ndarray): vector gradients of the POSITIVE log-pdf with respect to param. shape=(d, 1), where\n                d is the dimensionality of param.\n                hessian (np.ndarray): array of NEGATIVE second derivatives of the log-pdf with respect to param.\n                shape=(d, d), where d is the dimensionality of param.\n\n        \"\"\"\n        grad_sum = np.zeros(shape=state[param].shape)\n        if hessian_required:\n            hessian_sum = np.zeros(shape=(state[param].shape[0], state[param].shape[0]))\n\n        for dist in self.values():\n            grad_out = dist.grad_log_p(state, param, hessian_required=hessian_required)\n            if hessian_required:\n                grad_sum += grad_out[0]\n                hessian_sum += grad_out[1]\n            else:\n                grad_sum += grad_out\n\n        if hessian_required:\n            return grad_sum, hessian_sum\n\n        return grad_sum\n</code></pre>"},{"location":"openmcmc/model/#openmcmc.model.Model.conditional","title":"<code>conditional(param)</code>","text":"<p>Return sub-model which consists of the subset of distributions dependent on the supplied parameter.</p> <p>Parameters:</p> Name Type Description Default <code>param</code> <code>str</code> <p>parameter to find within the model distributions.</p> required <p>Returns:</p> Type Description <code>Model</code> <p>model object containing only distributions which have a dependence on param.</p> Source code in <code>src/openmcmc/model.py</code> <pre><code>def conditional(self, param: str):\n    \"\"\"Return sub-model which consists of the subset of distributions dependent on the supplied parameter.\n\n    Args:\n        param (str): parameter to find within the model distributions.\n\n    Returns:\n        (Model): model object containing only distributions which have a dependence on param.\n\n    \"\"\"\n    conditional_dist = []\n    for dst in self.values():\n        if param in dst.param_list:\n            conditional_dist.append(dst)\n    return Model(conditional_dist)\n</code></pre>"},{"location":"openmcmc/model/#openmcmc.model.Model.log_p","title":"<code>log_p(state)</code>","text":"<p>Compute the log-probability density for the full model.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary with current state information.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>POSITIVE log-probability density evaluated using the information in state.</p> Source code in <code>src/openmcmc/model.py</code> <pre><code>def log_p(self, state: dict) -&gt; Union[float, np.ndarray]:\n    \"\"\"Compute the log-probability density for the full model.\n\n    Args:\n        state (dict): dictionary with current state information.\n\n    Returns:\n        (Union[float, np.ndarray]): POSITIVE log-probability density evaluated using the information in state.\n\n    \"\"\"\n    log_prob = 0\n    for dst in self.values():\n        log_prob += dst.log_p(state)\n    return log_prob\n</code></pre>"},{"location":"openmcmc/model/#openmcmc.model.Model.grad_log_p","title":"<code>grad_log_p(state, param, hessian_required=True)</code>","text":"<p>Generate vector of derivatives of the log-pdf with respect to a given parameter, as the sum of the derivatives of all the individual components of the model. If required, also generate the Hessian.</p> <p>Function only defined for scalar- and vector-valued parameters param. If hessian_required=True, this function returns a tuple of (gradient, Hessian). If hessian_required=False, this function returns a np.ndarray (just the gradient of the log-density).</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state information.</p> required <code>param</code> <code>str</code> <p>name of the parameter for which we compute derivatives.</p> required <code>hessian_required</code> <code>bool</code> <p>flag for whether the Hessian should be calculated and supplied as an output.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tuple[ndarray, ndarray]]</code> <p>if hessian_required=True, then a tuple of (gradient, hessian) is returned. If hessian_required=False, then just a gradient vector is returned. The returned values are as follows: grad (np.ndarray): vector gradients of the POSITIVE log-pdf with respect to param. shape=(d, 1), where d is the dimensionality of param. hessian (np.ndarray): array of NEGATIVE second derivatives of the log-pdf with respect to param. shape=(d, d), where d is the dimensionality of param.</p> Source code in <code>src/openmcmc/model.py</code> <pre><code>def grad_log_p(\n    self, state: dict, param: str, hessian_required: bool = True\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"Generate vector of derivatives of the log-pdf with respect to a given parameter, as the sum of the derivatives of all the individual components of the model. If required, also generate the Hessian.\n\n    Function only defined for scalar- and vector-valued parameters param. If hessian_required=True, this function\n    returns a tuple of (gradient, Hessian). If hessian_required=False, this function returns a np.ndarray (just\n    the gradient of the log-density).\n\n    Args:\n        state (dict): current state information.\n        param (str): name of the parameter for which we compute derivatives.\n        hessian_required (bool): flag for whether the Hessian should be calculated and supplied as an output.\n\n    Returns:\n        (Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]): if hessian_required=True, then a tuple of (gradient,\n            hessian) is returned. If hessian_required=False, then just a gradient vector is returned. The returned\n            values are as follows:\n            grad (np.ndarray): vector gradients of the POSITIVE log-pdf with respect to param. shape=(d, 1), where\n            d is the dimensionality of param.\n            hessian (np.ndarray): array of NEGATIVE second derivatives of the log-pdf with respect to param.\n            shape=(d, d), where d is the dimensionality of param.\n\n    \"\"\"\n    grad_sum = np.zeros(shape=state[param].shape)\n    if hessian_required:\n        hessian_sum = np.zeros(shape=(state[param].shape[0], state[param].shape[0]))\n\n    for dist in self.values():\n        grad_out = dist.grad_log_p(state, param, hessian_required=hessian_required)\n        if hessian_required:\n            grad_sum += grad_out[0]\n            hessian_sum += grad_out[1]\n        else:\n            grad_sum += grad_out\n\n    if hessian_required:\n        return grad_sum, hessian_sum\n\n    return grad_sum\n</code></pre>"},{"location":"openmcmc/parameter/","title":"Parameter","text":""},{"location":"openmcmc/parameter/#parameter","title":"Parameter","text":"<p>Collection of possible parameter specifications for the distribution objects.</p> <p>Example choices defined:</p> <p>Identity:  f = x LinearCombination:  f = X @ beta + Y @ gamma LinearCombinationWithTransform: f = X @ exp(beta) + Y @ gamma ScaledMatrix f = lam * P MixtureParameterVector  f= X[I] MixtureParameterMatrix  f= np.diag(lam[I])</p>"},{"location":"openmcmc/parameter/#openmcmc.parameter.Parameter","title":"<code>Parameter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for parameter.</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>@dataclass\nclass Parameter(ABC):\n    \"\"\"Abstract base class for parameter.\"\"\"\n\n    @abstractmethod\n    def predictor(self, state: dict) -&gt; np.ndarray:\n        \"\"\"Create predictor from the state dictionary using the functional form defined in the specific subclass.\n\n        Args:\n            state (dict): dictionary object containing the current state information\n\n        Returns:\n            (np.ndarray): predictor vector\n\n        \"\"\"\n\n    @abstractmethod\n    def get_param_list(self) -&gt; list:\n        \"\"\"Extract list of components from parameter specification.\n\n        Returns:\n            (list): parameter included as part of predictor\n\n        \"\"\"\n\n    @abstractmethod\n    def get_grad_param_list(self) -&gt; list:\n        \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n        Returns:\n            (list): parameter that grad is defined for.\n\n        \"\"\"\n\n    @abstractmethod\n    def grad(self, state: dict, param: str) -&gt; np.ndarray:\n        \"\"\"Compute gradient of single parameter.\n\n        Args:\n            state (dict): Dictionary object containing the current state information\n            param (str): Compute derivatives WRT this variable\n\n        Returns:\n            (np.ndarray): [n_param x n_data] array, gradient with respect to param\n\n        \"\"\"\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.Parameter.predictor","title":"<code>predictor(state)</code>  <code>abstractmethod</code>","text":"<p>Create predictor from the state dictionary using the functional form defined in the specific subclass.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>predictor vector</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>@abstractmethod\ndef predictor(self, state: dict) -&gt; np.ndarray:\n    \"\"\"Create predictor from the state dictionary using the functional form defined in the specific subclass.\n\n    Args:\n        state (dict): dictionary object containing the current state information\n\n    Returns:\n        (np.ndarray): predictor vector\n\n    \"\"\"\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.Parameter.get_param_list","title":"<code>get_param_list()</code>  <code>abstractmethod</code>","text":"<p>Extract list of components from parameter specification.</p> <p>Returns:</p> Type Description <code>list</code> <p>parameter included as part of predictor</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>@abstractmethod\ndef get_param_list(self) -&gt; list:\n    \"\"\"Extract list of components from parameter specification.\n\n    Returns:\n        (list): parameter included as part of predictor\n\n    \"\"\"\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.Parameter.get_grad_param_list","title":"<code>get_grad_param_list()</code>  <code>abstractmethod</code>","text":"<p>Extract list of components from parameter specification that grad is defined for.</p> <p>Returns:</p> Type Description <code>list</code> <p>parameter that grad is defined for.</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>@abstractmethod\ndef get_grad_param_list(self) -&gt; list:\n    \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n    Returns:\n        (list): parameter that grad is defined for.\n\n    \"\"\"\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.Parameter.grad","title":"<code>grad(state, param)</code>  <code>abstractmethod</code>","text":"<p>Compute gradient of single parameter.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary object containing the current state information</p> required <code>param</code> <code>str</code> <p>Compute derivatives WRT this variable</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>[n_param x n_data] array, gradient with respect to param</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>@abstractmethod\ndef grad(self, state: dict, param: str) -&gt; np.ndarray:\n    \"\"\"Compute gradient of single parameter.\n\n    Args:\n        state (dict): Dictionary object containing the current state information\n        param (str): Compute derivatives WRT this variable\n\n    Returns:\n        (np.ndarray): [n_param x n_data] array, gradient with respect to param\n\n    \"\"\"\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.Identity","title":"<code>Identity</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Parameter</code></p> <p>Class specifying a simple predictor in a single term.</p> Predictor has the functional form <p>f = x</p> <p>The gradient should only be used for scalar and vector inputs</p> <p>Parameters:</p> Name Type Description Default <code>form</code> <code>str</code> <p>string specifying the element of state which determines the parameter</p> required <p>Attributes:</p> Name Type Description <code>form</code> <code>str</code> <p>string specifying the element of state which determines the parameter.</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>@dataclass\nclass Identity(Parameter):\n    \"\"\"Class specifying a simple predictor in a single term.\n\n    Predictor has the functional form:\n        f = x\n\n    The gradient should only be used for scalar and vector inputs\n\n    Args:\n        form (str): string specifying the element of state which determines the parameter\n\n    Attributes:\n        form (str): string specifying the element of state which determines the parameter.\n\n    \"\"\"\n\n    form: str\n\n    def predictor(self, state: dict) -&gt; np.ndarray:\n        \"\"\"Create predictor from the state dictionary using the functional form defined in the specific subclass.\n\n        Args:\n            state (dict): dictionary object containing the current state information\n\n        Returns:\n            (np.ndarray): predictor vector\n\n        \"\"\"\n        return state[self.form]\n\n    def get_param_list(self) -&gt; list:\n        \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n        Returns:\n            (list): parameter that grad is defined for.\n\n        \"\"\"\n        return [self.form]\n\n    def get_grad_param_list(self) -&gt; list:\n        \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n        Returns:\n            (list): parameter that grad is defined for.\n\n        \"\"\"\n        return [self.form]\n\n    def grad(self, state: dict, param: str) -&gt; np.ndarray:\n        \"\"\"Compute gradient of single parameter.\n\n        Args:\n            state (dict): Dictionary object containing the current state information\n            param (str): Compute derivatives WRT this variable\n\n        Returns:\n            (np.ndarray): [n_param x n_data] array, gradient with respect to param\n\n        \"\"\"\n        if state[self.form].shape[1] &gt; 1:\n            raise ValueError(\"Gradient in Identity should not be used for variables 2D and above.\")\n        p = state[self.form].size\n        if param == self.form:\n            grad = np.eye(p)\n        else:\n            grad = np.zeros(shape=(p, p))\n        return grad\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.Identity.predictor","title":"<code>predictor(state)</code>","text":"<p>Create predictor from the state dictionary using the functional form defined in the specific subclass.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>predictor vector</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def predictor(self, state: dict) -&gt; np.ndarray:\n    \"\"\"Create predictor from the state dictionary using the functional form defined in the specific subclass.\n\n    Args:\n        state (dict): dictionary object containing the current state information\n\n    Returns:\n        (np.ndarray): predictor vector\n\n    \"\"\"\n    return state[self.form]\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.Identity.get_param_list","title":"<code>get_param_list()</code>","text":"<p>Extract list of components from parameter specification that grad is defined for.</p> <p>Returns:</p> Type Description <code>list</code> <p>parameter that grad is defined for.</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def get_param_list(self) -&gt; list:\n    \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n    Returns:\n        (list): parameter that grad is defined for.\n\n    \"\"\"\n    return [self.form]\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.Identity.get_grad_param_list","title":"<code>get_grad_param_list()</code>","text":"<p>Extract list of components from parameter specification that grad is defined for.</p> <p>Returns:</p> Type Description <code>list</code> <p>parameter that grad is defined for.</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def get_grad_param_list(self) -&gt; list:\n    \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n    Returns:\n        (list): parameter that grad is defined for.\n\n    \"\"\"\n    return [self.form]\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.Identity.grad","title":"<code>grad(state, param)</code>","text":"<p>Compute gradient of single parameter.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary object containing the current state information</p> required <code>param</code> <code>str</code> <p>Compute derivatives WRT this variable</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>[n_param x n_data] array, gradient with respect to param</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def grad(self, state: dict, param: str) -&gt; np.ndarray:\n    \"\"\"Compute gradient of single parameter.\n\n    Args:\n        state (dict): Dictionary object containing the current state information\n        param (str): Compute derivatives WRT this variable\n\n    Returns:\n        (np.ndarray): [n_param x n_data] array, gradient with respect to param\n\n    \"\"\"\n    if state[self.form].shape[1] &gt; 1:\n        raise ValueError(\"Gradient in Identity should not be used for variables 2D and above.\")\n    p = state[self.form].size\n    if param == self.form:\n        grad = np.eye(p)\n    else:\n        grad = np.zeros(shape=(p, p))\n    return grad\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.LinearCombination","title":"<code>LinearCombination</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Parameter</code></p> <p>Class specifying linear combination form .</p> <p>This Parameter type is typically in the mean of a Normal distribution in a linear regression type case.</p> <p>Predictor has the form     predictor  = sum_i (value[i] @ key[i]) using the form dictionary input</p> <p>Attributes:</p> Name Type Description <code>form</code> <code>dict</code> <p>dict specifying the term and prefactor in the linear combination. example: {'beta': 'X', 'alpha': 'A'} produces linear combination X @ beta + A @ alpha.</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>@dataclass\nclass LinearCombination(Parameter):\n    \"\"\"Class specifying linear combination form .\n\n    This Parameter type is typically in the mean of a Normal distribution in a linear regression type case.\n\n    Predictor has the form\n        predictor  = sum_i (value[i] @ key[i])\n    using the form dictionary input\n\n    Attributes:\n        form (dict): dict specifying the term and prefactor in the linear combination.\n            example: {'beta': 'X', 'alpha': 'A'} produces linear combination X @ beta + A @ alpha.\n\n    \"\"\"\n\n    form: dict\n\n    def predictor(self, state: dict) -&gt; np.ndarray:\n        \"\"\"Create predictor from the state dictionary using the functional form defined in the specific subclass.\n\n        Args:\n            state (dict): dictionary object containing the current state information\n\n        Returns:\n            (np.ndarray): predictor vector\n\n        \"\"\"\n        return self.predictor_conditional(state)\n\n    def predictor_conditional(self, state: dict, term_to_exclude: Union[str, list] = None) -&gt; np.ndarray:\n        \"\"\"Extract predictor from the state dictionary using the functional form defined in the specific subclass excluding parameters.\n\n        Used when estimating conditional distributions of those parameters.\n\n        Args:\n            state (dict): dictionary object containing the current state information\n            term_to_exclude (Union[str, list]): terms to exclude from predictor\n\n        Returns:\n            (np.ndarray): predictor vector\n\n        \"\"\"\n        if term_to_exclude is None:\n            term_to_exclude = []\n\n        if isinstance(term_to_exclude, str):\n            term_to_exclude = [term_to_exclude]\n\n        sum_terms = 0\n        for prm, prefactor in self.form.items():\n            if prm not in term_to_exclude:\n                sum_terms += state[prefactor] @ state[prm]\n        return sum_terms\n\n    def get_param_list(self) -&gt; list:\n        \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n        Returns:\n            (list): parameter that grad is defined for.\n\n        \"\"\"\n        return list(self.form.keys()) + list(self.form.values())\n\n    def get_grad_param_list(self) -&gt; list:\n        \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n        Returns:\n            (list): parameter that grad is defined for.\n\n        \"\"\"\n        return list(self.form.keys())\n\n    def grad(self, state: dict, param: str) -&gt; np.ndarray:\n        \"\"\"Compute gradient of single parameter.\n\n        Args:\n            state (dict): Dictionary object containing the current state information\n            param (str): Compute derivatives WRT this variable\n\n        Returns:\n            (np.ndarray): [n_param x n_data] array, gradient with respect to param\n\n        \"\"\"\n        return state[self.form[param]].T\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.LinearCombination.predictor","title":"<code>predictor(state)</code>","text":"<p>Create predictor from the state dictionary using the functional form defined in the specific subclass.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>predictor vector</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def predictor(self, state: dict) -&gt; np.ndarray:\n    \"\"\"Create predictor from the state dictionary using the functional form defined in the specific subclass.\n\n    Args:\n        state (dict): dictionary object containing the current state information\n\n    Returns:\n        (np.ndarray): predictor vector\n\n    \"\"\"\n    return self.predictor_conditional(state)\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.LinearCombination.predictor_conditional","title":"<code>predictor_conditional(state, term_to_exclude=None)</code>","text":"<p>Extract predictor from the state dictionary using the functional form defined in the specific subclass excluding parameters.</p> <p>Used when estimating conditional distributions of those parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information</p> required <code>term_to_exclude</code> <code>Union[str, list]</code> <p>terms to exclude from predictor</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>predictor vector</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def predictor_conditional(self, state: dict, term_to_exclude: Union[str, list] = None) -&gt; np.ndarray:\n    \"\"\"Extract predictor from the state dictionary using the functional form defined in the specific subclass excluding parameters.\n\n    Used when estimating conditional distributions of those parameters.\n\n    Args:\n        state (dict): dictionary object containing the current state information\n        term_to_exclude (Union[str, list]): terms to exclude from predictor\n\n    Returns:\n        (np.ndarray): predictor vector\n\n    \"\"\"\n    if term_to_exclude is None:\n        term_to_exclude = []\n\n    if isinstance(term_to_exclude, str):\n        term_to_exclude = [term_to_exclude]\n\n    sum_terms = 0\n    for prm, prefactor in self.form.items():\n        if prm not in term_to_exclude:\n            sum_terms += state[prefactor] @ state[prm]\n    return sum_terms\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.LinearCombination.get_param_list","title":"<code>get_param_list()</code>","text":"<p>Extract list of components from parameter specification that grad is defined for.</p> <p>Returns:</p> Type Description <code>list</code> <p>parameter that grad is defined for.</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def get_param_list(self) -&gt; list:\n    \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n    Returns:\n        (list): parameter that grad is defined for.\n\n    \"\"\"\n    return list(self.form.keys()) + list(self.form.values())\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.LinearCombination.get_grad_param_list","title":"<code>get_grad_param_list()</code>","text":"<p>Extract list of components from parameter specification that grad is defined for.</p> <p>Returns:</p> Type Description <code>list</code> <p>parameter that grad is defined for.</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def get_grad_param_list(self) -&gt; list:\n    \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n    Returns:\n        (list): parameter that grad is defined for.\n\n    \"\"\"\n    return list(self.form.keys())\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.LinearCombination.grad","title":"<code>grad(state, param)</code>","text":"<p>Compute gradient of single parameter.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary object containing the current state information</p> required <code>param</code> <code>str</code> <p>Compute derivatives WRT this variable</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>[n_param x n_data] array, gradient with respect to param</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def grad(self, state: dict, param: str) -&gt; np.ndarray:\n    \"\"\"Compute gradient of single parameter.\n\n    Args:\n        state (dict): Dictionary object containing the current state information\n        param (str): Compute derivatives WRT this variable\n\n    Returns:\n        (np.ndarray): [n_param x n_data] array, gradient with respect to param\n\n    \"\"\"\n    return state[self.form[param]].T\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.LinearCombinationWithTransform","title":"<code>LinearCombinationWithTransform</code>  <code>dataclass</code>","text":"<p>               Bases: <code>LinearCombination</code></p> <p>Linear combination of parameters from the state, with optional exponential transformation for the parameter elements.</p> <p>Currently, the only allowed transformation is the exponential transform.</p> <p>This Parameter type is typically in the mean of a Normal distribution and could be used to impose positivity of the parameters</p> <p>Predictor has the form     predictor  = sum_i (value[i] @ transform(key[i])) using the form dictionary input</p> <p>Attributes:</p> Name Type Description <code>transform</code> <code>dict</code> <p>dict with logicals specifying whether exp(.) transform should be applied to parameter example: form={'beta': X}, transform={'beta': True} will produce X @ np.exp(beta)</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>@dataclass\nclass LinearCombinationWithTransform(LinearCombination):\n    \"\"\"Linear combination of parameters from the state, with optional exponential transformation for the parameter elements.\n\n    Currently, the only allowed transformation is the exponential transform.\n\n    This Parameter type is typically in the mean of a Normal distribution and could be\n    used to impose positivity of the parameters\n\n    Predictor has the form\n        predictor  = sum_i (value[i] @ transform(key[i]))\n    using the form dictionary input\n\n    Attributes:\n        transform (dict): dict with logicals specifying whether exp(.) transform should\n            be applied to parameter\n            example: form={'beta': X}, transform={'beta': True} will produce X @ np.exp(beta)\n\n    \"\"\"\n\n    transform: dict\n\n    def predictor_conditional(self, state: dict, term_to_exclude: Union[str, list] = None) -&gt; np.ndarray:\n        \"\"\"Extract predictor from the state dictionary using the functional form defined in the specific subclass excluding parameters.\n\n        Used when estimating conditional distributions of those parameters.\n\n        Args:\n            state (dict): dictionary object containing the current state information\n            term_to_exclude (list): terms to exclude from predictor\n\n        Returns:\n            (np.ndarray): predictor vector\n\n        \"\"\"\n        if term_to_exclude is None:\n            term_to_exclude = []\n\n        if isinstance(term_to_exclude, str):\n            term_to_exclude = [term_to_exclude]\n\n        sum_terms = 0\n        for prm, prefactor in self.form.items():\n            if prm not in term_to_exclude:\n                param = state[prm]\n                if self.transform[prm]:\n                    param = np.exp(param)\n                sum_terms += state[prefactor] @ param\n        return sum_terms\n\n    def grad(self, state: dict, param: str) -&gt; np.ndarray:\n        \"\"\"Compute gradient of single parameter.\n\n        Args:\n            state (dict): Dictionary object containing the current state information\n            param (str): Compute derivatives WRT this variable\n\n        Returns:\n            (np.ndarray): [n_param x n_data] array, gradient with respect to param\n\n        \"\"\"\n        if self.transform[param]:\n            if sparse.issparse(state[self.form[param]]):\n                return state[self.form[param]].multiply(np.exp(state[param]).flatten()).T\n            return np.exp(state[param]) * (state[self.form[param]].T)\n\n        return state[self.form[param]].T\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.LinearCombinationWithTransform.predictor_conditional","title":"<code>predictor_conditional(state, term_to_exclude=None)</code>","text":"<p>Extract predictor from the state dictionary using the functional form defined in the specific subclass excluding parameters.</p> <p>Used when estimating conditional distributions of those parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information</p> required <code>term_to_exclude</code> <code>list</code> <p>terms to exclude from predictor</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>predictor vector</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def predictor_conditional(self, state: dict, term_to_exclude: Union[str, list] = None) -&gt; np.ndarray:\n    \"\"\"Extract predictor from the state dictionary using the functional form defined in the specific subclass excluding parameters.\n\n    Used when estimating conditional distributions of those parameters.\n\n    Args:\n        state (dict): dictionary object containing the current state information\n        term_to_exclude (list): terms to exclude from predictor\n\n    Returns:\n        (np.ndarray): predictor vector\n\n    \"\"\"\n    if term_to_exclude is None:\n        term_to_exclude = []\n\n    if isinstance(term_to_exclude, str):\n        term_to_exclude = [term_to_exclude]\n\n    sum_terms = 0\n    for prm, prefactor in self.form.items():\n        if prm not in term_to_exclude:\n            param = state[prm]\n            if self.transform[prm]:\n                param = np.exp(param)\n            sum_terms += state[prefactor] @ param\n    return sum_terms\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.LinearCombinationWithTransform.grad","title":"<code>grad(state, param)</code>","text":"<p>Compute gradient of single parameter.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary object containing the current state information</p> required <code>param</code> <code>str</code> <p>Compute derivatives WRT this variable</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>[n_param x n_data] array, gradient with respect to param</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def grad(self, state: dict, param: str) -&gt; np.ndarray:\n    \"\"\"Compute gradient of single parameter.\n\n    Args:\n        state (dict): Dictionary object containing the current state information\n        param (str): Compute derivatives WRT this variable\n\n    Returns:\n        (np.ndarray): [n_param x n_data] array, gradient with respect to param\n\n    \"\"\"\n    if self.transform[param]:\n        if sparse.issparse(state[self.form[param]]):\n            return state[self.form[param]].multiply(np.exp(state[param]).flatten()).T\n        return np.exp(state[param]) * (state[self.form[param]].T)\n\n    return state[self.form[param]].T\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.ScaledMatrix","title":"<code>ScaledMatrix</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Parameter</code></p> <p>Defines parameter a scalar factor in front of a matrix.</p> <p>This is often used in case where we have a scalar variance in front of an unscaled precision matrix. Where we have a gamma distribution for the scalar parameter which wish to estimate</p> Linear combinations have the form <p>predictor = scalar * matrix</p> <p>Attributes:</p> Name Type Description <code>matrix</code> <code>str</code> <p>variable name of the un-scaled matrix</p> <code>scalar</code> <code>str</code> <p>variable name of the scalar term</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>@dataclass\nclass ScaledMatrix(Parameter):\n    \"\"\"Defines parameter a scalar factor in front of a matrix.\n\n    This is often used in case where we have a scalar variance in front of an unscaled precision matrix.\n    Where we have a gamma distribution for the scalar parameter which wish to estimate\n\n    Linear combinations have the form:\n        predictor = scalar * matrix\n\n    Attributes:\n        matrix (str): variable name of the un-scaled matrix\n        scalar (str): variable name of the scalar term\n\n    \"\"\"\n\n    matrix: str\n    scalar: str\n\n    def predictor(self, state: dict) -&gt; np.ndarray:\n        \"\"\"Create predictor from the state dictionary using the functional form defined in the specific subclass.\n\n        Args:\n            state (dict): dictionary object containing the current state information\n\n        Returns:\n            (np.ndarray): predictor vector\n\n        \"\"\"\n        return float(state[self.scalar].item()) * state[self.matrix]\n\n    def get_param_list(self) -&gt; list:\n        \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n        Returns:\n            (list): parameter that grad is defined for.\n\n        \"\"\"\n        return [self.scalar, self.matrix]\n\n    def get_grad_param_list(self) -&gt; list:\n        \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n        Returns:\n            (list): parameter that grad is defined for.\n\n        \"\"\"\n        return [self.scalar]\n\n    def grad(self, state: dict, param: str) -&gt; np.ndarray:\n        \"\"\"Compute gradient of single parameter.\n\n        Args:\n            state (dict): Dictionary object containing the current state information\n            param (str): Compute derivatives WRT this variable\n\n        Returns:\n            (np.ndarray): [n_param x n_data] array, gradient with respect to param\n\n        \"\"\"\n        return state[self.matrix]\n\n    def precision_unscaled(self, state: dict, _) -&gt; np.ndarray:\n        \"\"\"Return the precision matrix un-scaled by the scalar precision parameter.\n\n        Args:\n            state (dict): state dictionary\n            _ (int): argument unused but matches with version in MixtureParameterMatrix where element is needed\n\n        Returns:\n            (np.ndarray): unscaled precision matrix\n\n        \"\"\"\n        return state[self.matrix]\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.ScaledMatrix.predictor","title":"<code>predictor(state)</code>","text":"<p>Create predictor from the state dictionary using the functional form defined in the specific subclass.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>predictor vector</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def predictor(self, state: dict) -&gt; np.ndarray:\n    \"\"\"Create predictor from the state dictionary using the functional form defined in the specific subclass.\n\n    Args:\n        state (dict): dictionary object containing the current state information\n\n    Returns:\n        (np.ndarray): predictor vector\n\n    \"\"\"\n    return float(state[self.scalar].item()) * state[self.matrix]\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.ScaledMatrix.get_param_list","title":"<code>get_param_list()</code>","text":"<p>Extract list of components from parameter specification that grad is defined for.</p> <p>Returns:</p> Type Description <code>list</code> <p>parameter that grad is defined for.</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def get_param_list(self) -&gt; list:\n    \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n    Returns:\n        (list): parameter that grad is defined for.\n\n    \"\"\"\n    return [self.scalar, self.matrix]\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.ScaledMatrix.get_grad_param_list","title":"<code>get_grad_param_list()</code>","text":"<p>Extract list of components from parameter specification that grad is defined for.</p> <p>Returns:</p> Type Description <code>list</code> <p>parameter that grad is defined for.</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def get_grad_param_list(self) -&gt; list:\n    \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n    Returns:\n        (list): parameter that grad is defined for.\n\n    \"\"\"\n    return [self.scalar]\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.ScaledMatrix.grad","title":"<code>grad(state, param)</code>","text":"<p>Compute gradient of single parameter.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary object containing the current state information</p> required <code>param</code> <code>str</code> <p>Compute derivatives WRT this variable</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>[n_param x n_data] array, gradient with respect to param</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def grad(self, state: dict, param: str) -&gt; np.ndarray:\n    \"\"\"Compute gradient of single parameter.\n\n    Args:\n        state (dict): Dictionary object containing the current state information\n        param (str): Compute derivatives WRT this variable\n\n    Returns:\n        (np.ndarray): [n_param x n_data] array, gradient with respect to param\n\n    \"\"\"\n    return state[self.matrix]\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.ScaledMatrix.precision_unscaled","title":"<code>precision_unscaled(state, _)</code>","text":"<p>Return the precision matrix un-scaled by the scalar precision parameter.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>state dictionary</p> required <code>_</code> <code>int</code> <p>argument unused but matches with version in MixtureParameterMatrix where element is needed</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>unscaled precision matrix</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def precision_unscaled(self, state: dict, _) -&gt; np.ndarray:\n    \"\"\"Return the precision matrix un-scaled by the scalar precision parameter.\n\n    Args:\n        state (dict): state dictionary\n        _ (int): argument unused but matches with version in MixtureParameterMatrix where element is needed\n\n    Returns:\n        (np.ndarray): unscaled precision matrix\n\n    \"\"\"\n    return state[self.matrix]\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.MixtureParameter","title":"<code>MixtureParameter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Parameter</code>, <code>ABC</code></p> <p>Abstract Parameter class for a mixture distribution.</p> <p>Subclasses implemented for both:</p> <ul> <li>vector-valued parameter (MixtureParameterVector)</li> <li>diagonal matrix-valued parameter (MixtureParameterMatrix) where the elements of the vector or matrix diagonal are allocated based on the allocation parameter.</li> </ul> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>@dataclass\nclass MixtureParameter(Parameter, ABC):\n    \"\"\"Abstract Parameter class for a mixture distribution.\n\n    Subclasses implemented for both:\n\n    - vector-valued parameter (MixtureParameterVector)\n    - diagonal matrix-valued parameter (MixtureParameterMatrix)\n    where the elements of the vector or matrix diagonal are allocated based\n    on the allocation parameter.\n\n    \"\"\"\n\n    param: str\n    allocation: str\n\n    def get_element_match(self, state: dict, element_index: Union[int, np.ndarray]) -&gt; np.ndarray:\n        \"\"\"Extract the parts of self.allocation which have given element number.\n\n        used in the gradient function to pull out gradient for given element.\n\n        Args:\n            state (dict): state vector\n            element_index (int, np.array): element index or set of integers\n\n        Returns:\n            (np.array(dtype=int)): element matches with 1 where there is a match and 0 where there isn't\n\n        \"\"\"\n        if isinstance(element_index, np.ndarray) and element_index.size &gt; 1:\n            element_index = element_index.reshape((1, -1))\n\n        return np.array(state[self.allocation] == element_index, dtype=int, ndmin=2)\n\n    def get_param_list(self) -&gt; list:\n        \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n        Returns:\n            (list): parameter that grad is defined for.\n\n        \"\"\"\n        return [self.param, self.allocation]\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.MixtureParameter.get_element_match","title":"<code>get_element_match(state, element_index)</code>","text":"<p>Extract the parts of self.allocation which have given element number.</p> <p>used in the gradient function to pull out gradient for given element.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>state vector</p> required <code>element_index</code> <code>(int, array)</code> <p>element index or set of integers</p> required <p>Returns:</p> Type Description <code>array(dtype=int)</code> <p>element matches with 1 where there is a match and 0 where there isn't</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def get_element_match(self, state: dict, element_index: Union[int, np.ndarray]) -&gt; np.ndarray:\n    \"\"\"Extract the parts of self.allocation which have given element number.\n\n    used in the gradient function to pull out gradient for given element.\n\n    Args:\n        state (dict): state vector\n        element_index (int, np.array): element index or set of integers\n\n    Returns:\n        (np.array(dtype=int)): element matches with 1 where there is a match and 0 where there isn't\n\n    \"\"\"\n    if isinstance(element_index, np.ndarray) and element_index.size &gt; 1:\n        element_index = element_index.reshape((1, -1))\n\n    return np.array(state[self.allocation] == element_index, dtype=int, ndmin=2)\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.MixtureParameter.get_param_list","title":"<code>get_param_list()</code>","text":"<p>Extract list of components from parameter specification that grad is defined for.</p> <p>Returns:</p> Type Description <code>list</code> <p>parameter that grad is defined for.</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def get_param_list(self) -&gt; list:\n    \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n    Returns:\n        (list): parameter that grad is defined for.\n\n    \"\"\"\n    return [self.param, self.allocation]\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.MixtureParameterVector","title":"<code>MixtureParameterVector</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MixtureParameter</code></p> <p>Vector parameter: elements of the vector are obtained from sub-parameter 'param' according to the allocation.</p> <p>The allocation parameter defines a mapping between a R^m and R^n where typically m&lt;=n and m is the number true underlying number of parameters in the model but due to the representation/algebra in other parts of the model this is expanded out to an n parameter model where the values of m are copied according to the index vector</p> <p>predictor = param [allocation]</p> <p>Attributes:</p> Name Type Description <code>param</code> <code>str</code> <p>name of underlying state component used to generate parameter.</p> <code>allocation</code> <code>ndarray</code> <p>name of allocation parameter within state dict.</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>@dataclass\nclass MixtureParameterVector(MixtureParameter):\n    \"\"\"Vector parameter: elements of the vector are obtained from sub-parameter 'param' according to the allocation.\n\n    The allocation parameter defines a mapping between a R^m and R^n where typically m&lt;=n and m is the\n    number true underlying number of parameters in the model but due to the representation/algebra in\n    other parts of the model this is expanded out to an n parameter model where the values of m are copied\n    according to the index vector\n\n    predictor = param [allocation]\n\n    Attributes:\n        param (str): name of underlying state component used to generate parameter.\n        allocation (np.ndarray): name of allocation parameter within state dict.\n\n    \"\"\"\n\n    def predictor(self, state: dict) -&gt; np.ndarray:\n        \"\"\"Create predictor from the state dictionary using the functional form defined in the specific subclass.\n\n        Args:\n            state (dict): dictionary object containing the current state information\n\n        Returns:\n            (np.ndarray): predictor vector\n\n        \"\"\"\n        return state[self.param][state[self.allocation].flatten()]\n\n    def grad(self, state: dict, param: str):\n        \"\"\"Compute gradient of single parameter.\n\n        Args:\n            state (dict): Dictionary object containing the current state information\n            param (str): Compute derivatives WRT this variable\n\n        Returns:\n            (np.ndarray): [n_param x n_data] array, gradient with respect to param\n\n        \"\"\"\n        element_index = np.arange(0, state[param].size)\n\n        return self.get_element_match(state, element_index).astype(np.float64).T\n\n    def get_grad_param_list(self) -&gt; list:\n        \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n        Returns:\n            (list): parameter that grad is defined for.\n\n        \"\"\"\n        return [self.param]\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.MixtureParameterVector.predictor","title":"<code>predictor(state)</code>","text":"<p>Create predictor from the state dictionary using the functional form defined in the specific subclass.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>predictor vector</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def predictor(self, state: dict) -&gt; np.ndarray:\n    \"\"\"Create predictor from the state dictionary using the functional form defined in the specific subclass.\n\n    Args:\n        state (dict): dictionary object containing the current state information\n\n    Returns:\n        (np.ndarray): predictor vector\n\n    \"\"\"\n    return state[self.param][state[self.allocation].flatten()]\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.MixtureParameterVector.grad","title":"<code>grad(state, param)</code>","text":"<p>Compute gradient of single parameter.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary object containing the current state information</p> required <code>param</code> <code>str</code> <p>Compute derivatives WRT this variable</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>[n_param x n_data] array, gradient with respect to param</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def grad(self, state: dict, param: str):\n    \"\"\"Compute gradient of single parameter.\n\n    Args:\n        state (dict): Dictionary object containing the current state information\n        param (str): Compute derivatives WRT this variable\n\n    Returns:\n        (np.ndarray): [n_param x n_data] array, gradient with respect to param\n\n    \"\"\"\n    element_index = np.arange(0, state[param].size)\n\n    return self.get_element_match(state, element_index).astype(np.float64).T\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.MixtureParameterVector.get_grad_param_list","title":"<code>get_grad_param_list()</code>","text":"<p>Extract list of components from parameter specification that grad is defined for.</p> <p>Returns:</p> Type Description <code>list</code> <p>parameter that grad is defined for.</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def get_grad_param_list(self) -&gt; list:\n    \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n    Returns:\n        (list): parameter that grad is defined for.\n\n    \"\"\"\n    return [self.param]\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.MixtureParameterMatrix","title":"<code>MixtureParameterMatrix</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MixtureParameter</code></p> <p>Diagonal matrix parameter: elements of the diagonal are obtained from sub-parameter 'param' according to the allocation index vector.</p> <p>The allocation parameter defines a mapping between a R^m and R^n where typically m&lt;=n and m is the number true underlying number of parameters in the model but due to the representation/algebra in other parts of the model this is expanded out to an n parameter model where the values of m are copied according to the index vector</p> <p>predictor = np.diag( param [allocation] )</p> <p>Attributes:</p> Name Type Description <code>param</code> <code>str</code> <p>name of underlying state component used to generate parameter.</p> <code>allocation</code> <code>ndarray</code> <p>name of allocation parameter within state dict.</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>@dataclass\nclass MixtureParameterMatrix(MixtureParameter):\n    \"\"\"Diagonal matrix parameter: elements of the diagonal are obtained from sub-parameter 'param' according to the allocation index vector.\n\n    The allocation parameter defines a mapping between a R^m and R^n where typically m&lt;=n and m is the\n    number true underlying number of parameters in the model but due to the representation/algebra in\n    other parts of the model this is expanded out to an n parameter model where the values of m are copied\n    according to the index vector\n\n    predictor = np.diag( param [allocation] )\n\n    Attributes:\n        param (str): name of underlying state component used to generate parameter.\n        allocation (np.ndarray): name of allocation parameter within state dict.\n\n    \"\"\"\n\n    def predictor(self, state: dict) -&gt; sparse.csc_matrix:\n        \"\"\"Create predictor from the state dictionary using the functional form defined in the specific subclass.\n\n        Args:\n            state (dict): dictionary object containing the current state information\n\n        Returns:\n            (sparse.csc_matrix): predictor vector\n\n        \"\"\"\n        return sparse.diags(diagonals=state[self.param][state[self.allocation]].flatten(), offsets=0, format=\"csc\")\n\n    def grad(self, state: dict, param: str):\n        \"\"\"Compute gradient of single parameter.\n\n        Args:\n            state (dict): Dictionary object containing the current state information\n            param (str): Compute derivatives WRT this variable\n\n        Returns:\n            (np.ndarray): [n_param x n_data] array, gradient with respect to param\n\n        \"\"\"\n        raise TypeError(\"Not defined in this case\")\n\n    def get_grad_param_list(self) -&gt; list:\n        \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n        Returns:\n            (list): parameter that grad is defined for.\n\n        \"\"\"\n        return []\n\n    def precision_unscaled(self, state: dict, element_index: int) -&gt; np.ndarray:\n        \"\"\"Return the precision matrix un-scaled by the scalar precision parameter.\n\n        Args:\n            state (dict): state dictionary\n            element_index (int): index of element to subset\n\n        Returns:\n            (np.ndarray): unscaled precision matrix\n\n        \"\"\"\n        return sparse.diags(diagonals=self.get_element_match(state, element_index).flatten(), offsets=0, format=\"csc\")\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.MixtureParameterMatrix.predictor","title":"<code>predictor(state)</code>","text":"<p>Create predictor from the state dictionary using the functional form defined in the specific subclass.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information</p> required <p>Returns:</p> Type Description <code>csc_matrix</code> <p>predictor vector</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def predictor(self, state: dict) -&gt; sparse.csc_matrix:\n    \"\"\"Create predictor from the state dictionary using the functional form defined in the specific subclass.\n\n    Args:\n        state (dict): dictionary object containing the current state information\n\n    Returns:\n        (sparse.csc_matrix): predictor vector\n\n    \"\"\"\n    return sparse.diags(diagonals=state[self.param][state[self.allocation]].flatten(), offsets=0, format=\"csc\")\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.MixtureParameterMatrix.grad","title":"<code>grad(state, param)</code>","text":"<p>Compute gradient of single parameter.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>Dictionary object containing the current state information</p> required <code>param</code> <code>str</code> <p>Compute derivatives WRT this variable</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>[n_param x n_data] array, gradient with respect to param</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def grad(self, state: dict, param: str):\n    \"\"\"Compute gradient of single parameter.\n\n    Args:\n        state (dict): Dictionary object containing the current state information\n        param (str): Compute derivatives WRT this variable\n\n    Returns:\n        (np.ndarray): [n_param x n_data] array, gradient with respect to param\n\n    \"\"\"\n    raise TypeError(\"Not defined in this case\")\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.MixtureParameterMatrix.get_grad_param_list","title":"<code>get_grad_param_list()</code>","text":"<p>Extract list of components from parameter specification that grad is defined for.</p> <p>Returns:</p> Type Description <code>list</code> <p>parameter that grad is defined for.</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def get_grad_param_list(self) -&gt; list:\n    \"\"\"Extract list of components from parameter specification that grad is defined for.\n\n    Returns:\n        (list): parameter that grad is defined for.\n\n    \"\"\"\n    return []\n</code></pre>"},{"location":"openmcmc/parameter/#openmcmc.parameter.MixtureParameterMatrix.precision_unscaled","title":"<code>precision_unscaled(state, element_index)</code>","text":"<p>Return the precision matrix un-scaled by the scalar precision parameter.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>state dictionary</p> required <code>element_index</code> <code>int</code> <p>index of element to subset</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>unscaled precision matrix</p> Source code in <code>src/openmcmc/parameter.py</code> <pre><code>def precision_unscaled(self, state: dict, element_index: int) -&gt; np.ndarray:\n    \"\"\"Return the precision matrix un-scaled by the scalar precision parameter.\n\n    Args:\n        state (dict): state dictionary\n        element_index (int): index of element to subset\n\n    Returns:\n        (np.ndarray): unscaled precision matrix\n\n    \"\"\"\n    return sparse.diags(diagonals=self.get_element_match(state, element_index).flatten(), offsets=0, format=\"csc\")\n</code></pre>"},{"location":"openmcmc/distribution/distribution/","title":"Distribution","text":""},{"location":"openmcmc/distribution/distribution/#distribution","title":"Distribution","text":"<p>Collection of distributions for use with openMCMC code.</p> General assumptions about code functionality <ul> <li>The first dimension of a parameter array is assumed to represent the dimensionality of the parameter vector; the     second dimension is assumed to represent independent realizations of the parameter set. For example: an array     with shape=(d, n) would be assumed to hold n replicates of a d-dimensional parameter vector.</li> <li>self.response is a string containing the name of the response parameter for the distribution. For example, when     self.response=\"y\", all functions within the class will perform calculations using the value stored in     state[\"y\"].</li> </ul>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Distribution","title":"<code>Distribution</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract superclass for handling distribution objects.</p> <p>Attributes:</p> Name Type Description <code>response</code> <code>str</code> <p>specifies the name of the response variable of the distribution.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>@dataclass\nclass Distribution(ABC):\n    \"\"\"Abstract superclass for handling distribution objects.\n\n    Attributes:\n        response (str): specifies the name of the response variable of the distribution.\n\n    \"\"\"\n\n    response: str\n\n    @abstractmethod\n    def log_p(self, state: dict, by_observation: bool = False) -&gt; Union[np.ndarray, float]:\n        \"\"\"Compute the log of the probability density (for current parameter settings).\n\n        Args:\n            state (dict): dictionary object containing the current state information. state[distribution.response]\n                is expected to be p x n where: p is the number of responses; n is the number of independent\n                replicates/observations.\n            by_observation (bool, optional): If True, the log-likelihood is returned for each of the p responses of\n                the distribution separately. Defaults to False.\n\n        Returns:\n            (Union[np.ndarray, float]): POSITIVE log-density evaluated using the supplied state dictionary.\n\n        \"\"\"\n\n    @abstractmethod\n    def rvs(self, state: dict, n: int = 1) -&gt; np.ndarray:\n        \"\"\"Generate random samples from the distribution.\n\n        Args:\n            state (dict): dictionary object containing the current state information.\n            n (int, optional): specifies the number of replicate samples required. Defaults to 1.\n\n        Returns:\n            (np.ndarray): random variables generated from distribution returned as p x n where p is the\n                dimensionality of the response.\n\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def _dist_params(self) -&gt; list:\n        \"\"\"Get list of parameter labels across all Parameter objects in distribution (EXCLUDING the response).\n\n        Returns:\n            (list): list of parameter labels.\n\n        \"\"\"\n\n    @property\n    def param_list(self) -&gt; list:\n        \"\"\"Get list of all parameter labels in model (INCLUDING the response).\n\n        Returns:\n            (list): list of parameter labels\n\n        \"\"\"\n        lst = [self.response] + self._dist_params\n        return lst\n\n    def grad_log_p(\n        self, state: dict, param: str, hessian_required: bool = True\n    ) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"Generate vector of derivatives of the log-pdf with respect to a given parameter, and if required, also generate the Hessian.\n\n        Function only defined for scalar- and vector-valued parameters param. If hessian_required=True, this function\n        returns a tuple of (gradient, Hessian). If hessian_required=False, this function returns a np.ndarray (just\n        the gradient of the log-density).\n\n        As a default, the individual gradients are computed by finite-differencing the log_p function defined for the\n        distribution. Where analytical forms for the gradient exist, these will be defined in distribution-specific\n        subclasses.\n\n        Args:\n            state (dict): current state information.\n            param (str): name of the parameter for which we compute derivatives.\n            hessian_required (bool): flag for whether the Hessian should be calculated and supplied as an output.\n\n        Returns:\n            (Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]): if hessian_required=True, then a tuple of (gradient,\n                hessian) is returned. If hessian_required=False, then just a gradient vector is returned. The returned\n                values are as follows:\n                grad (np.ndarray): vector gradients of the POSITIVE log-pdf with respect to param. shape=(d, 1), where\n                d is the dimensionality of param.\n                hessian (np.ndarray): array of NEGATIVE second derivatives of the log-pdf with respect to param.\n                shape=(d, d), where d is the dimensionality of param.\n\n        \"\"\"\n        grad = self.grad_log_p_diff(state=state, param=param)\n        if hessian_required:\n            hessian = self.hessian_log_p_diff(state=state, param=param)\n            return grad, hessian\n        return grad\n\n    def grad_log_p_diff(self, state: dict, param: str, step_size: float = 1e-4) -&gt; np.ndarray:\n        \"\"\"Compute vector of derivatives of the POSITIVE log-pdf (with respect to param) using central differences.\n\n        Args:\n            state (dict): current state information.\n            param (str): name of the parameter for which we compute derivatives.\n            step_size (float, optional): step size to use for the finite difference derivatives. Defaults to 1e-4.\n\n        Returns:\n            (np.ndarray): vector of log-pdf gradients with respect to param. shape=(d, 1), where d is the dimensionality\n                of param.\n\n        \"\"\"\n        n_param = np.prod(state[param].shape)\n        grad_param = np.full(shape=n_param, fill_value=np.nan)\n        for k in range(n_param):\n            state_plus = deepcopy(state)\n            state_minus = deepcopy(state)\n\n            if sparse.issparse(state[param]):\n                m, n = state[param].shape\n                step_temp = sparse.csr_array(\n                    (np.array([step_size / 2]), np.unravel_index(np.array([k]), (m, n))), shape=(m, n)\n                )\n                state_plus[param] = state_plus[param] + step_temp\n                state_minus[param] = state_minus[param] - step_temp\n            else:\n                state_plus[param][np.unravel_index(k, state[param].shape)] += step_size / 2\n                state_minus[param][np.unravel_index(k, state[param].shape)] += -step_size / 2\n\n            log_p_plus = self.log_p(state=state_plus)\n            log_p_minus = self.log_p(state=state_minus)\n\n            grad_param[k] = (log_p_plus - log_p_minus) / step_size\n        return grad_param.reshape(state[param].shape)\n\n    def hessian_log_p_diff(self, state: dict, param: str, step_size: float = 1e-4) -&gt; np.ndarray:\n        \"\"\"Compute Hessian matrix of second derivatives of the NEGATIVE log-pdf (with respect to param) using finite differences.\n\n        Args:\n            state (dict): current state information.\n            param (str): name of the parameter for which we compute derivatives.\n            step_size (float, optional): step size to use for the finite difference derivatives. Defaults to 1e-4\n\n        Returns:\n            (np.ndarray): matrix of log-pdf second derivatives with respect to param. shape=(d, d), where d is the\n                dimensionality of param.\n\n        \"\"\"\n        n_param = np.prod(state[param].shape)\n        hess_param = np.full(shape=(n_param, n_param), fill_value=np.nan)\n        for k in range(n_param):\n            state_plus = deepcopy(state)\n            state_minus = deepcopy(state)\n\n            if sparse.issparse(state[param]):\n                m, n = state[param].shape\n                step_temp = sparse.csr_array(\n                    (np.array([step_size / 2]), np.unravel_index(np.array([k]), (m, n))), shape=(m, n)\n                )\n                state_plus[param] = state_plus[param] + step_temp\n                state_minus[param] = state_minus[param] - step_temp\n            else:\n                state_plus[param][np.unravel_index(k, state[param].shape)] += step_size / 2\n                state_minus[param][np.unravel_index(k, state[param].shape)] += -step_size / 2\n\n            grad_p_plus = self.grad_log_p(state_plus, param, hessian_required=False)\n            grad_p_minus = self.grad_log_p(state_minus, param, hessian_required=False)\n\n            hess_param[:, k] = (grad_p_minus - grad_p_plus).flatten() / step_size\n\n        return hess_param\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Distribution.param_list","title":"<code>param_list: list</code>  <code>property</code>","text":"<p>Get list of all parameter labels in model (INCLUDING the response).</p> <p>Returns:</p> Type Description <code>list</code> <p>list of parameter labels</p>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Distribution.log_p","title":"<code>log_p(state, by_observation=False)</code>  <code>abstractmethod</code>","text":"<p>Compute the log of the probability density (for current parameter settings).</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information. state[distribution.response] is expected to be p x n where: p is the number of responses; n is the number of independent replicates/observations.</p> required <code>by_observation</code> <code>bool</code> <p>If True, the log-likelihood is returned for each of the p responses of the distribution separately. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, float]</code> <p>POSITIVE log-density evaluated using the supplied state dictionary.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>@abstractmethod\ndef log_p(self, state: dict, by_observation: bool = False) -&gt; Union[np.ndarray, float]:\n    \"\"\"Compute the log of the probability density (for current parameter settings).\n\n    Args:\n        state (dict): dictionary object containing the current state information. state[distribution.response]\n            is expected to be p x n where: p is the number of responses; n is the number of independent\n            replicates/observations.\n        by_observation (bool, optional): If True, the log-likelihood is returned for each of the p responses of\n            the distribution separately. Defaults to False.\n\n    Returns:\n        (Union[np.ndarray, float]): POSITIVE log-density evaluated using the supplied state dictionary.\n\n    \"\"\"\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Distribution.rvs","title":"<code>rvs(state, n=1)</code>  <code>abstractmethod</code>","text":"<p>Generate random samples from the distribution.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information.</p> required <code>n</code> <code>int</code> <p>specifies the number of replicate samples required. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>random variables generated from distribution returned as p x n where p is the dimensionality of the response.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>@abstractmethod\ndef rvs(self, state: dict, n: int = 1) -&gt; np.ndarray:\n    \"\"\"Generate random samples from the distribution.\n\n    Args:\n        state (dict): dictionary object containing the current state information.\n        n (int, optional): specifies the number of replicate samples required. Defaults to 1.\n\n    Returns:\n        (np.ndarray): random variables generated from distribution returned as p x n where p is the\n            dimensionality of the response.\n\n    \"\"\"\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Distribution.grad_log_p","title":"<code>grad_log_p(state, param, hessian_required=True)</code>","text":"<p>Generate vector of derivatives of the log-pdf with respect to a given parameter, and if required, also generate the Hessian.</p> <p>Function only defined for scalar- and vector-valued parameters param. If hessian_required=True, this function returns a tuple of (gradient, Hessian). If hessian_required=False, this function returns a np.ndarray (just the gradient of the log-density).</p> <p>As a default, the individual gradients are computed by finite-differencing the log_p function defined for the distribution. Where analytical forms for the gradient exist, these will be defined in distribution-specific subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state information.</p> required <code>param</code> <code>str</code> <p>name of the parameter for which we compute derivatives.</p> required <code>hessian_required</code> <code>bool</code> <p>flag for whether the Hessian should be calculated and supplied as an output.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tuple[ndarray, ndarray]]</code> <p>if hessian_required=True, then a tuple of (gradient, hessian) is returned. If hessian_required=False, then just a gradient vector is returned. The returned values are as follows: grad (np.ndarray): vector gradients of the POSITIVE log-pdf with respect to param. shape=(d, 1), where d is the dimensionality of param. hessian (np.ndarray): array of NEGATIVE second derivatives of the log-pdf with respect to param. shape=(d, d), where d is the dimensionality of param.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def grad_log_p(\n    self, state: dict, param: str, hessian_required: bool = True\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"Generate vector of derivatives of the log-pdf with respect to a given parameter, and if required, also generate the Hessian.\n\n    Function only defined for scalar- and vector-valued parameters param. If hessian_required=True, this function\n    returns a tuple of (gradient, Hessian). If hessian_required=False, this function returns a np.ndarray (just\n    the gradient of the log-density).\n\n    As a default, the individual gradients are computed by finite-differencing the log_p function defined for the\n    distribution. Where analytical forms for the gradient exist, these will be defined in distribution-specific\n    subclasses.\n\n    Args:\n        state (dict): current state information.\n        param (str): name of the parameter for which we compute derivatives.\n        hessian_required (bool): flag for whether the Hessian should be calculated and supplied as an output.\n\n    Returns:\n        (Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]): if hessian_required=True, then a tuple of (gradient,\n            hessian) is returned. If hessian_required=False, then just a gradient vector is returned. The returned\n            values are as follows:\n            grad (np.ndarray): vector gradients of the POSITIVE log-pdf with respect to param. shape=(d, 1), where\n            d is the dimensionality of param.\n            hessian (np.ndarray): array of NEGATIVE second derivatives of the log-pdf with respect to param.\n            shape=(d, d), where d is the dimensionality of param.\n\n    \"\"\"\n    grad = self.grad_log_p_diff(state=state, param=param)\n    if hessian_required:\n        hessian = self.hessian_log_p_diff(state=state, param=param)\n        return grad, hessian\n    return grad\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Distribution.grad_log_p_diff","title":"<code>grad_log_p_diff(state, param, step_size=0.0001)</code>","text":"<p>Compute vector of derivatives of the POSITIVE log-pdf (with respect to param) using central differences.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state information.</p> required <code>param</code> <code>str</code> <p>name of the parameter for which we compute derivatives.</p> required <code>step_size</code> <code>float</code> <p>step size to use for the finite difference derivatives. Defaults to 1e-4.</p> <code>0.0001</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>vector of log-pdf gradients with respect to param. shape=(d, 1), where d is the dimensionality of param.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def grad_log_p_diff(self, state: dict, param: str, step_size: float = 1e-4) -&gt; np.ndarray:\n    \"\"\"Compute vector of derivatives of the POSITIVE log-pdf (with respect to param) using central differences.\n\n    Args:\n        state (dict): current state information.\n        param (str): name of the parameter for which we compute derivatives.\n        step_size (float, optional): step size to use for the finite difference derivatives. Defaults to 1e-4.\n\n    Returns:\n        (np.ndarray): vector of log-pdf gradients with respect to param. shape=(d, 1), where d is the dimensionality\n            of param.\n\n    \"\"\"\n    n_param = np.prod(state[param].shape)\n    grad_param = np.full(shape=n_param, fill_value=np.nan)\n    for k in range(n_param):\n        state_plus = deepcopy(state)\n        state_minus = deepcopy(state)\n\n        if sparse.issparse(state[param]):\n            m, n = state[param].shape\n            step_temp = sparse.csr_array(\n                (np.array([step_size / 2]), np.unravel_index(np.array([k]), (m, n))), shape=(m, n)\n            )\n            state_plus[param] = state_plus[param] + step_temp\n            state_minus[param] = state_minus[param] - step_temp\n        else:\n            state_plus[param][np.unravel_index(k, state[param].shape)] += step_size / 2\n            state_minus[param][np.unravel_index(k, state[param].shape)] += -step_size / 2\n\n        log_p_plus = self.log_p(state=state_plus)\n        log_p_minus = self.log_p(state=state_minus)\n\n        grad_param[k] = (log_p_plus - log_p_minus) / step_size\n    return grad_param.reshape(state[param].shape)\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Distribution.hessian_log_p_diff","title":"<code>hessian_log_p_diff(state, param, step_size=0.0001)</code>","text":"<p>Compute Hessian matrix of second derivatives of the NEGATIVE log-pdf (with respect to param) using finite differences.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state information.</p> required <code>param</code> <code>str</code> <p>name of the parameter for which we compute derivatives.</p> required <code>step_size</code> <code>float</code> <p>step size to use for the finite difference derivatives. Defaults to 1e-4</p> <code>0.0001</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>matrix of log-pdf second derivatives with respect to param. shape=(d, d), where d is the dimensionality of param.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def hessian_log_p_diff(self, state: dict, param: str, step_size: float = 1e-4) -&gt; np.ndarray:\n    \"\"\"Compute Hessian matrix of second derivatives of the NEGATIVE log-pdf (with respect to param) using finite differences.\n\n    Args:\n        state (dict): current state information.\n        param (str): name of the parameter for which we compute derivatives.\n        step_size (float, optional): step size to use for the finite difference derivatives. Defaults to 1e-4\n\n    Returns:\n        (np.ndarray): matrix of log-pdf second derivatives with respect to param. shape=(d, d), where d is the\n            dimensionality of param.\n\n    \"\"\"\n    n_param = np.prod(state[param].shape)\n    hess_param = np.full(shape=(n_param, n_param), fill_value=np.nan)\n    for k in range(n_param):\n        state_plus = deepcopy(state)\n        state_minus = deepcopy(state)\n\n        if sparse.issparse(state[param]):\n            m, n = state[param].shape\n            step_temp = sparse.csr_array(\n                (np.array([step_size / 2]), np.unravel_index(np.array([k]), (m, n))), shape=(m, n)\n            )\n            state_plus[param] = state_plus[param] + step_temp\n            state_minus[param] = state_minus[param] - step_temp\n        else:\n            state_plus[param][np.unravel_index(k, state[param].shape)] += step_size / 2\n            state_minus[param][np.unravel_index(k, state[param].shape)] += -step_size / 2\n\n        grad_p_plus = self.grad_log_p(state_plus, param, hessian_required=False)\n        grad_p_minus = self.grad_log_p(state_minus, param, hessian_required=False)\n\n        hess_param[:, k] = (grad_p_minus - grad_p_plus).flatten() / step_size\n\n    return hess_param\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Gamma","title":"<code>Gamma</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>Gamma distribution class defined using shape and rate convention.</p> <p>f(x) = x^(shape-1) * exp(-rate*x) * rate^shape / Gamma(shape)</p> <p>Attributes:</p> Name Type Description <code>shape</code> <code>Union[str, Identity, LinearCombination, MixtureParameterVector]</code> <p>Gamma shape parameter.</p> <code>rate</code> <code>Union[str, Identity, LinearCombination, MixtureParameterVector]</code> <p>Gamma rate parameter.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>@dataclass\nclass Gamma(Distribution):\n    \"\"\"Gamma distribution class defined using shape and rate convention.\n\n    f(x) = x^(shape-1) * exp(-rate*x) * rate^shape / Gamma(shape)\n\n    Attributes:\n        shape (Union[str, Identity, LinearCombination, MixtureParameterVector]): Gamma shape parameter.\n        rate (Union[str, Identity, LinearCombination, MixtureParameterVector]): Gamma rate parameter.\n\n    \"\"\"\n\n    shape: Union[str, Identity, LinearCombination, MixtureParameterVector]\n    rate: Union[str, Identity, LinearCombination, MixtureParameterVector]\n\n    def __post_init__(self):\n        \"\"\"Parse any str parameter inputs as Parameter.Identity, and check the parameter types.\"\"\"\n        if isinstance(self.shape, str):\n            self.shape = Identity(self.shape)\n\n        if not isinstance(self.shape, (Identity, LinearCombination, MixtureParameterVector)):\n            raise TypeError(\"shape expected to be one of [Identity, LinearCombination, MixtureParameterVector]\")\n\n        if isinstance(self.rate, str):\n            self.rate = Identity(self.rate)\n\n        if not isinstance(self.rate, (Identity, LinearCombination, MixtureParameterVector)):\n            raise TypeError(\"rate expected to be one of [Identity, LinearCombination, MixtureParameterVector]\")\n\n    @property\n    def _dist_params(self) -&gt; list:\n        \"\"\"Get list of parameter labels across all Parameter objects in distribution (EXCLUDING the response).\n\n        Returns:\n            (list): list of parameter labels.\n\n        \"\"\"\n        lst = self.shape.get_param_list() + self.rate.get_param_list()\n        return lst\n\n    def log_p(self, state: dict, by_observation: bool = False) -&gt; Union[np.ndarray, float]:\n        \"\"\"Compute the log of the probability density (for current parameter settings).\n\n        Args:\n            state (dict): dictionary object containing the current state information. state[distribution.response]\n                is expected to be p x n where: p is the number of parameters; n is the number of independent\n                replicates/observations.\n            by_observation (bool, optional): If True, the log-likelihood is returned for each of the p parameters of\n                the distribution separately. Defaults to False.\n\n        Returns:\n            (Union[np.ndarray, float]): POSITIVE log-density evaluated using the supplied state dictionary.\n\n        \"\"\"\n        log_p = np.sum(\n            stats.gamma.logpdf(state[self.response], self.shape.predictor(state), scale=1 / self.rate.predictor(state)),\n            axis=0,\n        )\n        if not by_observation:\n            log_p = np.sum(log_p)\n        return log_p\n\n    def rvs(self, state, n: int = 1) -&gt; np.ndarray:\n        \"\"\"Generate random samples from the Gamma distribution.\n\n        Args:\n            state (dict): dictionary object containing the current state information.\n            n (int, optional): specifies the number of replicate samples required. Defaults to 1.\n\n        Returns:\n            (np.ndarray): random variables generated from distribution returned as p x n where p is the\n                dimensionality of the response.\n\n        \"\"\"\n        shape = self.shape.predictor(state)\n        rate = self.rate.predictor(state)\n        p = max(shape.shape[0], rate.shape[0])\n        return stats.gamma.rvs(shape, scale=1 / rate, size=(p, n))\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Gamma.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Parse any str parameter inputs as Parameter.Identity, and check the parameter types.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Parse any str parameter inputs as Parameter.Identity, and check the parameter types.\"\"\"\n    if isinstance(self.shape, str):\n        self.shape = Identity(self.shape)\n\n    if not isinstance(self.shape, (Identity, LinearCombination, MixtureParameterVector)):\n        raise TypeError(\"shape expected to be one of [Identity, LinearCombination, MixtureParameterVector]\")\n\n    if isinstance(self.rate, str):\n        self.rate = Identity(self.rate)\n\n    if not isinstance(self.rate, (Identity, LinearCombination, MixtureParameterVector)):\n        raise TypeError(\"rate expected to be one of [Identity, LinearCombination, MixtureParameterVector]\")\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Gamma.log_p","title":"<code>log_p(state, by_observation=False)</code>","text":"<p>Compute the log of the probability density (for current parameter settings).</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information. state[distribution.response] is expected to be p x n where: p is the number of parameters; n is the number of independent replicates/observations.</p> required <code>by_observation</code> <code>bool</code> <p>If True, the log-likelihood is returned for each of the p parameters of the distribution separately. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, float]</code> <p>POSITIVE log-density evaluated using the supplied state dictionary.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def log_p(self, state: dict, by_observation: bool = False) -&gt; Union[np.ndarray, float]:\n    \"\"\"Compute the log of the probability density (for current parameter settings).\n\n    Args:\n        state (dict): dictionary object containing the current state information. state[distribution.response]\n            is expected to be p x n where: p is the number of parameters; n is the number of independent\n            replicates/observations.\n        by_observation (bool, optional): If True, the log-likelihood is returned for each of the p parameters of\n            the distribution separately. Defaults to False.\n\n    Returns:\n        (Union[np.ndarray, float]): POSITIVE log-density evaluated using the supplied state dictionary.\n\n    \"\"\"\n    log_p = np.sum(\n        stats.gamma.logpdf(state[self.response], self.shape.predictor(state), scale=1 / self.rate.predictor(state)),\n        axis=0,\n    )\n    if not by_observation:\n        log_p = np.sum(log_p)\n    return log_p\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Gamma.rvs","title":"<code>rvs(state, n=1)</code>","text":"<p>Generate random samples from the Gamma distribution.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information.</p> required <code>n</code> <code>int</code> <p>specifies the number of replicate samples required. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>random variables generated from distribution returned as p x n where p is the dimensionality of the response.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def rvs(self, state, n: int = 1) -&gt; np.ndarray:\n    \"\"\"Generate random samples from the Gamma distribution.\n\n    Args:\n        state (dict): dictionary object containing the current state information.\n        n (int, optional): specifies the number of replicate samples required. Defaults to 1.\n\n    Returns:\n        (np.ndarray): random variables generated from distribution returned as p x n where p is the\n            dimensionality of the response.\n\n    \"\"\"\n    shape = self.shape.predictor(state)\n    rate = self.rate.predictor(state)\n    p = max(shape.shape[0], rate.shape[0])\n    return stats.gamma.rvs(shape, scale=1 / rate, size=(p, n))\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Categorical","title":"<code>Categorical</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>Categorical distribution: equivalent to a single trial of a multinomial distribution.</p> <p>A 2-category categorical distribution is equivalent to a Bernoulli distribution.</p> <p>The response of this distribution is a category index in {0, 1, 2,..., n_cat}: thus, state[self.response] is expected to be a np.array with dtype=int. As per other distributions, the expected shape of state[self.response] is (p, n), where p=dimensionality of response, and n=number of replicates.</p> <p>The prior probability parameter is expected to be a np.ndarray with shape=(p, n_cat).</p> <p>Attributes:</p> Name Type Description <code>prob</code> <code>(Identity, str)</code> <p>allocation probability parameter.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>@dataclass\nclass Categorical(Distribution):\n    \"\"\"Categorical distribution: equivalent to a single trial of a multinomial distribution.\n\n    A 2-category categorical distribution is equivalent to a Bernoulli distribution.\n\n    The response of this distribution is a category index in {0, 1, 2,..., n_cat}: thus, state[self.response] is\n    expected to be a np.array with dtype=int. As per other distributions, the expected shape of state[self.response]\n    is (p, n), where p=dimensionality of response, and n=number of replicates.\n\n    The prior probability parameter is expected to be a np.ndarray with shape=(p, n_cat).\n\n    Attributes:\n        prob (Identity, str): allocation probability parameter.\n\n    \"\"\"\n\n    prob: Union[str, Identity]\n\n    def __post_init__(self):\n        \"\"\"Parse any str parameter inputs as Parameter.Identity(), and check the parameter types.\"\"\"\n        if isinstance(self.prob, str):\n            self.prob = Identity(self.prob)\n\n        if not isinstance(self.prob, Identity):\n            raise TypeError(\"prob expected to be Identity\")\n\n    @property\n    def _dist_params(self) -&gt; list:\n        \"\"\"Get list of parameter labels across all Parameter objects in distribution (EXCLUDING the response).\n\n        Returns:\n            (list): list of parameter labels.\n\n        \"\"\"\n        return self.prob.get_param_list()\n\n    def log_p(self, state: dict, by_observation: bool = False) -&gt; np.ndarray:\n        \"\"\"Compute the log of the probability density (for current parameter settings).\n\n        Args:\n            state (dict): dictionary object containing the current state information. state[distribution.response]\n                is expected to be p x n where: p is the number of responses; n is the number of independent\n                replicates/observations.\n            by_observation (bool, optional): If True, the log-likelihood is returned for each of the p responses of\n                the distribution separately. Defaults to False.\n\n        Returns:\n            (Union[np.ndarray, float]): POSITIVE log-density evaluated using the supplied state dictionary.\n\n        \"\"\"\n        n_categories = self.prob.predictor(state).shape[1]\n        n = state[self.response].shape[1]\n\n        if n &gt; 1:\n            x = np.atleast_3d(state[self.response])\n            x = np.equal(np.transpose(x, (0, 2, 1)), np.atleast_3d(range(n_categories)))\n        else:\n            x = state[self.response] == range(n_categories)\n\n        if by_observation:\n            if n &gt; 1:\n                prob = np.transpose(np.atleast_3d(self.prob.predictor(state)), (0, 2, 1))\n                log_p = stats.multinomial.logpmf(np.transpose(x, (0, 2, 1)), n=1, p=prob)\n            else:\n                log_p = stats.multinomial.logpmf(x, n=1, p=self.prob.predictor(state))\n        else:\n            if n &gt; 1:\n                x = np.sum(x, axis=2)\n            log_p = stats.multinomial.logpmf(x, n=n, p=self.prob.predictor(state))\n\n        return np.sum(log_p, axis=0)\n\n    def rvs(self, state, n: int = 1) -&gt; np.ndarray:\n        \"\"\"Generate a random sample from the distribution.\n\n        Args:\n            state (dict): dictionary object containing the current state information\n            n (int, optional): specifies the number of random variables required. Defaults to 1\n\n        Returns:\n            (np.ndarray): random sample from the categorical distribution. shape=(p, n)\n\n        \"\"\"\n        prob = self.prob.predictor(state)\n\n        d, _ = prob.shape\n\n        cat = np.empty((d, n))\n        for i in range(d):\n            Z = stats.multinomial.rvs(n=1, p=prob[i, :], size=n)\n            _, cat[i, :] = np.nonzero(Z)\n\n        return cat\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Categorical.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Parse any str parameter inputs as Parameter.Identity(), and check the parameter types.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Parse any str parameter inputs as Parameter.Identity(), and check the parameter types.\"\"\"\n    if isinstance(self.prob, str):\n        self.prob = Identity(self.prob)\n\n    if not isinstance(self.prob, Identity):\n        raise TypeError(\"prob expected to be Identity\")\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Categorical.log_p","title":"<code>log_p(state, by_observation=False)</code>","text":"<p>Compute the log of the probability density (for current parameter settings).</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information. state[distribution.response] is expected to be p x n where: p is the number of responses; n is the number of independent replicates/observations.</p> required <code>by_observation</code> <code>bool</code> <p>If True, the log-likelihood is returned for each of the p responses of the distribution separately. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, float]</code> <p>POSITIVE log-density evaluated using the supplied state dictionary.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def log_p(self, state: dict, by_observation: bool = False) -&gt; np.ndarray:\n    \"\"\"Compute the log of the probability density (for current parameter settings).\n\n    Args:\n        state (dict): dictionary object containing the current state information. state[distribution.response]\n            is expected to be p x n where: p is the number of responses; n is the number of independent\n            replicates/observations.\n        by_observation (bool, optional): If True, the log-likelihood is returned for each of the p responses of\n            the distribution separately. Defaults to False.\n\n    Returns:\n        (Union[np.ndarray, float]): POSITIVE log-density evaluated using the supplied state dictionary.\n\n    \"\"\"\n    n_categories = self.prob.predictor(state).shape[1]\n    n = state[self.response].shape[1]\n\n    if n &gt; 1:\n        x = np.atleast_3d(state[self.response])\n        x = np.equal(np.transpose(x, (0, 2, 1)), np.atleast_3d(range(n_categories)))\n    else:\n        x = state[self.response] == range(n_categories)\n\n    if by_observation:\n        if n &gt; 1:\n            prob = np.transpose(np.atleast_3d(self.prob.predictor(state)), (0, 2, 1))\n            log_p = stats.multinomial.logpmf(np.transpose(x, (0, 2, 1)), n=1, p=prob)\n        else:\n            log_p = stats.multinomial.logpmf(x, n=1, p=self.prob.predictor(state))\n    else:\n        if n &gt; 1:\n            x = np.sum(x, axis=2)\n        log_p = stats.multinomial.logpmf(x, n=n, p=self.prob.predictor(state))\n\n    return np.sum(log_p, axis=0)\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Categorical.rvs","title":"<code>rvs(state, n=1)</code>","text":"<p>Generate a random sample from the distribution.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information</p> required <code>n</code> <code>int</code> <p>specifies the number of random variables required. Defaults to 1</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>random sample from the categorical distribution. shape=(p, n)</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def rvs(self, state, n: int = 1) -&gt; np.ndarray:\n    \"\"\"Generate a random sample from the distribution.\n\n    Args:\n        state (dict): dictionary object containing the current state information\n        n (int, optional): specifies the number of random variables required. Defaults to 1\n\n    Returns:\n        (np.ndarray): random sample from the categorical distribution. shape=(p, n)\n\n    \"\"\"\n    prob = self.prob.predictor(state)\n\n    d, _ = prob.shape\n\n    cat = np.empty((d, n))\n    for i in range(d):\n        Z = stats.multinomial.rvs(n=1, p=prob[i, :], size=n)\n        _, cat[i, :] = np.nonzero(Z)\n\n    return cat\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Uniform","title":"<code>Uniform</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>Uniform distribution class for a p-dimensional hyper-rectangle.</p> <p>Attributes:</p> Name Type Description <code>domain_response_lower</code> <code>Union[float, ndarray]</code> <p>shape=(p, 1): lower limits for uniform distribution in each dimension. Defaults to 0.0.</p> <code>domain_response_upper</code> <code>Union[float, ndarray]</code> <p>shape=(p, 1) upper limits for uniform distribution in each dimension. Defaults to 1.0.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>@dataclass\nclass Uniform(Distribution):\n    \"\"\"Uniform distribution class for a p-dimensional hyper-rectangle.\n\n    Attributes:\n        domain_response_lower (Union[float, np.ndarray]): shape=(p, 1): lower limits for uniform distribution in each\n            dimension. Defaults to 0.0.\n        domain_response_upper (Union[float, np.ndarray]): shape=(p, 1) upper limits for uniform distribution in each\n            dimension. Defaults to 1.0.\n\n    \"\"\"\n\n    domain_response_lower: Union[float, np.ndarray] = 0.0\n    domain_response_upper: Union[float, np.ndarray] = 1.0\n\n    def __post_init__(self):\n        \"\"\"Convert any domain limits supplied as floats to np.ndarray.\"\"\"\n        self.domain_response_lower = np.array(self.domain_response_lower, ndmin=2)\n        if self.domain_response_lower.shape[0] == 1:\n            self.domain_response_lower = self.domain_response_lower.T\n        self.domain_response_upper = np.array(self.domain_response_upper, ndmin=2)\n        if self.domain_response_upper.shape[0] == 1:\n            self.domain_response_upper = self.domain_response_upper.T\n\n    @property\n    def _dist_params(self) -&gt; list:\n        \"\"\"Uniform distribution doesn't have parameters, so return an empty list.\"\"\"\n        return []\n\n    def domain_range(self, state) -&gt; np.ndarray:\n        \"\"\"Get the domain range (upper-lower) from domain_limits.\n\n        Args:\n            state (dict): dictionary with current state information.\n\n        Returns:\n            (np.ndarray): domain range. shape=(p, 1).\n\n        \"\"\"\n        d = state[self.response].shape[0]\n        domain_range = self.domain_response_upper - self.domain_response_lower\n        if domain_range.size == 1:\n            domain_range = np.ones((d, 1)) * domain_range\n        return domain_range\n\n    def log_p(self, state: dict, by_observation: bool = False) -&gt; Union[np.ndarray, float]:\n        \"\"\"Compute the log of the probability density (for current parameter settings).\n\n        Args:\n            state (dict): dictionary object containing the current state information. state[distribution.response]\n                is expected to be p x n where: p is the number of responses; n is the number of independent\n                replicates/observations.\n            by_observation (bool, optional): If True, the log-likelihood is returned for each of the p responses of\n                the distribution separately. Defaults to False.\n\n        Returns:\n            (Union[np.ndarray, float]): POSITIVE log-density evaluated using the supplied state dictionary.\n\n        \"\"\"\n        n = state[self.response].shape[1]\n        log_p = -np.sum(np.log(self.domain_range(state)))\n        if by_observation:\n            log_p = np.ones(n) * log_p\n        else:\n            log_p = n * log_p\n        return log_p\n\n    def rvs(self, state, n: int = 1) -&gt; np.ndarray:\n        \"\"\"Generate random samples from the distribution.\n\n        Args:\n            state (dict): dictionary object containing the current state information.\n            n (int, optional): specifies the number of replicate samples required. Defaults to 1.\n\n        Returns:\n            (np.ndarray): random variables generated from distribution returned as p x n where p is the\n                dimensionality of the response.\n\n        \"\"\"\n        standard_unif = np.random.rand(state[self.response].shape[0], n)\n        return self.domain_response_lower + self.domain_range(state) * standard_unif\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Uniform.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Convert any domain limits supplied as floats to np.ndarray.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Convert any domain limits supplied as floats to np.ndarray.\"\"\"\n    self.domain_response_lower = np.array(self.domain_response_lower, ndmin=2)\n    if self.domain_response_lower.shape[0] == 1:\n        self.domain_response_lower = self.domain_response_lower.T\n    self.domain_response_upper = np.array(self.domain_response_upper, ndmin=2)\n    if self.domain_response_upper.shape[0] == 1:\n        self.domain_response_upper = self.domain_response_upper.T\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Uniform.domain_range","title":"<code>domain_range(state)</code>","text":"<p>Get the domain range (upper-lower) from domain_limits.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary with current state information.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>domain range. shape=(p, 1).</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def domain_range(self, state) -&gt; np.ndarray:\n    \"\"\"Get the domain range (upper-lower) from domain_limits.\n\n    Args:\n        state (dict): dictionary with current state information.\n\n    Returns:\n        (np.ndarray): domain range. shape=(p, 1).\n\n    \"\"\"\n    d = state[self.response].shape[0]\n    domain_range = self.domain_response_upper - self.domain_response_lower\n    if domain_range.size == 1:\n        domain_range = np.ones((d, 1)) * domain_range\n    return domain_range\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Uniform.log_p","title":"<code>log_p(state, by_observation=False)</code>","text":"<p>Compute the log of the probability density (for current parameter settings).</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information. state[distribution.response] is expected to be p x n where: p is the number of responses; n is the number of independent replicates/observations.</p> required <code>by_observation</code> <code>bool</code> <p>If True, the log-likelihood is returned for each of the p responses of the distribution separately. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, float]</code> <p>POSITIVE log-density evaluated using the supplied state dictionary.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def log_p(self, state: dict, by_observation: bool = False) -&gt; Union[np.ndarray, float]:\n    \"\"\"Compute the log of the probability density (for current parameter settings).\n\n    Args:\n        state (dict): dictionary object containing the current state information. state[distribution.response]\n            is expected to be p x n where: p is the number of responses; n is the number of independent\n            replicates/observations.\n        by_observation (bool, optional): If True, the log-likelihood is returned for each of the p responses of\n            the distribution separately. Defaults to False.\n\n    Returns:\n        (Union[np.ndarray, float]): POSITIVE log-density evaluated using the supplied state dictionary.\n\n    \"\"\"\n    n = state[self.response].shape[1]\n    log_p = -np.sum(np.log(self.domain_range(state)))\n    if by_observation:\n        log_p = np.ones(n) * log_p\n    else:\n        log_p = n * log_p\n    return log_p\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Uniform.rvs","title":"<code>rvs(state, n=1)</code>","text":"<p>Generate random samples from the distribution.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information.</p> required <code>n</code> <code>int</code> <p>specifies the number of replicate samples required. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>random variables generated from distribution returned as p x n where p is the dimensionality of the response.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def rvs(self, state, n: int = 1) -&gt; np.ndarray:\n    \"\"\"Generate random samples from the distribution.\n\n    Args:\n        state (dict): dictionary object containing the current state information.\n        n (int, optional): specifies the number of replicate samples required. Defaults to 1.\n\n    Returns:\n        (np.ndarray): random variables generated from distribution returned as p x n where p is the\n            dimensionality of the response.\n\n    \"\"\"\n    standard_unif = np.random.rand(state[self.response].shape[0], n)\n    return self.domain_response_lower + self.domain_range(state) * standard_unif\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Poisson","title":"<code>Poisson</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>Poisson distribution for count data.</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>Union[str, Identity, LinearCombination, MixtureParameterVector]</code> <p>Poisson rate parameter.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>@dataclass\nclass Poisson(Distribution):\n    \"\"\"Poisson distribution for count data.\n\n    Attributes:\n        rate (Union[str, Identity, LinearCombination, MixtureParameterVector]): Poisson rate parameter.\n\n    \"\"\"\n\n    rate: Union[str, Identity, LinearCombination, MixtureParameterVector]\n\n    def __post_init__(self):\n        \"\"\"Parse any str parameter inputs as Parameter.Identity, and check the parameter types.\"\"\"\n        if isinstance(self.rate, str):\n            self.rate = Identity(self.rate)\n\n        if not isinstance(self.rate, (Identity, LinearCombination, MixtureParameterVector)):\n            raise TypeError(\"rate expected to be one of [Identity, LinearCombination, MixtureParameterVector]\")\n\n    @property\n    def _dist_params(self) -&gt; list:\n        \"\"\"Get list of parameter labels across all Parameter objects in distribution (EXCLUDING the response).\n\n        Returns:\n            (list): list of parameter labels.\n\n        \"\"\"\n        return self.rate.get_param_list()\n\n    def log_p(self, state: dict, by_observation: bool = False) -&gt; np.ndarray:\n        \"\"\"Compute the log of the probability density (for current parameter settings).\n\n        Args:\n            state (dict): dictionary object containing the current state information. state[distribution.response]\n                is expected to be p x n where: p is the number of parameters; n is the number of independent\n                replicates/observations.\n            by_observation (bool, optional): If True, the log-likelihood is returned for each of the p parameters of\n                the distribution separately. Defaults to False.\n\n        Returns:\n            (Union[np.ndarray, float]): POSITIVE log-density evaluated using the supplied state dictionary.\n\n        \"\"\"\n        rate = self.rate.predictor(state)\n        logpmf = np.sum(stats.poisson.logpmf(state[self.response], rate), axis=0)\n        if not by_observation:\n            logpmf = np.sum(logpmf)\n        return logpmf\n\n    def rvs(self, state: dict, n: int = 1) -&gt; np.ndarray:\n        \"\"\"Generate random samples from the Poisson distribution.\n\n        Args:\n            state (dict): dictionary object containing the current state information.\n            n (int, optional): specifies the number of replicate samples required. Defaults to 1.\n\n        Returns:\n            (np.ndarray): random variables generated from distribution returned as p x n where p is the\n                dimensionality of the response.\n\n        \"\"\"\n        rate = self.rate.predictor(state)\n        return stats.poisson.rvs(mu=rate, size=(rate.shape[0], n))\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Poisson.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Parse any str parameter inputs as Parameter.Identity, and check the parameter types.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Parse any str parameter inputs as Parameter.Identity, and check the parameter types.\"\"\"\n    if isinstance(self.rate, str):\n        self.rate = Identity(self.rate)\n\n    if not isinstance(self.rate, (Identity, LinearCombination, MixtureParameterVector)):\n        raise TypeError(\"rate expected to be one of [Identity, LinearCombination, MixtureParameterVector]\")\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Poisson.log_p","title":"<code>log_p(state, by_observation=False)</code>","text":"<p>Compute the log of the probability density (for current parameter settings).</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information. state[distribution.response] is expected to be p x n where: p is the number of parameters; n is the number of independent replicates/observations.</p> required <code>by_observation</code> <code>bool</code> <p>If True, the log-likelihood is returned for each of the p parameters of the distribution separately. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, float]</code> <p>POSITIVE log-density evaluated using the supplied state dictionary.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def log_p(self, state: dict, by_observation: bool = False) -&gt; np.ndarray:\n    \"\"\"Compute the log of the probability density (for current parameter settings).\n\n    Args:\n        state (dict): dictionary object containing the current state information. state[distribution.response]\n            is expected to be p x n where: p is the number of parameters; n is the number of independent\n            replicates/observations.\n        by_observation (bool, optional): If True, the log-likelihood is returned for each of the p parameters of\n            the distribution separately. Defaults to False.\n\n    Returns:\n        (Union[np.ndarray, float]): POSITIVE log-density evaluated using the supplied state dictionary.\n\n    \"\"\"\n    rate = self.rate.predictor(state)\n    logpmf = np.sum(stats.poisson.logpmf(state[self.response], rate), axis=0)\n    if not by_observation:\n        logpmf = np.sum(logpmf)\n    return logpmf\n</code></pre>"},{"location":"openmcmc/distribution/distribution/#openmcmc.distribution.distribution.Poisson.rvs","title":"<code>rvs(state, n=1)</code>","text":"<p>Generate random samples from the Poisson distribution.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information.</p> required <code>n</code> <code>int</code> <p>specifies the number of replicate samples required. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>random variables generated from distribution returned as p x n where p is the dimensionality of the response.</p> Source code in <code>src/openmcmc/distribution/distribution.py</code> <pre><code>def rvs(self, state: dict, n: int = 1) -&gt; np.ndarray:\n    \"\"\"Generate random samples from the Poisson distribution.\n\n    Args:\n        state (dict): dictionary object containing the current state information.\n        n (int, optional): specifies the number of replicate samples required. Defaults to 1.\n\n    Returns:\n        (np.ndarray): random variables generated from distribution returned as p x n where p is the\n            dimensionality of the response.\n\n    \"\"\"\n    rate = self.rate.predictor(state)\n    return stats.poisson.rvs(mu=rate, size=(rate.shape[0], n))\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/","title":"Location Scale","text":""},{"location":"openmcmc/distribution/location_scale/#location-scale","title":"Location Scale","text":"<p>LocationScale module.</p> <p>This module provides a class definition of the LocationScale class an abstract base class for distributions defined by a mean and a precision such as the Normal and Lognormal.</p>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.LocationScale","title":"<code>LocationScale</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Distribution</code>, <code>ABC</code></p> <p>Abstract base class for distributions defined by a mean and a precision such as the Normal and Lognormal.</p> <p>Attributes:</p> Name Type Description <code>mean</code> <code>Union[str, Identity, LinearCombination, MixtureParameterVector]</code> <p>mean parameter (of class Parameter).</p> <code>precision</code> <code>Union[str, Identity, ScaledMatrix, MixtureParameterMatrix]</code> <p>precision parameter (of class Parameter).</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>@dataclass\nclass LocationScale(Distribution, ABC):\n    \"\"\"Abstract base class for distributions defined by a mean and a precision such as the Normal and Lognormal.\n\n    Attributes:\n       mean (Union[str, Identity, LinearCombination, MixtureParameterVector]): mean parameter (of class Parameter).\n       precision (Union[str, Identity, ScaledMatrix, MixtureParameterMatrix]): precision parameter (of class Parameter).\n\n    \"\"\"\n\n    mean: Union[str, Identity, LinearCombination, MixtureParameterVector]\n    precision: Union[str, Identity, ScaledMatrix, MixtureParameterMatrix]\n\n    @property\n    def _dist_params(self) -&gt; list:\n        \"\"\"Return the full list of state elements used in the mean and precision parameters.\"\"\"\n        lst = self.mean.get_param_list() + self.precision.get_param_list()\n        return lst\n\n    def __post_init__(self):\n        \"\"\"Parse any str parameter inputs as Parameter classes.\"\"\"\n        if isinstance(self.mean, str):\n            self.mean = Identity(self.mean)\n\n        if not isinstance(self.mean, (Identity, LinearCombination, MixtureParameterVector)):\n            raise TypeError(\"mean expected to be one of [Identity, LinearCombination, MixtureParameterVector]\")\n\n        if isinstance(self.precision, str):\n            self.precision = Identity(self.precision)\n\n        if not isinstance(self.precision, (Identity, ScaledMatrix, MixtureParameterMatrix)):\n            raise TypeError(\"precision expected to be one of [Identity, ScaledMatrix, MixtureParameterMatrix]\")\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.LocationScale.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Parse any str parameter inputs as Parameter classes.</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Parse any str parameter inputs as Parameter classes.\"\"\"\n    if isinstance(self.mean, str):\n        self.mean = Identity(self.mean)\n\n    if not isinstance(self.mean, (Identity, LinearCombination, MixtureParameterVector)):\n        raise TypeError(\"mean expected to be one of [Identity, LinearCombination, MixtureParameterVector]\")\n\n    if isinstance(self.precision, str):\n        self.precision = Identity(self.precision)\n\n    if not isinstance(self.precision, (Identity, ScaledMatrix, MixtureParameterMatrix)):\n        raise TypeError(\"precision expected to be one of [Identity, ScaledMatrix, MixtureParameterMatrix]\")\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.NullDistribution","title":"<code>NullDistribution</code>  <code>dataclass</code>","text":"<p>               Bases: <code>LocationScale</code></p> <p>Null distribution, which returns 0 for the log-likelihood, a zero vector for the gradient and a zero matrix for the Hessian.</p> <p>Used in prior recovery testing for reversible jump sampler.</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>class NullDistribution(LocationScale):\n    \"\"\"Null distribution, which returns 0 for the log-likelihood, a zero vector for the gradient and a zero matrix for the Hessian.\n\n    Used in prior recovery testing for reversible jump sampler.\n\n    \"\"\"\n\n    def log_p(self, state: dict, by_observation: bool = False) -&gt; float:\n        \"\"\"Null log-density function: returns 0.\n\n        Args:\n            state (dict): dictionary object containing the current state information. state[distribution.response]\n                is expected to be p x n where: p is the number of responses; n is the number of independent\n                replicates/observations.\n            by_observation (bool, optional): If True, the log-likelihood is returned for each of the p responses of\n                the distribution separately. Defaults to False.\n\n        Returns:\n            (float): 0.0.\n\n        \"\"\"\n        return 0.0\n\n    def grad_log_p(\n        self, state: dict, param: str, hessian_required: bool = True\n    ) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"Null gradient function returning an all-zero vector for the gradient, and an all-zero matrix for the Hessian.\n\n        Args:\n            state (dict): current state information.\n            param (str): name of the parameter for which we compute derivatives.\n            hessian_required (bool): flag for whether the Hessian should be calculated and supplied as an output.\n\n        Returns:\n            (Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]): if hessian_required=True, then a tuple of (gradient,\n                hessian) is returned. If hessian_required=False, then just a gradient vector is returned. The returned\n                values are as follows:\n                grad (np.ndarray): all-zero vector. shape=(d, 1), where d is the dimensionality of param.\n                hessian (np.ndarray): all-zero matrix. shape=(d, d), where d is the dimensionality of param.\n\n        \"\"\"\n        if hessian_required:\n            return np.zeros(state[param].shape), np.zeros((state[param].shape[0], state[param].shape[0]))\n\n        return np.zeros(state[param].shape)\n\n    def rvs(self, state: dict, n: int = 1) -&gt; None:\n        \"\"\"Null random sampling function.\n\n        Args:\n            state (dict): dictionary object containing the current state information.\n            n (int, optional): specifies the number of replicate samples required. Defaults to 1.\n\n        Returns:\n            (None): simply returns None value.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.NullDistribution.log_p","title":"<code>log_p(state, by_observation=False)</code>","text":"<p>Null log-density function: returns 0.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information. state[distribution.response] is expected to be p x n where: p is the number of responses; n is the number of independent replicates/observations.</p> required <code>by_observation</code> <code>bool</code> <p>If True, the log-likelihood is returned for each of the p responses of the distribution separately. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>0.0.</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>def log_p(self, state: dict, by_observation: bool = False) -&gt; float:\n    \"\"\"Null log-density function: returns 0.\n\n    Args:\n        state (dict): dictionary object containing the current state information. state[distribution.response]\n            is expected to be p x n where: p is the number of responses; n is the number of independent\n            replicates/observations.\n        by_observation (bool, optional): If True, the log-likelihood is returned for each of the p responses of\n            the distribution separately. Defaults to False.\n\n    Returns:\n        (float): 0.0.\n\n    \"\"\"\n    return 0.0\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.NullDistribution.grad_log_p","title":"<code>grad_log_p(state, param, hessian_required=True)</code>","text":"<p>Null gradient function returning an all-zero vector for the gradient, and an all-zero matrix for the Hessian.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state information.</p> required <code>param</code> <code>str</code> <p>name of the parameter for which we compute derivatives.</p> required <code>hessian_required</code> <code>bool</code> <p>flag for whether the Hessian should be calculated and supplied as an output.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tuple[ndarray, ndarray]]</code> <p>if hessian_required=True, then a tuple of (gradient, hessian) is returned. If hessian_required=False, then just a gradient vector is returned. The returned values are as follows: grad (np.ndarray): all-zero vector. shape=(d, 1), where d is the dimensionality of param. hessian (np.ndarray): all-zero matrix. shape=(d, d), where d is the dimensionality of param.</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>def grad_log_p(\n    self, state: dict, param: str, hessian_required: bool = True\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"Null gradient function returning an all-zero vector for the gradient, and an all-zero matrix for the Hessian.\n\n    Args:\n        state (dict): current state information.\n        param (str): name of the parameter for which we compute derivatives.\n        hessian_required (bool): flag for whether the Hessian should be calculated and supplied as an output.\n\n    Returns:\n        (Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]): if hessian_required=True, then a tuple of (gradient,\n            hessian) is returned. If hessian_required=False, then just a gradient vector is returned. The returned\n            values are as follows:\n            grad (np.ndarray): all-zero vector. shape=(d, 1), where d is the dimensionality of param.\n            hessian (np.ndarray): all-zero matrix. shape=(d, d), where d is the dimensionality of param.\n\n    \"\"\"\n    if hessian_required:\n        return np.zeros(state[param].shape), np.zeros((state[param].shape[0], state[param].shape[0]))\n\n    return np.zeros(state[param].shape)\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.NullDistribution.rvs","title":"<code>rvs(state, n=1)</code>","text":"<p>Null random sampling function.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information.</p> required <code>n</code> <code>int</code> <p>specifies the number of replicate samples required. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>None</code> <p>simply returns None value.</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>def rvs(self, state: dict, n: int = 1) -&gt; None:\n    \"\"\"Null random sampling function.\n\n    Args:\n        state (dict): dictionary object containing the current state information.\n        n (int, optional): specifies the number of replicate samples required. Defaults to 1.\n\n    Returns:\n        (None): simply returns None value.\n\n    \"\"\"\n    return None\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.Normal","title":"<code>Normal</code>  <code>dataclass</code>","text":"<p>               Bases: <code>LocationScale</code></p> <p>Multivariate normal distribution class.</p> <p>Supports both standard multivariate normal and truncated normal distribution cases. By default, no truncation is assumed. To truncate the distribution, one or both of self.domain_response_lower or self.domain_response_upper must be specified.</p> <p>Attributes:</p> Name Type Description <code>domain_response_lower</code> <code>array</code> <p>check lower bound domain to implement truncated sampling. Defaults to None.</p> <code>domain_response_upper</code> <code>array</code> <p>check upper bound domain to implement truncated sampling. Defaults to None.</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>@dataclass\nclass Normal(LocationScale):\n    \"\"\"Multivariate normal distribution class.\n\n    Supports both standard multivariate normal and truncated normal distribution cases. By default, no truncation is\n    assumed. To truncate the distribution, one or both of self.domain_response_lower or self.domain_response_upper must\n    be specified.\n\n    Attributes:\n        domain_response_lower (np.array, optional): check lower bound domain to implement truncated sampling. Defaults\n            to None.\n        domain_response_upper (np.array, optional): check upper bound domain to implement truncated sampling. Defaults\n            to None.\n\n    \"\"\"\n\n    domain_response_lower: np.ndarray = None\n    domain_response_upper: np.ndarray = None\n\n    def log_p(self, state: dict, by_observation: bool = False) -&gt; Union[np.ndarray, float]:\n        \"\"\"Compute the log of the probability density for a given state.\n\n        NOTE: This function simply computes the non-truncated Gaussian density: i.e. the extra normalization for the\n        truncation is NOT accounted for. Relative densities (differences of log-probabilities) are still valid when\n        comparing different response parameter values (with fixed mean and precision parameter values). Comparisons\n        for different mean or precision parameters are not valid, since such changes would affect the normalization.\n\n        Args:\n            state (dict): dictionary object containing the current parameter information.\n            by_observation (bool, optional): indicates whether log-density should be computed for each individual\n                response in the distribution. Defaults to False (i.e. the overall log-density is computed).\n\n        Returns:\n            (Union[np.ndarray, float]): log-density computed using the values in state.\n\n        \"\"\"\n        Q = self.precision.predictor(state)\n        mu = self.mean.predictor(state)\n        if self.check_domain_response(state):\n            return -np.inf\n        log_p = gmrf.multivariate_normal_pdf(x=state[self.response], mu=mu, Q=Q, by_observation=by_observation)\n        return log_p\n\n    def check_domain_response(self, state: dict) -&gt; bool:\n        \"\"\"Checks whether the distributional response lies OUTSIDE the defined limits.\n\n        Returns True if the current value of self.response in the supplied state lies OUTSIDE the stated domain;\n        returns False otherwise.\n\n        Args:\n            state (dict): dictionary object containing the current parameter information.\n\n        Returns:\n            (bool): True when the response lies OUTSIDE the valid response domain; False when it lies INSIDE.\n\n        \"\"\"\n        if self.domain_response_lower is not None:\n            if np.any(state[self.response] &lt; self.domain_response_lower):\n                return True\n        if self.domain_response_upper is not None:\n            if np.any(state[self.response] &gt; self.domain_response_upper):\n                return True\n        return False\n\n    def grad_log_p(\n        self, state: dict, param: str, hessian_required: bool = True\n    ) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"Gradient and Hessian of the log-Gaussian density, with respect to a given parameter.\n\n        See also distribution.grad_log_p() for more information.\n\n        Handles three possibilities:\n            1) param is the response of the distribution, in which case the standard gradient of the log-density is\n                returned.\n            2) param is a parameter used in the computation of the mean (through a parameter object) and not in the\n                computation of the precision, in which case the gradient is computed through application of the chain\n                rule. Note that the Hessian calculated in this case is only valid if the dependence of self.mean on\n                param is linear.\n            3) neither of the above conditions is True, in which case the default finite-difference gradient is\n                calculated (using self.grad_log_p_diff() and self.hessian_log_p_diff()). Note that as per those\n                docstrings, it is only possible to compute gradients with respect to scalar or vector parameters.\n\n        Args:\n            state (dict): current state information.\n            param (str): name of the parameter for which we compute derivatives.\n            hessian_required (bool): flag for whether the Hessian should be calculated and supplied as an output.\n\n        Returns:\n            (Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]): if hessian_required=True, then a tuple of (gradient,\n                hessian) is returned. If hessian_required=False, then just a gradient vector is returned. The returned\n                values are as follows:\n                grad (np.ndarray): vector gradients of the POSITIVE log-pdf with respect to param. shape=(n_param, 1)\n                hessian (np.ndarray): array of NEGATIVE second derivatives of the log-pdf with respect to param.\n                shape=(n_param, n_param)\n\n        \"\"\"\n        if param in self.response:\n            Q = self.precision.predictor(state)\n            r = state[self.response] - self.mean.predictor(state)\n            grad = -Q @ r\n            if hessian_required:\n                hessian = Q\n                if state[param].shape[1] &gt; 1 and sparse.issparse(Q):\n                    hessian = sparse.kron(Q, sparse.eye(state[param].shape[1]))\n                elif state[param].shape[1] &gt; 1:\n                    hessian = np.kron(Q, np.eye(state[param].shape[1]))\n                return grad, hessian\n\n        elif param in self.mean.get_grad_param_list() and param not in self.precision.get_grad_param_list():\n            Q = self.precision.predictor(state)\n            r = np.sum(state[self.response] - self.mean.predictor(state), axis=1, keepdims=True)\n            grad_param = self.mean.grad(state, param)\n            grad_times_prec = grad_param @ Q\n            grad = grad_times_prec @ r\n            if hessian_required:\n                hessian = state[self.response].shape[1] * grad_times_prec @ grad_param.T\n                return grad, hessian\n\n        else:\n            grad = self.grad_log_p_diff(state, param)\n            if hessian_required:\n                hessian = self.hessian_log_p_diff(state, param)\n                return grad, hessian\n\n        return grad\n\n    def rvs(self, state: dict, n: int = 1) -&gt; np.ndarray:\n        \"\"\"Generate random samples from the multivariate Gaussian distribution.\n\n        Args:\n            state (dict): dictionary object containing the current state information.\n            n (int, optional): specifies the number of replicate samples required. Defaults to 1.\n\n        Returns:\n            (np.ndarray): random variables generated from distribution returned as p x n where p is the\n                dimensionality of the response.\n\n        \"\"\"\n        mean = self.mean.predictor(state)\n        precision = self.precision.predictor(state)\n\n        if self.domain_response_lower is None and self.domain_response_upper is None:\n            return gmrf.sample_normal(mu=mean, Q=precision, n=n)\n\n        return gmrf.sample_truncated_normal(\n            mu=mean, Q=precision, lower=self.domain_response_lower, upper=self.domain_response_upper, n=n\n        )\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.Normal.log_p","title":"<code>log_p(state, by_observation=False)</code>","text":"<p>Compute the log of the probability density for a given state.</p> <p>NOTE: This function simply computes the non-truncated Gaussian density: i.e. the extra normalization for the truncation is NOT accounted for. Relative densities (differences of log-probabilities) are still valid when comparing different response parameter values (with fixed mean and precision parameter values). Comparisons for different mean or precision parameters are not valid, since such changes would affect the normalization.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current parameter information.</p> required <code>by_observation</code> <code>bool</code> <p>indicates whether log-density should be computed for each individual response in the distribution. Defaults to False (i.e. the overall log-density is computed).</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, float]</code> <p>log-density computed using the values in state.</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>def log_p(self, state: dict, by_observation: bool = False) -&gt; Union[np.ndarray, float]:\n    \"\"\"Compute the log of the probability density for a given state.\n\n    NOTE: This function simply computes the non-truncated Gaussian density: i.e. the extra normalization for the\n    truncation is NOT accounted for. Relative densities (differences of log-probabilities) are still valid when\n    comparing different response parameter values (with fixed mean and precision parameter values). Comparisons\n    for different mean or precision parameters are not valid, since such changes would affect the normalization.\n\n    Args:\n        state (dict): dictionary object containing the current parameter information.\n        by_observation (bool, optional): indicates whether log-density should be computed for each individual\n            response in the distribution. Defaults to False (i.e. the overall log-density is computed).\n\n    Returns:\n        (Union[np.ndarray, float]): log-density computed using the values in state.\n\n    \"\"\"\n    Q = self.precision.predictor(state)\n    mu = self.mean.predictor(state)\n    if self.check_domain_response(state):\n        return -np.inf\n    log_p = gmrf.multivariate_normal_pdf(x=state[self.response], mu=mu, Q=Q, by_observation=by_observation)\n    return log_p\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.Normal.check_domain_response","title":"<code>check_domain_response(state)</code>","text":"<p>Checks whether the distributional response lies OUTSIDE the defined limits.</p> <p>Returns True if the current value of self.response in the supplied state lies OUTSIDE the stated domain; returns False otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current parameter information.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True when the response lies OUTSIDE the valid response domain; False when it lies INSIDE.</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>def check_domain_response(self, state: dict) -&gt; bool:\n    \"\"\"Checks whether the distributional response lies OUTSIDE the defined limits.\n\n    Returns True if the current value of self.response in the supplied state lies OUTSIDE the stated domain;\n    returns False otherwise.\n\n    Args:\n        state (dict): dictionary object containing the current parameter information.\n\n    Returns:\n        (bool): True when the response lies OUTSIDE the valid response domain; False when it lies INSIDE.\n\n    \"\"\"\n    if self.domain_response_lower is not None:\n        if np.any(state[self.response] &lt; self.domain_response_lower):\n            return True\n    if self.domain_response_upper is not None:\n        if np.any(state[self.response] &gt; self.domain_response_upper):\n            return True\n    return False\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.Normal.grad_log_p","title":"<code>grad_log_p(state, param, hessian_required=True)</code>","text":"<p>Gradient and Hessian of the log-Gaussian density, with respect to a given parameter.</p> <p>See also distribution.grad_log_p() for more information.</p> Handles three possibilities <p>1) param is the response of the distribution, in which case the standard gradient of the log-density is     returned. 2) param is a parameter used in the computation of the mean (through a parameter object) and not in the     computation of the precision, in which case the gradient is computed through application of the chain     rule. Note that the Hessian calculated in this case is only valid if the dependence of self.mean on     param is linear. 3) neither of the above conditions is True, in which case the default finite-difference gradient is     calculated (using self.grad_log_p_diff() and self.hessian_log_p_diff()). Note that as per those     docstrings, it is only possible to compute gradients with respect to scalar or vector parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state information.</p> required <code>param</code> <code>str</code> <p>name of the parameter for which we compute derivatives.</p> required <code>hessian_required</code> <code>bool</code> <p>flag for whether the Hessian should be calculated and supplied as an output.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tuple[ndarray, ndarray]]</code> <p>if hessian_required=True, then a tuple of (gradient, hessian) is returned. If hessian_required=False, then just a gradient vector is returned. The returned values are as follows: grad (np.ndarray): vector gradients of the POSITIVE log-pdf with respect to param. shape=(n_param, 1) hessian (np.ndarray): array of NEGATIVE second derivatives of the log-pdf with respect to param. shape=(n_param, n_param)</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>def grad_log_p(\n    self, state: dict, param: str, hessian_required: bool = True\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"Gradient and Hessian of the log-Gaussian density, with respect to a given parameter.\n\n    See also distribution.grad_log_p() for more information.\n\n    Handles three possibilities:\n        1) param is the response of the distribution, in which case the standard gradient of the log-density is\n            returned.\n        2) param is a parameter used in the computation of the mean (through a parameter object) and not in the\n            computation of the precision, in which case the gradient is computed through application of the chain\n            rule. Note that the Hessian calculated in this case is only valid if the dependence of self.mean on\n            param is linear.\n        3) neither of the above conditions is True, in which case the default finite-difference gradient is\n            calculated (using self.grad_log_p_diff() and self.hessian_log_p_diff()). Note that as per those\n            docstrings, it is only possible to compute gradients with respect to scalar or vector parameters.\n\n    Args:\n        state (dict): current state information.\n        param (str): name of the parameter for which we compute derivatives.\n        hessian_required (bool): flag for whether the Hessian should be calculated and supplied as an output.\n\n    Returns:\n        (Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]): if hessian_required=True, then a tuple of (gradient,\n            hessian) is returned. If hessian_required=False, then just a gradient vector is returned. The returned\n            values are as follows:\n            grad (np.ndarray): vector gradients of the POSITIVE log-pdf with respect to param. shape=(n_param, 1)\n            hessian (np.ndarray): array of NEGATIVE second derivatives of the log-pdf with respect to param.\n            shape=(n_param, n_param)\n\n    \"\"\"\n    if param in self.response:\n        Q = self.precision.predictor(state)\n        r = state[self.response] - self.mean.predictor(state)\n        grad = -Q @ r\n        if hessian_required:\n            hessian = Q\n            if state[param].shape[1] &gt; 1 and sparse.issparse(Q):\n                hessian = sparse.kron(Q, sparse.eye(state[param].shape[1]))\n            elif state[param].shape[1] &gt; 1:\n                hessian = np.kron(Q, np.eye(state[param].shape[1]))\n            return grad, hessian\n\n    elif param in self.mean.get_grad_param_list() and param not in self.precision.get_grad_param_list():\n        Q = self.precision.predictor(state)\n        r = np.sum(state[self.response] - self.mean.predictor(state), axis=1, keepdims=True)\n        grad_param = self.mean.grad(state, param)\n        grad_times_prec = grad_param @ Q\n        grad = grad_times_prec @ r\n        if hessian_required:\n            hessian = state[self.response].shape[1] * grad_times_prec @ grad_param.T\n            return grad, hessian\n\n    else:\n        grad = self.grad_log_p_diff(state, param)\n        if hessian_required:\n            hessian = self.hessian_log_p_diff(state, param)\n            return grad, hessian\n\n    return grad\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.Normal.rvs","title":"<code>rvs(state, n=1)</code>","text":"<p>Generate random samples from the multivariate Gaussian distribution.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information.</p> required <code>n</code> <code>int</code> <p>specifies the number of replicate samples required. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>random variables generated from distribution returned as p x n where p is the dimensionality of the response.</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>def rvs(self, state: dict, n: int = 1) -&gt; np.ndarray:\n    \"\"\"Generate random samples from the multivariate Gaussian distribution.\n\n    Args:\n        state (dict): dictionary object containing the current state information.\n        n (int, optional): specifies the number of replicate samples required. Defaults to 1.\n\n    Returns:\n        (np.ndarray): random variables generated from distribution returned as p x n where p is the\n            dimensionality of the response.\n\n    \"\"\"\n    mean = self.mean.predictor(state)\n    precision = self.precision.predictor(state)\n\n    if self.domain_response_lower is None and self.domain_response_upper is None:\n        return gmrf.sample_normal(mu=mean, Q=precision, n=n)\n\n    return gmrf.sample_truncated_normal(\n        mu=mean, Q=precision, lower=self.domain_response_lower, upper=self.domain_response_upper, n=n\n    )\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.LogNormal","title":"<code>LogNormal</code>  <code>dataclass</code>","text":"<p>               Bases: <code>LocationScale</code></p> <p>Multivariate log-normal distribution class.</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>@dataclass\nclass LogNormal(LocationScale):\n    \"\"\"Multivariate log-normal distribution class.\"\"\"\n\n    def log_p(self, state: dict, by_observation: bool = False) -&gt; np.ndarray:\n        \"\"\"Compute the log of the probability density (for current parameter settings).\n\n        Args:\n            state (dict): dictionary object containing the current state information. state[distribution.response]\n                is expected to be p x n where: p is the number of responses; n is the number of independent\n                replicates/observations.\n            by_observation (bool, optional): If True, the log-likelihood is returned for each of the p responses of\n                the distribution separately. Defaults to False.\n\n        Returns:\n            (Union[np.ndarray, float]): POSITIVE log-density evaluated using the supplied state dictionary.\n\n        \"\"\"\n        Q = self.precision.predictor(state)\n        mu = self.mean.predictor(state)\n        log_p = gmrf.multivariate_normal_pdf(x=np.log(state[self.response]), mu=mu, Q=Q, by_observation=True) - np.sum(\n            np.log(state[self.response]), axis=0\n        )\n        if not by_observation:\n            log_p = np.sum(log_p)\n        return log_p\n\n    def grad_log_p(\n        self, state: dict, param: str, hessian_required: bool = True\n    ) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"Generate vector of derivatives of the log-pdf with respect to a given parameter, and if required, also generate the Hessian.\n\n        See also distribution.grad_log_p() for more information.\n\n        Handles 3 possibilities:\n            1) param is the response of the distribution, in which case the standard gradient of the log-density is\n                returned.\n            2) param is a parameter used in the computation of the mean (through a parameter object) and not in the\n                computation of the precision, in which case the gradient is computed through application of the chain\n                rule. Note that the Hessian calculated in this case is only valid if the dependence of self.mean on\n                param is linear.\n            3) neither of the above conditions is True, in which case the default finite-difference gradient is\n                calculated (using self.grad_log_p_diff() and self.hessian_log_p_diff()). Note that as per those\n                docstrings, it is only possible to compute gradients with respect to scalar or vector parameters.\n\n        Args:\n            state (dict): current state information.\n            param (str): name of the parameter for which we compute derivatives.\n            hessian_required (bool): flag for whether the Hessian should be calculated and supplied as an output.\n\n        Returns:\n            (Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]): if hessian_required=True, then a tuple of (gradient,\n                hessian) is returned. If hessian_required=False, then just a gradient vector is returned. The returned\n                values are as follows:\n                grad (np.ndarray): vector gradients of the POSITIVE log-pdf with respect to param. shape=(d, 1), where\n                d is the dimensionality of param.\n                hessian (np.ndarray): array of NEGATIVE second derivatives of the log-pdf with respect to param.\n                shape=(d, d), where d is the dimensionality of param.\n\n        \"\"\"\n        Q = self.precision.predictor(state)\n        if param in self.response:\n            r = np.log(state[self.response]) - self.mean.predictor(state)\n            grad = -(1 / state[self.response]) * (1 + Q @ r)\n        elif param in self.mean.get_grad_param_list() and param not in self.precision.get_grad_param_list():\n            r = np.sum(np.log(state[self.response]) - self.mean.predictor(state), axis=1, keepdims=True)\n            grad_param = self.mean.grad(state, param)\n            grad = grad_param @ Q @ r\n        else:\n            grad = self.grad_log_p_diff(state, param)\n\n        if hessian_required:\n            hessian = self.hessian_log_p(state, param)\n            return grad, hessian\n\n        return grad\n\n    def hessian_log_p(self, state: dict, param: str) -&gt; np.ndarray:\n        \"\"\"Compute Hessian of the log-density with respect to a given parameter.\n\n        Handles 3 possibilities:\n            1) param is the response of the distribution, in which case the Hessian of the log-density is computed\n                directly.\n            2) param is a parameter used in the computation of the mean (through a parameter object) and not in the\n                computation of the precision, and the dependence of the mean parameter on param is linear. The chain\n                rule is used to determine the Hessian.\n            3) neither of the above conditions is True, in which case the default finite-difference gradient is\n                calculated (using self.hessian_log_p_diff()). Note that as per the docstring of\n                self.hessian_log_p_diff(), it is only possible to compute gradients with respect to scalar or vector\n                parameters.\n\n        NOTE: sparse implementation of response hessian currently converts Q from sparse.\n\n        Args:\n            state (dict): contains current state information.\n            param (str): name of the parameter for which we compute derivatives.\n\n        Returns:\n            (np.ndarray): Hessian of log-density wrt the specified param.\n\n        \"\"\"\n        if param in self.response:\n            Q = self.precision.predictor(state)\n            r = np.log(state[self.response]) - self.mean.predictor(state)\n            reciprocal = 1 / state[self.response]\n\n            if sparse.issparse(Q):\n                hess_p = -sparse.diags((np.power(reciprocal, 2) * (1 + Q @ r)).flatten(), offsets=0)\n                Q = Q.toarray()\n            else:\n                hess_p = -np.diagflat(np.power(reciprocal, 2) * (1 + Q @ (r)))\n\n            dim, n = state[self.response].shape\n            out = np.zeros((n, dim, n, dim))\n            diag = np.einsum(\"ijik-&gt;ijk\", out)\n            np.einsum(\"ik, ij, jk -&gt; kij\", reciprocal, Q, reciprocal, out=diag)\n            out = out.transpose((1, 0, 3, 2))\n            out = out.reshape((n * dim, n * dim))\n            hess_p = out + hess_p\n\n        elif param in self.mean.get_grad_param_list() and param not in self.precision.get_grad_param_list():\n            Q = self.precision.predictor(state)\n            grad_param = self.mean.grad(state, param)\n            hess_p = state[self.response].shape[1] * grad_param @ Q @ grad_param.T\n        else:\n            hess_p = self.hessian_log_p_diff(state, param)\n\n        return hess_p\n\n    def rvs(self, state: dict, n: int = 1) -&gt; np.ndarray:\n        \"\"\"Generate random samples from the multivariate log-Gaussian distribution.\n\n        Args:\n            state (dict): dictionary object containing the current state information.\n            n (int, optional): specifies the number of replicate samples required. Defaults to 1.\n\n        Returns:\n            (np.ndarray): random variables generated from distribution returned as p x n where p is the\n                dimensionality of the response.\n\n        \"\"\"\n        mean = self.mean.predictor(state)\n        precision = self.precision.predictor(state)\n        return np.exp(gmrf.sample_normal(mu=mean, Q=precision, n=n))\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.LogNormal.log_p","title":"<code>log_p(state, by_observation=False)</code>","text":"<p>Compute the log of the probability density (for current parameter settings).</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information. state[distribution.response] is expected to be p x n where: p is the number of responses; n is the number of independent replicates/observations.</p> required <code>by_observation</code> <code>bool</code> <p>If True, the log-likelihood is returned for each of the p responses of the distribution separately. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, float]</code> <p>POSITIVE log-density evaluated using the supplied state dictionary.</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>def log_p(self, state: dict, by_observation: bool = False) -&gt; np.ndarray:\n    \"\"\"Compute the log of the probability density (for current parameter settings).\n\n    Args:\n        state (dict): dictionary object containing the current state information. state[distribution.response]\n            is expected to be p x n where: p is the number of responses; n is the number of independent\n            replicates/observations.\n        by_observation (bool, optional): If True, the log-likelihood is returned for each of the p responses of\n            the distribution separately. Defaults to False.\n\n    Returns:\n        (Union[np.ndarray, float]): POSITIVE log-density evaluated using the supplied state dictionary.\n\n    \"\"\"\n    Q = self.precision.predictor(state)\n    mu = self.mean.predictor(state)\n    log_p = gmrf.multivariate_normal_pdf(x=np.log(state[self.response]), mu=mu, Q=Q, by_observation=True) - np.sum(\n        np.log(state[self.response]), axis=0\n    )\n    if not by_observation:\n        log_p = np.sum(log_p)\n    return log_p\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.LogNormal.grad_log_p","title":"<code>grad_log_p(state, param, hessian_required=True)</code>","text":"<p>Generate vector of derivatives of the log-pdf with respect to a given parameter, and if required, also generate the Hessian.</p> <p>See also distribution.grad_log_p() for more information.</p> Handles 3 possibilities <p>1) param is the response of the distribution, in which case the standard gradient of the log-density is     returned. 2) param is a parameter used in the computation of the mean (through a parameter object) and not in the     computation of the precision, in which case the gradient is computed through application of the chain     rule. Note that the Hessian calculated in this case is only valid if the dependence of self.mean on     param is linear. 3) neither of the above conditions is True, in which case the default finite-difference gradient is     calculated (using self.grad_log_p_diff() and self.hessian_log_p_diff()). Note that as per those     docstrings, it is only possible to compute gradients with respect to scalar or vector parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>current state information.</p> required <code>param</code> <code>str</code> <p>name of the parameter for which we compute derivatives.</p> required <code>hessian_required</code> <code>bool</code> <p>flag for whether the Hessian should be calculated and supplied as an output.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tuple[ndarray, ndarray]]</code> <p>if hessian_required=True, then a tuple of (gradient, hessian) is returned. If hessian_required=False, then just a gradient vector is returned. The returned values are as follows: grad (np.ndarray): vector gradients of the POSITIVE log-pdf with respect to param. shape=(d, 1), where d is the dimensionality of param. hessian (np.ndarray): array of NEGATIVE second derivatives of the log-pdf with respect to param. shape=(d, d), where d is the dimensionality of param.</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>def grad_log_p(\n    self, state: dict, param: str, hessian_required: bool = True\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"Generate vector of derivatives of the log-pdf with respect to a given parameter, and if required, also generate the Hessian.\n\n    See also distribution.grad_log_p() for more information.\n\n    Handles 3 possibilities:\n        1) param is the response of the distribution, in which case the standard gradient of the log-density is\n            returned.\n        2) param is a parameter used in the computation of the mean (through a parameter object) and not in the\n            computation of the precision, in which case the gradient is computed through application of the chain\n            rule. Note that the Hessian calculated in this case is only valid if the dependence of self.mean on\n            param is linear.\n        3) neither of the above conditions is True, in which case the default finite-difference gradient is\n            calculated (using self.grad_log_p_diff() and self.hessian_log_p_diff()). Note that as per those\n            docstrings, it is only possible to compute gradients with respect to scalar or vector parameters.\n\n    Args:\n        state (dict): current state information.\n        param (str): name of the parameter for which we compute derivatives.\n        hessian_required (bool): flag for whether the Hessian should be calculated and supplied as an output.\n\n    Returns:\n        (Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]): if hessian_required=True, then a tuple of (gradient,\n            hessian) is returned. If hessian_required=False, then just a gradient vector is returned. The returned\n            values are as follows:\n            grad (np.ndarray): vector gradients of the POSITIVE log-pdf with respect to param. shape=(d, 1), where\n            d is the dimensionality of param.\n            hessian (np.ndarray): array of NEGATIVE second derivatives of the log-pdf with respect to param.\n            shape=(d, d), where d is the dimensionality of param.\n\n    \"\"\"\n    Q = self.precision.predictor(state)\n    if param in self.response:\n        r = np.log(state[self.response]) - self.mean.predictor(state)\n        grad = -(1 / state[self.response]) * (1 + Q @ r)\n    elif param in self.mean.get_grad_param_list() and param not in self.precision.get_grad_param_list():\n        r = np.sum(np.log(state[self.response]) - self.mean.predictor(state), axis=1, keepdims=True)\n        grad_param = self.mean.grad(state, param)\n        grad = grad_param @ Q @ r\n    else:\n        grad = self.grad_log_p_diff(state, param)\n\n    if hessian_required:\n        hessian = self.hessian_log_p(state, param)\n        return grad, hessian\n\n    return grad\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.LogNormal.hessian_log_p","title":"<code>hessian_log_p(state, param)</code>","text":"<p>Compute Hessian of the log-density with respect to a given parameter.</p> Handles 3 possibilities <p>1) param is the response of the distribution, in which case the Hessian of the log-density is computed     directly. 2) param is a parameter used in the computation of the mean (through a parameter object) and not in the     computation of the precision, and the dependence of the mean parameter on param is linear. The chain     rule is used to determine the Hessian. 3) neither of the above conditions is True, in which case the default finite-difference gradient is     calculated (using self.hessian_log_p_diff()). Note that as per the docstring of     self.hessian_log_p_diff(), it is only possible to compute gradients with respect to scalar or vector     parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>contains current state information.</p> required <code>param</code> <code>str</code> <p>name of the parameter for which we compute derivatives.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Hessian of log-density wrt the specified param.</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>def hessian_log_p(self, state: dict, param: str) -&gt; np.ndarray:\n    \"\"\"Compute Hessian of the log-density with respect to a given parameter.\n\n    Handles 3 possibilities:\n        1) param is the response of the distribution, in which case the Hessian of the log-density is computed\n            directly.\n        2) param is a parameter used in the computation of the mean (through a parameter object) and not in the\n            computation of the precision, and the dependence of the mean parameter on param is linear. The chain\n            rule is used to determine the Hessian.\n        3) neither of the above conditions is True, in which case the default finite-difference gradient is\n            calculated (using self.hessian_log_p_diff()). Note that as per the docstring of\n            self.hessian_log_p_diff(), it is only possible to compute gradients with respect to scalar or vector\n            parameters.\n\n    NOTE: sparse implementation of response hessian currently converts Q from sparse.\n\n    Args:\n        state (dict): contains current state information.\n        param (str): name of the parameter for which we compute derivatives.\n\n    Returns:\n        (np.ndarray): Hessian of log-density wrt the specified param.\n\n    \"\"\"\n    if param in self.response:\n        Q = self.precision.predictor(state)\n        r = np.log(state[self.response]) - self.mean.predictor(state)\n        reciprocal = 1 / state[self.response]\n\n        if sparse.issparse(Q):\n            hess_p = -sparse.diags((np.power(reciprocal, 2) * (1 + Q @ r)).flatten(), offsets=0)\n            Q = Q.toarray()\n        else:\n            hess_p = -np.diagflat(np.power(reciprocal, 2) * (1 + Q @ (r)))\n\n        dim, n = state[self.response].shape\n        out = np.zeros((n, dim, n, dim))\n        diag = np.einsum(\"ijik-&gt;ijk\", out)\n        np.einsum(\"ik, ij, jk -&gt; kij\", reciprocal, Q, reciprocal, out=diag)\n        out = out.transpose((1, 0, 3, 2))\n        out = out.reshape((n * dim, n * dim))\n        hess_p = out + hess_p\n\n    elif param in self.mean.get_grad_param_list() and param not in self.precision.get_grad_param_list():\n        Q = self.precision.predictor(state)\n        grad_param = self.mean.grad(state, param)\n        hess_p = state[self.response].shape[1] * grad_param @ Q @ grad_param.T\n    else:\n        hess_p = self.hessian_log_p_diff(state, param)\n\n    return hess_p\n</code></pre>"},{"location":"openmcmc/distribution/location_scale/#openmcmc.distribution.location_scale.LogNormal.rvs","title":"<code>rvs(state, n=1)</code>","text":"<p>Generate random samples from the multivariate log-Gaussian distribution.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>dictionary object containing the current state information.</p> required <code>n</code> <code>int</code> <p>specifies the number of replicate samples required. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>random variables generated from distribution returned as p x n where p is the dimensionality of the response.</p> Source code in <code>src/openmcmc/distribution/location_scale.py</code> <pre><code>def rvs(self, state: dict, n: int = 1) -&gt; np.ndarray:\n    \"\"\"Generate random samples from the multivariate log-Gaussian distribution.\n\n    Args:\n        state (dict): dictionary object containing the current state information.\n        n (int, optional): specifies the number of replicate samples required. Defaults to 1.\n\n    Returns:\n        (np.ndarray): random variables generated from distribution returned as p x n where p is the\n            dimensionality of the response.\n\n    \"\"\"\n    mean = self.mean.predictor(state)\n    precision = self.precision.predictor(state)\n    return np.exp(gmrf.sample_normal(mu=mean, Q=precision, n=n))\n</code></pre>"},{"location":"openmcmc/sampler/metropolis_hastings/","title":"Metropolis-Hastings","text":""},{"location":"openmcmc/sampler/metropolis_hastings/#metropolis-hastings","title":"Metropolis Hastings","text":"<p>MetropolisHastings module.</p> <p>This module provides a class definition of the MetropolisHastings class an abstract base class for implementation of Metropolis-Hastings-type sampling algorithms for a model.</p>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.AcceptRate","title":"<code>AcceptRate</code>  <code>dataclass</code>","text":"<p>Class for dealing with calculation of acceptance rates.</p> <p>Called from MetropolisHastings-type samplers.</p> <p>Attributes:</p> Name Type Description <code>count</code> <p>counters of current number of proposals and accepted proposals from a MH chain</p> Source code in <code>src/openmcmc/sampler/metropolis_hastings.py</code> <pre><code>@dataclass\nclass AcceptRate:\n    \"\"\"Class for dealing with calculation of acceptance rates.\n\n    Called from MetropolisHastings-type samplers.\n\n    Attributes:\n        count: counters of current number of proposals and accepted proposals from a MH chain\n\n    \"\"\"\n\n    def __init__(self):\n        self.count = {\"accept\": 0, \"proposal\": 0}\n\n    @property\n    def acceptance_rate(self) -&gt; float:\n        \"\"\"Acceptance rate property, as calculated from counters.\n\n        Returns:\n            (float): percentage proposals accepted in chain\n\n        \"\"\"\n        return self.count[\"accept\"] / self.count[\"proposal\"] * 100\n\n    def get_acceptance_rate(self) -&gt; str:\n        \"\"\"Return acceptance rate formatted as string.\n\n        Returns:\n            (str): acceptance rate string print out\n\n        \"\"\"\n        if self.count[\"proposal\"] == 0:\n            return \"No proposals\"\n        return f\"Acceptance rate {self.acceptance_rate:.0f}%\"\n\n    def increment_accept(self):\n        \"\"\"Increment acceptance count.\"\"\"\n        self.count[\"accept\"] += 1\n\n    def increment_proposal(self):\n        \"\"\"Increment proposal count.\"\"\"\n        self.count[\"proposal\"] += 1\n</code></pre>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.AcceptRate.acceptance_rate","title":"<code>acceptance_rate: float</code>  <code>property</code>","text":"<p>Acceptance rate property, as calculated from counters.</p> <p>Returns:</p> Type Description <code>float</code> <p>percentage proposals accepted in chain</p>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.AcceptRate.get_acceptance_rate","title":"<code>get_acceptance_rate()</code>","text":"<p>Return acceptance rate formatted as string.</p> <p>Returns:</p> Type Description <code>str</code> <p>acceptance rate string print out</p> Source code in <code>src/openmcmc/sampler/metropolis_hastings.py</code> <pre><code>def get_acceptance_rate(self) -&gt; str:\n    \"\"\"Return acceptance rate formatted as string.\n\n    Returns:\n        (str): acceptance rate string print out\n\n    \"\"\"\n    if self.count[\"proposal\"] == 0:\n        return \"No proposals\"\n    return f\"Acceptance rate {self.acceptance_rate:.0f}%\"\n</code></pre>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.AcceptRate.increment_accept","title":"<code>increment_accept()</code>","text":"<p>Increment acceptance count.</p> Source code in <code>src/openmcmc/sampler/metropolis_hastings.py</code> <pre><code>def increment_accept(self):\n    \"\"\"Increment acceptance count.\"\"\"\n    self.count[\"accept\"] += 1\n</code></pre>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.AcceptRate.increment_proposal","title":"<code>increment_proposal()</code>","text":"<p>Increment proposal count.</p> Source code in <code>src/openmcmc/sampler/metropolis_hastings.py</code> <pre><code>def increment_proposal(self):\n    \"\"\"Increment proposal count.\"\"\"\n    self.count[\"proposal\"] += 1\n</code></pre>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.MetropolisHastings","title":"<code>MetropolisHastings</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MCMCSampler</code></p> <p>Abstract base class for implementation of Metropolis-Hastings-type sampling algorithms for a model.</p> <p>Subclasses include RandomWalk and ManifoldMALA.</p> <p>https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm</p> <p>Attributes:</p> Name Type Description <code>step</code> <code>ndarray</code> <p>step size for Metropolis-Hastings proposals. Should either have shape=(p, 1) or shape=(p, n), where p is the dimension of the parameter, and n is the number of replicates.</p> <code>accept_rate</code> <code>AcceptRate</code> <p>Acceptance Rate counter to keep track of proposals.</p> Source code in <code>src/openmcmc/sampler/metropolis_hastings.py</code> <pre><code>@dataclass\nclass MetropolisHastings(MCMCSampler):\n    \"\"\"Abstract base class for implementation of Metropolis-Hastings-type sampling algorithms for a model.\n\n    Subclasses include RandomWalk and ManifoldMALA.\n\n    https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm\n\n    Attributes:\n        step (np.ndarray): step size for Metropolis-Hastings proposals. Should either have shape=(p, 1) or shape=(p, n),\n            where p is the dimension of the parameter, and n is the number of replicates.\n        accept_rate (AcceptRate): Acceptance Rate counter to keep track of proposals.\n\n    \"\"\"\n\n    step: np.ndarray = field(default_factory=lambda: np.array([0.2], ndmin=2), init=True)\n    accept_rate: AcceptRate = field(default_factory=lambda: AcceptRate(), init=False)\n\n    @abstractmethod\n    def proposal(self, current_state: dict, param_index: int = None) -&gt; Tuple[dict, float, float]:\n        \"\"\"Method which generates proposed state from current state, and computes corresponding transition probabilities.\n\n        Args:\n            current_state (dict): current state\n            param_index (int): subset of parameter used in proposal, If none all parameters are used\n\n        Returns:\n            prop_state (dict): updated proposal_state dictionary.\n            logp_pr_g_cr (float): log-density of proposed state given current state.\n            logp_cr_g_pr (float): log-density of current state given proposed state.\n\n        \"\"\"\n\n    def sample(self, current_state: dict) -&gt; dict:\n        \"\"\"Generate a sample from the specified Metropolis-Hastings-type method.\n\n        https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm\n\n        generate proposal state x' from  current_state x and accept or reject proposal according to the probability:\n        A(x',x) = min(1, (P(x')g(x|x'))/(P(x)g(x'|x)))\n        where:\n            - P(x) is the probability of the state x\n            - g(x|x') is the probability of moving from state x to x'\n\n        The exact method for the proposal (and therefore the form of the proposal distribution) is determined by the\n        specific type of MetropolisHastings Sampler used.\n\n        Args:\n            current_state (dict): dictionary containing the current sampler state.\n\n        Returns:\n            current_state (dict): with updated sample for self.param.\n\n        \"\"\"\n        prop_state, logp_pr_g_cr, logp_cr_g_pr = self.proposal(current_state)\n        current_state = self._accept_reject_proposal(current_state, prop_state, logp_pr_g_cr, logp_cr_g_pr)\n        return current_state\n\n    def _accept_reject_proposal(\n        self, current_state: dict, prop_state: dict, logp_pr_g_cr: float, logp_cr_g_pr: float\n    ) -&gt; dict:\n        \"\"\"Accept or Reject Metropolis-Hastings-type proposal.\n\n        Computes the log posterior for the current and proposed states, and evaluates the log acceptance probability.\n        Accepts the proposal with probability A(x, x'), and returns either the proposed or the current state\n        accordingly.\n\n        Increments self.acceptance_rate() to indicate that a proposal has been made, and also increments the acceptance\n        counter if the proposal is subsequently accepted.\n\n        Args:\n            current_state (dict): current state dictionary\n            prop_state (dict): proposal_state dictionary\n            logp_pr_g_cr (float): log posterior of proposal given current state\n            logp_cr_g_pr (float): log posterior of current state given proposals\n\n        Returns:\n            (dict): updated current state dictionary, after the proposal has either been accepted or rejected.\n\n        \"\"\"\n        self.accept_rate.increment_proposal()\n        logp_cs = 0\n        logp_pr = 0\n        for model in self.model.values():\n            logp_cs += model.log_p(current_state)\n            logp_pr += model.log_p(prop_state)\n        log_accept = logp_pr + logp_cr_g_pr - (logp_cs + logp_pr_g_cr)\n\n        if self.accept_proposal(log_accept):\n            current_state = prop_state\n            self.accept_rate.increment_accept()\n        return current_state\n\n    @staticmethod\n    def accept_proposal(log_accept: float) -&gt; bool:\n        \"\"\"Decide to accept or reject proposal based on log acceptance probability.\n\n        Args:\n            log_accept (np.float64): log acceptance probability.\n\n        Returns:\n            (bool): True for accept, False for Reject.\n\n        \"\"\"\n        return np.log(np.random.rand()) &lt; log_accept\n</code></pre>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.MetropolisHastings.proposal","title":"<code>proposal(current_state, param_index=None)</code>  <code>abstractmethod</code>","text":"<p>Method which generates proposed state from current state, and computes corresponding transition probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>current state</p> required <code>param_index</code> <code>int</code> <p>subset of parameter used in proposal, If none all parameters are used</p> <code>None</code> <p>Returns:</p> Name Type Description <code>prop_state</code> <code>dict</code> <p>updated proposal_state dictionary.</p> <code>logp_pr_g_cr</code> <code>float</code> <p>log-density of proposed state given current state.</p> <code>logp_cr_g_pr</code> <code>float</code> <p>log-density of current state given proposed state.</p> Source code in <code>src/openmcmc/sampler/metropolis_hastings.py</code> <pre><code>@abstractmethod\ndef proposal(self, current_state: dict, param_index: int = None) -&gt; Tuple[dict, float, float]:\n    \"\"\"Method which generates proposed state from current state, and computes corresponding transition probabilities.\n\n    Args:\n        current_state (dict): current state\n        param_index (int): subset of parameter used in proposal, If none all parameters are used\n\n    Returns:\n        prop_state (dict): updated proposal_state dictionary.\n        logp_pr_g_cr (float): log-density of proposed state given current state.\n        logp_cr_g_pr (float): log-density of current state given proposed state.\n\n    \"\"\"\n</code></pre>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.MetropolisHastings.sample","title":"<code>sample(current_state)</code>","text":"<p>Generate a sample from the specified Metropolis-Hastings-type method.</p> <p>https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm</p> <p>generate proposal state x' from  current_state x and accept or reject proposal according to the probability: A(x',x) = min(1, (P(x')g(x|x'))/(P(x)g(x'|x))) where:     - P(x) is the probability of the state x     - g(x|x') is the probability of moving from state x to x'</p> <p>The exact method for the proposal (and therefore the form of the proposal distribution) is determined by the specific type of MetropolisHastings Sampler used.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary containing the current sampler state.</p> required <p>Returns:</p> Name Type Description <code>current_state</code> <code>dict</code> <p>with updated sample for self.param.</p> Source code in <code>src/openmcmc/sampler/metropolis_hastings.py</code> <pre><code>def sample(self, current_state: dict) -&gt; dict:\n    \"\"\"Generate a sample from the specified Metropolis-Hastings-type method.\n\n    https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm\n\n    generate proposal state x' from  current_state x and accept or reject proposal according to the probability:\n    A(x',x) = min(1, (P(x')g(x|x'))/(P(x)g(x'|x)))\n    where:\n        - P(x) is the probability of the state x\n        - g(x|x') is the probability of moving from state x to x'\n\n    The exact method for the proposal (and therefore the form of the proposal distribution) is determined by the\n    specific type of MetropolisHastings Sampler used.\n\n    Args:\n        current_state (dict): dictionary containing the current sampler state.\n\n    Returns:\n        current_state (dict): with updated sample for self.param.\n\n    \"\"\"\n    prop_state, logp_pr_g_cr, logp_cr_g_pr = self.proposal(current_state)\n    current_state = self._accept_reject_proposal(current_state, prop_state, logp_pr_g_cr, logp_cr_g_pr)\n    return current_state\n</code></pre>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.MetropolisHastings.accept_proposal","title":"<code>accept_proposal(log_accept)</code>  <code>staticmethod</code>","text":"<p>Decide to accept or reject proposal based on log acceptance probability.</p> <p>Parameters:</p> Name Type Description Default <code>log_accept</code> <code>float64</code> <p>log acceptance probability.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True for accept, False for Reject.</p> Source code in <code>src/openmcmc/sampler/metropolis_hastings.py</code> <pre><code>@staticmethod\ndef accept_proposal(log_accept: float) -&gt; bool:\n    \"\"\"Decide to accept or reject proposal based on log acceptance probability.\n\n    Args:\n        log_accept (np.float64): log acceptance probability.\n\n    Returns:\n        (bool): True for accept, False for Reject.\n\n    \"\"\"\n    return np.log(np.random.rand()) &lt; log_accept\n</code></pre>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.RandomWalk","title":"<code>RandomWalk</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetropolisHastings</code></p> <p>Subtype of MetropolisHastings sampler that uses Gaussian random Walk proposals.</p> <p>Supports both non-truncated and truncated Gaussian proposals: specifying self.domain limits leads to a truncated proposal mechanism.</p> <p>Allows for the possibility that other elements of the model state have a dependence on the value of self.param, and if so should change when this value changes. If supplied, the self.state_update_function() property is called by the proposal function to update any other elements of the state as required.</p> <p>Attributes:</p> Name Type Description <code>domain_limits</code> <code>ndarray</code> <p>array with shape=(p, 2), where p is the dimensionality of the parameter being sampled. The first column gives the lower limits for the proposal, the second column gives the upper limits.</p> <code>state_update_function</code> <code>Callable</code> <p>function which updates other elements of proposed state based on the proposed value for param.</p> Source code in <code>src/openmcmc/sampler/metropolis_hastings.py</code> <pre><code>@dataclass\nclass RandomWalk(MetropolisHastings):\n    \"\"\"Subtype of MetropolisHastings sampler that uses Gaussian random Walk proposals.\n\n    Supports both non-truncated and truncated Gaussian proposals: specifying self.domain limits leads to a truncated\n    proposal mechanism.\n\n    Allows for the possibility that other elements of the model state have a dependence on the value of self.param, and\n    if so should change when this value changes. If supplied, the self.state_update_function() property is called by the\n    proposal function to update any other elements of the state as required.\n\n    Attributes:\n        domain_limits (np.ndarray): array with shape=(p, 2), where p is the dimensionality of the parameter being\n            sampled. The first column gives the lower limits for the proposal, the second column gives the upper limits.\n        state_update_function (Callable): function which updates other elements of proposed state based on the proposed\n            value for param.\n\n    \"\"\"\n\n    domain_limits: np.ndarray = None\n    state_update_function: Callable = None\n\n    def __post_init__(self):\n        \"\"\"Derive conditional model instead of storing all distributions where things are simple.\n\n        However, this should not be done in the case where a state_update_function is provided as we don't know  in\n        general what/how parameters might change so need to keep full model to avoid incorrect conditioning.\n\n        \"\"\"\n        if self.state_update_function is None:\n            self.model = self.model.conditional(self.param)\n        self.step = np.array(self.step, ndmin=2)\n\n    def proposal(self, current_state: dict, param_index: int = None) -&gt; Tuple[dict, float, float]:\n        \"\"\"Updates the current value of self.param using a (truncated) Gaussian random walk proposal.\n\n        In the non-truncated case, the proposal mechanism is symmetric, i.e. logp_pr_g_cr = logp_cr_g_pr. In this\n        instance, the function simply returns logp_pr_g_cr = logp_cr_g_pr = 0, since these terms would anyway cancel\n        in the calculation of the acceptance ratio.\n\n        Introducing a truncation into the proposal distribution means that the proposal is no longer symmetric, and so\n        the log-proposal densities are computed in these cases.\n\n        Enables 3 different possibilities for the step size:\n            1) shape=(1, 1): scalar step size, identical for every element of the parameter.\n            2) shape=(p, 1): step size with the same shape as the parameter being sampled (for one or many replicates).\n            3) shape=(p, n): a p-dimensional step size for each of n-replicates.\n\n        Args:\n            current_state (dict): dictionary containing current parameter values.\n            param_index (int): subset of parameter used in proposal, If none all parameters are used\n\n        Returns:\n            prop_state (dict): updated proposal_state dictionary.\n            logp_pr_g_cr (float): log-density of proposed state given current state.\n            logp_cr_g_pr (float): log-density of current state given proposed state.\n\n        \"\"\"\n        prop_state = deepcopy(current_state)\n\n        if param_index is None:\n            mu = prop_state[self.param]\n            step = self.step\n        else:\n            mu = prop_state[self.param][:, param_index]\n            if self.step.shape[1] == 1:\n                step = self.step.flatten()\n            else:\n                step = self.step[:, param_index].flatten()\n\n        if self.domain_limits is None:\n            z = mu + norm.rvs(size=prop_state[self.param].shape, scale=step)\n            logp_pr_g_cr = logp_cr_g_pr = 0.0\n        else:\n            lb = self.domain_limits[:, 0]\n            ub = self.domain_limits[:, 1]\n            z = gmrf.truncated_normal_rv(mean=mu, scale=step, lower=lb, upper=ub, size=len(lb))\n            logp_pr_g_cr = np.sum(gmrf.truncated_normal_log_pdf(z, mu, step, lower=lb, upper=ub))\n            logp_cr_g_pr = np.sum(gmrf.truncated_normal_log_pdf(mu, z, step, lower=lb, upper=ub))\n\n        if param_index is None:\n            prop_state[self.param] = z\n        else:\n            prop_state[self.param][:, param_index] = z\n\n        if callable(self.state_update_function):\n            prop_state = self.state_update_function(prop_state, param_index)\n\n        return prop_state, logp_pr_g_cr, logp_cr_g_pr\n</code></pre>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.RandomWalk.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Derive conditional model instead of storing all distributions where things are simple.</p> <p>However, this should not be done in the case where a state_update_function is provided as we don't know  in general what/how parameters might change so need to keep full model to avoid incorrect conditioning.</p> Source code in <code>src/openmcmc/sampler/metropolis_hastings.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Derive conditional model instead of storing all distributions where things are simple.\n\n    However, this should not be done in the case where a state_update_function is provided as we don't know  in\n    general what/how parameters might change so need to keep full model to avoid incorrect conditioning.\n\n    \"\"\"\n    if self.state_update_function is None:\n        self.model = self.model.conditional(self.param)\n    self.step = np.array(self.step, ndmin=2)\n</code></pre>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.RandomWalk.proposal","title":"<code>proposal(current_state, param_index=None)</code>","text":"<p>Updates the current value of self.param using a (truncated) Gaussian random walk proposal.</p> <p>In the non-truncated case, the proposal mechanism is symmetric, i.e. logp_pr_g_cr = logp_cr_g_pr. In this instance, the function simply returns logp_pr_g_cr = logp_cr_g_pr = 0, since these terms would anyway cancel in the calculation of the acceptance ratio.</p> <p>Introducing a truncation into the proposal distribution means that the proposal is no longer symmetric, and so the log-proposal densities are computed in these cases.</p> Enables 3 different possibilities for the step size <p>1) shape=(1, 1): scalar step size, identical for every element of the parameter. 2) shape=(p, 1): step size with the same shape as the parameter being sampled (for one or many replicates). 3) shape=(p, n): a p-dimensional step size for each of n-replicates.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary containing current parameter values.</p> required <code>param_index</code> <code>int</code> <p>subset of parameter used in proposal, If none all parameters are used</p> <code>None</code> <p>Returns:</p> Name Type Description <code>prop_state</code> <code>dict</code> <p>updated proposal_state dictionary.</p> <code>logp_pr_g_cr</code> <code>float</code> <p>log-density of proposed state given current state.</p> <code>logp_cr_g_pr</code> <code>float</code> <p>log-density of current state given proposed state.</p> Source code in <code>src/openmcmc/sampler/metropolis_hastings.py</code> <pre><code>def proposal(self, current_state: dict, param_index: int = None) -&gt; Tuple[dict, float, float]:\n    \"\"\"Updates the current value of self.param using a (truncated) Gaussian random walk proposal.\n\n    In the non-truncated case, the proposal mechanism is symmetric, i.e. logp_pr_g_cr = logp_cr_g_pr. In this\n    instance, the function simply returns logp_pr_g_cr = logp_cr_g_pr = 0, since these terms would anyway cancel\n    in the calculation of the acceptance ratio.\n\n    Introducing a truncation into the proposal distribution means that the proposal is no longer symmetric, and so\n    the log-proposal densities are computed in these cases.\n\n    Enables 3 different possibilities for the step size:\n        1) shape=(1, 1): scalar step size, identical for every element of the parameter.\n        2) shape=(p, 1): step size with the same shape as the parameter being sampled (for one or many replicates).\n        3) shape=(p, n): a p-dimensional step size for each of n-replicates.\n\n    Args:\n        current_state (dict): dictionary containing current parameter values.\n        param_index (int): subset of parameter used in proposal, If none all parameters are used\n\n    Returns:\n        prop_state (dict): updated proposal_state dictionary.\n        logp_pr_g_cr (float): log-density of proposed state given current state.\n        logp_cr_g_pr (float): log-density of current state given proposed state.\n\n    \"\"\"\n    prop_state = deepcopy(current_state)\n\n    if param_index is None:\n        mu = prop_state[self.param]\n        step = self.step\n    else:\n        mu = prop_state[self.param][:, param_index]\n        if self.step.shape[1] == 1:\n            step = self.step.flatten()\n        else:\n            step = self.step[:, param_index].flatten()\n\n    if self.domain_limits is None:\n        z = mu + norm.rvs(size=prop_state[self.param].shape, scale=step)\n        logp_pr_g_cr = logp_cr_g_pr = 0.0\n    else:\n        lb = self.domain_limits[:, 0]\n        ub = self.domain_limits[:, 1]\n        z = gmrf.truncated_normal_rv(mean=mu, scale=step, lower=lb, upper=ub, size=len(lb))\n        logp_pr_g_cr = np.sum(gmrf.truncated_normal_log_pdf(z, mu, step, lower=lb, upper=ub))\n        logp_cr_g_pr = np.sum(gmrf.truncated_normal_log_pdf(mu, z, step, lower=lb, upper=ub))\n\n    if param_index is None:\n        prop_state[self.param] = z\n    else:\n        prop_state[self.param][:, param_index] = z\n\n    if callable(self.state_update_function):\n        prop_state = self.state_update_function(prop_state, param_index)\n\n    return prop_state, logp_pr_g_cr, logp_cr_g_pr\n</code></pre>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.RandomWalkLoop","title":"<code>RandomWalkLoop</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RandomWalk</code></p> <p>Subtype of MetropolisHastings sampler which updates each of n replicates of a parameter one-at-a-time, rather than all simultaneously.</p> Source code in <code>src/openmcmc/sampler/metropolis_hastings.py</code> <pre><code>@dataclass\nclass RandomWalkLoop(RandomWalk):\n    \"\"\"Subtype of MetropolisHastings sampler which updates each of n replicates of a parameter one-at-a-time, rather than all simultaneously.\"\"\"\n\n    def sample(self, current_state: dict) -&gt; dict:\n        \"\"\"Update each of n replicates of a given parameter in a loop, rather than simultaneously.\n\n        Args:\n            current_state (dict): dictionary containing the current sampler state.\n\n        Returns:\n            current_state (dict): with updated sample for self.param.\n\n        \"\"\"\n        for param_index in range(current_state[self.param].shape[1]):\n            prop_state, logp_pr_g_cr, logp_cr_g_pr = self.proposal(current_state, param_index)\n            current_state = self._accept_reject_proposal(current_state, prop_state, logp_pr_g_cr, logp_cr_g_pr)\n        return current_state\n</code></pre>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.RandomWalkLoop.sample","title":"<code>sample(current_state)</code>","text":"<p>Update each of n replicates of a given parameter in a loop, rather than simultaneously.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary containing the current sampler state.</p> required <p>Returns:</p> Name Type Description <code>current_state</code> <code>dict</code> <p>with updated sample for self.param.</p> Source code in <code>src/openmcmc/sampler/metropolis_hastings.py</code> <pre><code>def sample(self, current_state: dict) -&gt; dict:\n    \"\"\"Update each of n replicates of a given parameter in a loop, rather than simultaneously.\n\n    Args:\n        current_state (dict): dictionary containing the current sampler state.\n\n    Returns:\n        current_state (dict): with updated sample for self.param.\n\n    \"\"\"\n    for param_index in range(current_state[self.param].shape[1]):\n        prop_state, logp_pr_g_cr, logp_cr_g_pr = self.proposal(current_state, param_index)\n        current_state = self._accept_reject_proposal(current_state, prop_state, logp_pr_g_cr, logp_cr_g_pr)\n    return current_state\n</code></pre>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.ManifoldMALA","title":"<code>ManifoldMALA</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetropolisHastings</code></p> <p>Class implementing manifold Metropolis-adjusted Langevin algorithm (mMALA) proposal mechanism.</p> <p>Reference: Riemann manifold Langevin and Hamiltonian Monte Carlo methods, Mark Girolami, Ben Calderhead, 03 March 2011 https://doi.org/10.1111/j.1467-9868.2010.00765.x</p> Source code in <code>src/openmcmc/sampler/metropolis_hastings.py</code> <pre><code>@dataclass\nclass ManifoldMALA(MetropolisHastings):\n    \"\"\"Class implementing manifold Metropolis-adjusted Langevin algorithm (mMALA) proposal mechanism.\n\n    Reference: Riemann manifold Langevin and Hamiltonian Monte Carlo methods, Mark Girolami, Ben Calderhead,\n    03 March 2011 https://doi.org/10.1111/j.1467-9868.2010.00765.x\n\n    \"\"\"\n\n    def proposal(self, current_state: dict, param_index: int = None) -&gt; Tuple[dict, np.ndarray, np.ndarray]:\n        \"\"\"Generate mMALA proposed state from current state using gradient and hessian, and compute corresponding log-transition probabilities.\n\n        Args:\n            current_state (dict): dictionary containing current parameter values.\n            param_index (int): required input from superclass. Not used; defaults to None.\n\n        Returns:\n            prop_state (dict): updated proposal_state dictionary.\n            logp_pr_g_cr (np.ndarray): log-density of proposed state given current state.\n            logp_cr_g_pr (np.ndarray): log-density of current state given proposed state.\n\n        \"\"\"\n        prop_state = deepcopy(current_state)\n\n        mu_cr, chol_cr = self._proposal_params(current_state)\n        prop_state[self.param] = gmrf.sample_normal(mu_cr, L=chol_cr)\n        logp_pr_g_cr = self._log_proposal_density(prop_state, mu_cr, chol_cr)\n\n        mu_pr, chol_pr = self._proposal_params(prop_state)\n        logp_cr_g_pr = self._log_proposal_density(current_state, mu_pr, chol_pr)\n\n        return prop_state, logp_pr_g_cr, logp_cr_g_pr\n\n    def _proposal_params(self, current_state: dict) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Returns the mean vector and the Cholesky factorization of the precision matrix for the mMALA proposal.\n\n        The density for either the forward or return proposal in an mMALA scheme is a Gaussian. In the case of the\n        forward proposal, the density is as follows:\n            q(prop | theta_0) ~ N(mu*, stp^2 * H ^-1 )\n        where:\n            mu* = theta_0 + 1/2 * stp^2 * H ^-1 @ G\n            H = hessian(theta_0)\n            G = gradient(theta_0)\n\n        Args:\n            current_state (dict): dictionary containing current parameter values.\n\n        Returns:\n            mu_cr (np.ndarray): mean for proposal distribution, shape=(p, 1).\n            chol_cr (np.ndarray): lower triangular Cholesky factorization of precision matrix, shape=(p, p).\n\n        \"\"\"\n        grad_cr, hessian_cr = self.model.grad_log_p(current_state, param=self.param, hessian_required=True)\n        precision_cr = hessian_cr / (self.step**2)\n        chol_cr = gmrf.cholesky(precision_cr)\n        mu_cr = current_state[self.param] + (1 / 2) * gmrf.cho_solve((chol_cr, True), grad_cr).reshape(grad_cr.shape)\n        return mu_cr, chol_cr\n\n    def _log_proposal_density(self, state: dict, mu: np.ndarray, chol: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Evaluate the log-proposal density for the mMALA transition.\n\n        log determinant calculated using:\n        https://blogs.sas.com/content/iml/2012/10/31/compute-the-log-determinant-of-a-matrix.html\n\n\n        A quadratic form can be expressed in terms of the Cholesky factorization of the matrix as:\n            r' Q r = r' L L' r = w' w =sum(w^2)\n        where:\n            w = L' r\n            r = prm - mu\n\n        Args:\n            state (dict): dictionary containing parameter values.\n            mu (np.ndarray): mean vector, shape=(p, 1).\n            chol (np.ndarray): LOWER triangular cholesky factorization of the precision matrix, shape=(p, p)\n\n        Returns:\n            (np.ndarray): log-transition probability.\n\n        \"\"\"\n        w = chol.transpose() @ (state[self.param] - mu)\n        return np.sum(np.log(chol.diagonal())) - 0.5 * w.T.dot(w)\n</code></pre>"},{"location":"openmcmc/sampler/metropolis_hastings/#openmcmc.sampler.metropolis_hastings.ManifoldMALA.proposal","title":"<code>proposal(current_state, param_index=None)</code>","text":"<p>Generate mMALA proposed state from current state using gradient and hessian, and compute corresponding log-transition probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary containing current parameter values.</p> required <code>param_index</code> <code>int</code> <p>required input from superclass. Not used; defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>prop_state</code> <code>dict</code> <p>updated proposal_state dictionary.</p> <code>logp_pr_g_cr</code> <code>ndarray</code> <p>log-density of proposed state given current state.</p> <code>logp_cr_g_pr</code> <code>ndarray</code> <p>log-density of current state given proposed state.</p> Source code in <code>src/openmcmc/sampler/metropolis_hastings.py</code> <pre><code>def proposal(self, current_state: dict, param_index: int = None) -&gt; Tuple[dict, np.ndarray, np.ndarray]:\n    \"\"\"Generate mMALA proposed state from current state using gradient and hessian, and compute corresponding log-transition probabilities.\n\n    Args:\n        current_state (dict): dictionary containing current parameter values.\n        param_index (int): required input from superclass. Not used; defaults to None.\n\n    Returns:\n        prop_state (dict): updated proposal_state dictionary.\n        logp_pr_g_cr (np.ndarray): log-density of proposed state given current state.\n        logp_cr_g_pr (np.ndarray): log-density of current state given proposed state.\n\n    \"\"\"\n    prop_state = deepcopy(current_state)\n\n    mu_cr, chol_cr = self._proposal_params(current_state)\n    prop_state[self.param] = gmrf.sample_normal(mu_cr, L=chol_cr)\n    logp_pr_g_cr = self._log_proposal_density(prop_state, mu_cr, chol_cr)\n\n    mu_pr, chol_pr = self._proposal_params(prop_state)\n    logp_cr_g_pr = self._log_proposal_density(current_state, mu_pr, chol_pr)\n\n    return prop_state, logp_pr_g_cr, logp_cr_g_pr\n</code></pre>"},{"location":"openmcmc/sampler/reversible_jump/","title":"Reversible Jump","text":""},{"location":"openmcmc/sampler/reversible_jump/#reversible-jump","title":"Reversible Jump","text":"<p>ReversibleJump module.</p> <p>This module provides a class definition of the ReversibleJump class a class for reversible jump sampling for given parameter and associated parameters.</p>"},{"location":"openmcmc/sampler/reversible_jump/#openmcmc.sampler.reversible_jump.ReversibleJump","title":"<code>ReversibleJump</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetropolisHastings</code></p> <p>Reversible jump sampling for given parameter and associated parameter.</p> <p>self.param corresponds to a number of elements, which will either increase of decrease by 1. self.associated_params corresponds to an associated set of self.param parameters, to which we either add or remove an element for a birth or death move.</p> <p>The attributes self.state_birth_function and self.state_death_function can be used to supply functions which implement problem-specific alterations to elements of the state on the occurrence of a birth or death move respectively. For example, it may be required to update a basis matrix in the state after a change in the number of knots/locations associated with the basis definition.</p> <p>The functions self.matched_birth_transition and self.matched_death_transition implement optional functionality which can be used to ensure consistency between sets of basis parameters before and after a transition. These work by ensuring that the basis predictions before and after the transition match, then applies Gaussian random noise (with a given standard deviation) to the coefficient of the new element.</p> <p>Attributes:</p> Name Type Description <code>associated_params</code> <code>list or string</code> <p>a list or a string associated with the dimension jump. List of additional parameters that need to be created/removed as part of the dimension change. The default behaviour is to sample the necessary additional values from the associated parameter prior distribution. Defaults to None.</p> <code>n_max</code> <code>int</code> <p>upper limit on self.param (lower limit is assumed to be 1).</p> <code>birth_probability</code> <code>float</code> <p>probability that a birth move is chosen on any given iteration of the algorithm (death_probability = 1 - birth_probability). Defaults to 0.5.</p> <code>state_birth_function</code> <code>Callable</code> <p>function which implements problem-specific requirements for updates to elements of the state as part of a birth function (e.g. updates to a problem-specific basis matrix based given additional location parameters). Defaults to None.</p> <code>state_death_function</code> <code>Callable</code> <p>function which implements problem-specific requirements for updates to elements of state as part of a death function. Should mirror the supplied state_birth_function. Defaults to None.</p> <code>matching_params</code> <code>dict</code> <p>dictionary of parameters required for the matched coefficient transitions- for details of what it should contain, see self.matched_birth_transition.</p> Source code in <code>src/openmcmc/sampler/reversible_jump.py</code> <pre><code>@dataclass\nclass ReversibleJump(MetropolisHastings):\n    \"\"\"Reversible jump sampling for given parameter and associated parameter.\n\n    self.param corresponds to a number of elements, which will either increase of decrease by 1. self.associated_params\n    corresponds to an associated set of self.param parameters, to which we either add or remove an element for a birth\n    or death move.\n\n    The attributes self.state_birth_function and self.state_death_function can be used to supply functions which\n    implement problem-specific alterations to elements of the state on the occurrence of a birth or death move\n    respectively. For example, it may be required to update a basis matrix in the state after a change in the number\n    of knots/locations associated with the basis definition.\n\n    The functions self.matched_birth_transition and self.matched_death_transition implement optional functionality which\n    can be used to ensure consistency between sets of basis parameters before and after a transition. These work by\n    ensuring that the basis predictions before and after the transition match, then applies Gaussian random noise (with\n    a given standard deviation) to the coefficient of the new element.\n\n    Attributes:\n        associated_params (list or string): a list or a string associated with the dimension jump. List of additional\n            parameters that need to be created/removed as part of the dimension change. The default behaviour is to\n            sample the necessary additional values from the associated parameter prior distribution. Defaults to None.\n        n_max (int): upper limit on self.param (lower limit is assumed to be 1).\n        birth_probability (float): probability that a birth move is chosen on any given iteration of the algorithm\n            (death_probability = 1 - birth_probability). Defaults to 0.5.\n        state_birth_function (Callable): function which implements problem-specific requirements for updates to elements\n            of the state as part of a birth function (e.g. updates to a problem-specific basis matrix based given\n            additional location parameters). Defaults to None.\n        state_death_function (Callable): function which implements problem-specific requirements for updates to elements\n            of state as part of a death function. Should mirror the supplied state_birth_function. Defaults to None.\n        matching_params (dict): dictionary of parameters required for the matched coefficient transitions- for details\n            of what it should contain, see self.matched_birth_transition.\n\n    \"\"\"\n\n    associated_params: Union[list, str, None] = None\n    n_max: Union[int, None] = None\n    birth_probability: float = 0.5\n    state_birth_function: Union[Callable, None] = None\n    state_death_function: Union[Callable, None] = None\n    matching_params: Union[dict, None] = None\n\n    def __post_init__(self):\n        \"\"\"Empty function to prevent super.__post_init__ from being run.\n\n        The whole model should be attached in this instance, rather than simply those elements with a dependence on\n        self.param.\n\n        \"\"\"\n        if isinstance(self.associated_params, str):\n            self.associated_params = [self.associated_params]\n\n    def proposal(self, current_state: dict, param_index: int = None) -&gt; Tuple[dict, float, float]:\n        \"\"\"Make a proposal, and compute related transition probabilities for the move.\n\n        Args:\n            current_state (dict): dictionary with current parameter values.\n            param_index (int): not used, included for compatibility with superclass.\n\n        Returns:\n            prop_state (dict): dictionary updated with proposed value for self.param.\n            logp_pr_g_cr (float): transition probability for proposed state given current state.\n            logp_cr_g_pr (float): transition probability for current state given proposed state.\n\n        \"\"\"\n        birth = self.get_move_type(current_state)\n        if birth:\n            prop_state, logp_pr_g_cr, logp_cr_g_pr = self.birth_proposal(current_state=current_state)\n        else:\n            prop_state, logp_pr_g_cr, logp_cr_g_pr = self.death_proposal(current_state=current_state)\n        return prop_state, logp_pr_g_cr, logp_cr_g_pr\n\n    def birth_proposal(self, current_state: dict) -&gt; Tuple[dict, float, float]:\n        \"\"\"Make a birth proposal move: INCREASES state[self.param] by 1.\n\n        Also makes a proposal for a new element of an associated parameter, state[self.associated_params], by generating a draw\n        from the prior distribution for self.associated_params.\n\n        self.state_birth_function() is a function which can be optionally specified for altering the dimensionality of\n        any other parameters associated with the dimension change (e.g. a basis matrix, or an allocation parameter).\n\n        If the self.matching_params dictionary is specified, self.matched_birth_transition() is used to generate a\n        proposal for a set of basis parameters such that the predicted values match before and after the transition.\n\n        NOTE: log-probability for deletion of a particular knot (-log(n + 1)) is cancelled by the contribution from\n        the order statistics densities, log((n + 1)! / n!) = log(n + 1). Therefore, both contributions are omitted from\n        the calculation. For further information, see Richardson &amp; Green 1997, Section 3.2:\n        https://people.maths.bris.ac.uk/~mapjg/papers/RichardsonGreenRSSB.pdf\n\n        NOTE: log-probability density for the full model is obtained from summing the contribution of the log-density\n        for the individual distributions corresponding to each jump parameter.\n\n        Args:\n            current_state (dict): dictionary with current parameter values.\n\n        Returns:\n            prop_state (dict): dictionary updated with proposed state.\n            logp_pr_g_cr (float): transition probability for proposed state given current state.\n            logp_cr_g_pr (float): transition probability for current state given proposed state.\n\n        \"\"\"\n        prop_state = deepcopy(current_state)\n        prop_state[self.param] = prop_state[self.param] + 1\n        log_prop_density = 0\n\n        for associated_key in self.associated_params:\n            new_element = self.model[associated_key].rvs(state=current_state, n=1)\n            prop_state[associated_key] = np.concatenate((prop_state[associated_key], new_element), axis=1)\n            log_prop_density += self.model[associated_key].log_p(current_state, by_observation=True)\n        if callable(self.state_birth_function):\n            prop_state, logp_pr_g_cr, logp_cr_g_pr = self.state_birth_function(current_state, prop_state)\n        else:\n            logp_pr_g_cr, logp_cr_g_pr = 0.0, 0.0\n        if self.matching_params is not None:\n            prop_state, logp_pr_g_cr, logp_cr_g_pr = self.matched_birth_transition(\n                current_state, prop_state, logp_pr_g_cr, logp_cr_g_pr\n            )\n\n        p_birth, p_death = self.get_move_probabilities(current_state, True)\n        logp_pr_g_cr += np.log(p_birth) + log_prop_density[-1]\n        logp_cr_g_pr += np.log(p_death)\n\n        return prop_state, logp_pr_g_cr, logp_cr_g_pr\n\n    def death_proposal(self, current_state: dict) -&gt; Tuple[dict, float, float]:\n        \"\"\"Make a death proposal move: DECREASES state[self.param] by 1.\n\n        Also adjusts the associated parameter state[self.associated_params] by deleting a randomly-selected element.\n\n        self.state_death_function() and self.matched_death_transition() can be used (optional) to specify transitions\n        opposite to those used in the birth move.\n\n        NOTE: log-probability density for the full model is obtained from summing the contribution of the log-density\n        for the individual distributions corresponding to each jump parameter.\n\n        For further information about the transition, see also self.birth_proposal().\n\n        Args:\n            current_state (dict): dictionary with current parameter values.\n\n        Returns:\n            prop_state (dict): dictionary updated with proposed state.\n            logp_pr_g_cr (float): transition probability for proposed state given current state.\n            logp_cr_g_pr (float): transition probability for current state given proposed state.\n\n        \"\"\"\n        prop_state = deepcopy(current_state)\n        prop_state[self.param] = prop_state[self.param] - 1\n        log_prop_density = 0\n        deletion_index = randint.rvs(low=0, high=current_state[self.param])\n        for associated_key in self.associated_params:\n            prop_state[associated_key] = np.delete(prop_state[associated_key], obj=deletion_index, axis=1)\n            log_prop_density += self.model[associated_key].log_p(current_state, by_observation=True)\n\n        if callable(self.state_death_function):\n            prop_state, logp_pr_g_cr, logp_cr_g_pr = self.state_death_function(\n                current_state, prop_state, deletion_index\n            )\n        else:\n            logp_pr_g_cr, logp_cr_g_pr = 0.0, 0.0\n        if self.matching_params is not None:\n            prop_state, logp_pr_g_cr, logp_cr_g_pr = self.matched_death_transition(\n                current_state, prop_state, logp_pr_g_cr, logp_cr_g_pr, deletion_index\n            )\n\n        p_birth, p_death = self.get_move_probabilities(current_state, False)\n        logp_pr_g_cr += np.log(p_death)\n        logp_cr_g_pr += np.log(p_birth) + log_prop_density[-1]\n\n        return prop_state, logp_pr_g_cr, logp_cr_g_pr\n\n    def matched_birth_transition(\n        self, current_state: dict, prop_state: dict, logp_pr_g_cr: float, logp_cr_g_pr: float\n    ) -&gt; Tuple[dict, float, float]:\n        \"\"\"Generate a proposal for coefficients associated with a birth move, using the principle of matching the predictions before and after the move.\n\n        The parameter vector in the proposed state is computed as: beta* = F @ beta_aug, where:\n        F = [G, 0\n             0', 1]\n        G = (X*' @ X*)^{-1} @ (X*' @ X)\n        where X is the original basis matrix, and X* is the augmented basis matrix. For a detailed explanation of the\n        approach, see: https://ygraigarw.github.io/ZnnEA1D19.pdf\n\n        The basis matrix in the proposed state should already have been updated in self.state_birth_function(), before\n        the call to this function (along with any other associated parameters that need to change shape).\n\n        The following fields should be supplied as part of the self.matching_params dictionary:\n            - \"variable\" (str): reference to the coefficient parameter vector in the state.\n            - \"matrix\" (str): reference to the associated basis matrix in state.\n            - \"scale\" (float): scale of Gaussian noise added to proposal.\n            - \"limits\" (list): [lower, upper] limit for truncated Normal proposals.\n\n        The proposal for the additional basis parameter can be either from:\n            - a standard normal distribution (when self.matching_params[\"limits\"] is passed as None).\n            - a truncated normal distribution (when self.matching_params[\"limits\"] is a two-element list of the lower\n                and upper limits).\n\n        Args:\n            current_state (dict): current parameter state as dictionary.\n            prop_state (dict): proposed state dictionary, with updated basis matrix.\n            logp_pr_g_cr (float): transition probability for proposed state given current state.\n            logp_cr_g_pr (float): transition probability for current state given proposed state.\n\n        Returns:\n            prop_state (dict): proposed state with updated parameter vector.\n            logp_pr_g_cr (float): updated transition probability.\n            logp_cr_g_pr (float): updated transition probability.\n\n        \"\"\"\n        vector = self.matching_params[\"variable\"]\n        matrix = self.matching_params[\"matrix\"]\n        proposal_scale = self.matching_params[\"scale\"]\n        proposal_limits = self.matching_params[\"limits\"]\n\n        current_basis = current_state[matrix]\n        prop_basis = prop_state[matrix]\n        G = np.linalg.solve(\n            prop_basis.T @ prop_basis + 1e-10 * np.eye(prop_basis.shape[1]), prop_basis.T @ current_basis\n        )\n        F = np.concatenate((G, np.eye(N=G.shape[0], M=1, k=-G.shape[0] + 1)), axis=1)\n        mu_star = G @ current_state[vector]\n        prop_state[vector] = deepcopy(mu_star)\n\n        if proposal_limits is not None:\n            prop_state[vector][-1] = gmrf.truncated_normal_rv(\n                mean=mu_star[-1], scale=proposal_scale, lower=proposal_limits[0], upper=proposal_limits[1], size=1\n            )\n            logp_pr_g_cr += gmrf.truncated_normal_log_pdf(\n                prop_state[vector][-1], mu_star[-1], proposal_scale, lower=proposal_limits[0], upper=proposal_limits[1]\n            )\n        else:\n            Q = np.array(1 / (proposal_scale**2), ndmin=2)\n            prop_state[vector][-1] = gmrf.sample_normal(mu=mu_star[-1], Q=Q, n=1)\n            logp_pr_g_cr += gmrf.multivariate_normal_pdf(x=prop_state[vector][-1], mu=mu_star[-1], Q=Q)\n\n        logp_cr_g_pr += np.log(np.linalg.det(F))\n\n        return prop_state, logp_pr_g_cr, logp_cr_g_pr\n\n    def matched_death_transition(\n        self, current_state: dict, prop_state: dict, logp_pr_g_cr: float, logp_cr_g_pr: float, deletion_index: int\n    ) -&gt; Tuple[dict, float, float]:\n        \"\"\"Generate a proposal for coefficients associated with a death move, as the reverse of the birth proposal in self.matched_birth_transition().\n\n        See self.matched_birth_transition() for further details.\n\n        Args:\n            current_state (dict): current parameter state as dictionary.\n            prop_state (dict): proposed state dictionary, with updated basis matrix.\n            logp_pr_g_cr (float): transition probability for proposed state given current state.\n            logp_cr_g_pr (float): transition probability for current state given proposed state.\n            deletion_index (int): index of the basis element to be deleted\n\n        Returns:\n            prop_state (dict): proposed state with updated parameter vector.\n            logp_pr_g_cr (float): updated transition probability.\n            logp_cr_g_pr (float): updated transition probability.\n\n        \"\"\"\n        vector = self.matching_params[\"variable\"]\n        matrix = self.matching_params[\"matrix\"]\n        proposal_scale = self.matching_params[\"scale\"]\n        proposal_limits = self.matching_params[\"limits\"]\n\n        current_basis = current_state[matrix]\n        prop_basis = prop_state[matrix]\n        G = np.linalg.solve(\n            current_basis.T @ current_basis + 1e-10 * np.eye(current_basis.shape[1]), current_basis.T @ prop_basis\n        )\n        F = np.insert(G, obj=deletion_index, values=np.eye(N=G.shape[0], M=1, k=-deletion_index).flatten(), axis=1)\n        mu_aug = np.linalg.solve(F, current_state[vector])\n        param_del = mu_aug[deletion_index]\n        prop_state[vector] = np.delete(mu_aug, obj=deletion_index, axis=0)\n\n        logp_pr_g_cr += np.log(np.linalg.det(F))\n        if proposal_limits is not None:\n            logp_cr_g_pr += gmrf.truncated_normal_log_pdf(\n                param_del, np.array(0), proposal_scale, lower=proposal_limits[0], upper=proposal_limits[1]\n            )\n        else:\n            logp_cr_g_pr += gmrf.multivariate_normal_pdf(\n                x=param_del, mu=np.array(0.0, ndmin=2), Q=np.array(1 / (proposal_scale**2), ndmin=2)\n            )\n\n        return prop_state, logp_pr_g_cr, logp_cr_g_pr\n\n    def get_move_type(self, current_state: dict) -&gt; bool:\n        \"\"\"Select the type of move (birth or death) to be made at the current iteration.\n\n        Logic for the choice of move is as follows:\n            - if state[self.param]=self.n_max, it is not possible to increase self.param, so a death move is chosen.\n            - if state[self.param]=1, it is not possible to decrease self.param, so a birth move is chosen.\n            - in any other state, a birth move is chosen with probability self.birth_probability, or a death move is\n                chosen with probability (1 - self.birth_probability).\n\n        Args:\n            current_state (dict): dictionary with current parameter values.\n\n        Returns:\n            (bool): if True, make a birth proposal; if False, make a death proposal.\n\n        \"\"\"\n        if current_state[self.param] == self.n_max:\n            return False\n        if current_state[self.param] == 1:\n            return True\n\n        return uniform.rvs() &lt;= self.birth_probability\n\n    def get_move_probabilities(self, current_state: dict, birth: bool) -&gt; Tuple[float, float]:\n        \"\"\"Get the state-dependent probabilities of the forward and reverse moves, accounting for edge cases.\n\n        Returns a tuple of (p_birth, p_death), where these should be interpreted as follows:\n            Birth move: p_birth = probability of birth from CURRENT state.\n                        p_death = probability of death from PROPOSED state.\n            Death move: p_death = probability of death in CURRENT state.\n                        p_birth = probability of birth in PROPOSED state.\n\n        In standard cases (away from the limits, assumed to be at [1, n_max]):\n            p_birth = q; p_death = 1 - q\n\n        In edge cases (either where we are at one of the limits, or where our chosen move takes us into a limiting\n        case), we adjust the probability of either the forward or the reverse move to account for this. E.g.: if n=2,\n        q=0.5 and a death is proposed (i.e. proposed value n*=1), then p_death=0.5 (equal probabilities of birth/death\n        in CURRENT state), and p_birth=1 (because death is not possible in PROPOSED state).\n\n        Args:\n            current_state (dict): dictionary with current parameter values.\n            birth (bool): indicator for birth or death move.\n\n        Returns:\n            p_birth (float): state-dependent probability of birth move.\n            p_death (float): state-dependent probability of death move.\n\n        \"\"\"\n        p_birth = self.birth_probability\n        p_death = 1.0 - self.birth_probability\n\n        if current_state[self.param] == self.n_max:\n            p_death = 1.0\n        if current_state[self.param] == (self.n_max - 1) and birth:\n            p_death = 1.0\n\n        if current_state[self.param] == 1:\n            p_birth = 1.0\n        if current_state[self.param] == 2 and not birth:\n            p_birth = 1.0\n        return p_birth, p_death\n</code></pre>"},{"location":"openmcmc/sampler/reversible_jump/#openmcmc.sampler.reversible_jump.ReversibleJump.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Empty function to prevent super.post_init from being run.</p> <p>The whole model should be attached in this instance, rather than simply those elements with a dependence on self.param.</p> Source code in <code>src/openmcmc/sampler/reversible_jump.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Empty function to prevent super.__post_init__ from being run.\n\n    The whole model should be attached in this instance, rather than simply those elements with a dependence on\n    self.param.\n\n    \"\"\"\n    if isinstance(self.associated_params, str):\n        self.associated_params = [self.associated_params]\n</code></pre>"},{"location":"openmcmc/sampler/reversible_jump/#openmcmc.sampler.reversible_jump.ReversibleJump.proposal","title":"<code>proposal(current_state, param_index=None)</code>","text":"<p>Make a proposal, and compute related transition probabilities for the move.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary with current parameter values.</p> required <code>param_index</code> <code>int</code> <p>not used, included for compatibility with superclass.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>prop_state</code> <code>dict</code> <p>dictionary updated with proposed value for self.param.</p> <code>logp_pr_g_cr</code> <code>float</code> <p>transition probability for proposed state given current state.</p> <code>logp_cr_g_pr</code> <code>float</code> <p>transition probability for current state given proposed state.</p> Source code in <code>src/openmcmc/sampler/reversible_jump.py</code> <pre><code>def proposal(self, current_state: dict, param_index: int = None) -&gt; Tuple[dict, float, float]:\n    \"\"\"Make a proposal, and compute related transition probabilities for the move.\n\n    Args:\n        current_state (dict): dictionary with current parameter values.\n        param_index (int): not used, included for compatibility with superclass.\n\n    Returns:\n        prop_state (dict): dictionary updated with proposed value for self.param.\n        logp_pr_g_cr (float): transition probability for proposed state given current state.\n        logp_cr_g_pr (float): transition probability for current state given proposed state.\n\n    \"\"\"\n    birth = self.get_move_type(current_state)\n    if birth:\n        prop_state, logp_pr_g_cr, logp_cr_g_pr = self.birth_proposal(current_state=current_state)\n    else:\n        prop_state, logp_pr_g_cr, logp_cr_g_pr = self.death_proposal(current_state=current_state)\n    return prop_state, logp_pr_g_cr, logp_cr_g_pr\n</code></pre>"},{"location":"openmcmc/sampler/reversible_jump/#openmcmc.sampler.reversible_jump.ReversibleJump.birth_proposal","title":"<code>birth_proposal(current_state)</code>","text":"<p>Make a birth proposal move: INCREASES state[self.param] by 1.</p> <p>Also makes a proposal for a new element of an associated parameter, state[self.associated_params], by generating a draw from the prior distribution for self.associated_params.</p> <p>self.state_birth_function() is a function which can be optionally specified for altering the dimensionality of any other parameters associated with the dimension change (e.g. a basis matrix, or an allocation parameter).</p> <p>If the self.matching_params dictionary is specified, self.matched_birth_transition() is used to generate a proposal for a set of basis parameters such that the predicted values match before and after the transition.</p> <p>NOTE: log-probability for deletion of a particular knot (-log(n + 1)) is cancelled by the contribution from the order statistics densities, log((n + 1)! / n!) = log(n + 1). Therefore, both contributions are omitted from the calculation. For further information, see Richardson &amp; Green 1997, Section 3.2: https://people.maths.bris.ac.uk/~mapjg/papers/RichardsonGreenRSSB.pdf</p> <p>NOTE: log-probability density for the full model is obtained from summing the contribution of the log-density for the individual distributions corresponding to each jump parameter.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary with current parameter values.</p> required <p>Returns:</p> Name Type Description <code>prop_state</code> <code>dict</code> <p>dictionary updated with proposed state.</p> <code>logp_pr_g_cr</code> <code>float</code> <p>transition probability for proposed state given current state.</p> <code>logp_cr_g_pr</code> <code>float</code> <p>transition probability for current state given proposed state.</p> Source code in <code>src/openmcmc/sampler/reversible_jump.py</code> <pre><code>def birth_proposal(self, current_state: dict) -&gt; Tuple[dict, float, float]:\n    \"\"\"Make a birth proposal move: INCREASES state[self.param] by 1.\n\n    Also makes a proposal for a new element of an associated parameter, state[self.associated_params], by generating a draw\n    from the prior distribution for self.associated_params.\n\n    self.state_birth_function() is a function which can be optionally specified for altering the dimensionality of\n    any other parameters associated with the dimension change (e.g. a basis matrix, or an allocation parameter).\n\n    If the self.matching_params dictionary is specified, self.matched_birth_transition() is used to generate a\n    proposal for a set of basis parameters such that the predicted values match before and after the transition.\n\n    NOTE: log-probability for deletion of a particular knot (-log(n + 1)) is cancelled by the contribution from\n    the order statistics densities, log((n + 1)! / n!) = log(n + 1). Therefore, both contributions are omitted from\n    the calculation. For further information, see Richardson &amp; Green 1997, Section 3.2:\n    https://people.maths.bris.ac.uk/~mapjg/papers/RichardsonGreenRSSB.pdf\n\n    NOTE: log-probability density for the full model is obtained from summing the contribution of the log-density\n    for the individual distributions corresponding to each jump parameter.\n\n    Args:\n        current_state (dict): dictionary with current parameter values.\n\n    Returns:\n        prop_state (dict): dictionary updated with proposed state.\n        logp_pr_g_cr (float): transition probability for proposed state given current state.\n        logp_cr_g_pr (float): transition probability for current state given proposed state.\n\n    \"\"\"\n    prop_state = deepcopy(current_state)\n    prop_state[self.param] = prop_state[self.param] + 1\n    log_prop_density = 0\n\n    for associated_key in self.associated_params:\n        new_element = self.model[associated_key].rvs(state=current_state, n=1)\n        prop_state[associated_key] = np.concatenate((prop_state[associated_key], new_element), axis=1)\n        log_prop_density += self.model[associated_key].log_p(current_state, by_observation=True)\n    if callable(self.state_birth_function):\n        prop_state, logp_pr_g_cr, logp_cr_g_pr = self.state_birth_function(current_state, prop_state)\n    else:\n        logp_pr_g_cr, logp_cr_g_pr = 0.0, 0.0\n    if self.matching_params is not None:\n        prop_state, logp_pr_g_cr, logp_cr_g_pr = self.matched_birth_transition(\n            current_state, prop_state, logp_pr_g_cr, logp_cr_g_pr\n        )\n\n    p_birth, p_death = self.get_move_probabilities(current_state, True)\n    logp_pr_g_cr += np.log(p_birth) + log_prop_density[-1]\n    logp_cr_g_pr += np.log(p_death)\n\n    return prop_state, logp_pr_g_cr, logp_cr_g_pr\n</code></pre>"},{"location":"openmcmc/sampler/reversible_jump/#openmcmc.sampler.reversible_jump.ReversibleJump.death_proposal","title":"<code>death_proposal(current_state)</code>","text":"<p>Make a death proposal move: DECREASES state[self.param] by 1.</p> <p>Also adjusts the associated parameter state[self.associated_params] by deleting a randomly-selected element.</p> <p>self.state_death_function() and self.matched_death_transition() can be used (optional) to specify transitions opposite to those used in the birth move.</p> <p>NOTE: log-probability density for the full model is obtained from summing the contribution of the log-density for the individual distributions corresponding to each jump parameter.</p> <p>For further information about the transition, see also self.birth_proposal().</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary with current parameter values.</p> required <p>Returns:</p> Name Type Description <code>prop_state</code> <code>dict</code> <p>dictionary updated with proposed state.</p> <code>logp_pr_g_cr</code> <code>float</code> <p>transition probability for proposed state given current state.</p> <code>logp_cr_g_pr</code> <code>float</code> <p>transition probability for current state given proposed state.</p> Source code in <code>src/openmcmc/sampler/reversible_jump.py</code> <pre><code>def death_proposal(self, current_state: dict) -&gt; Tuple[dict, float, float]:\n    \"\"\"Make a death proposal move: DECREASES state[self.param] by 1.\n\n    Also adjusts the associated parameter state[self.associated_params] by deleting a randomly-selected element.\n\n    self.state_death_function() and self.matched_death_transition() can be used (optional) to specify transitions\n    opposite to those used in the birth move.\n\n    NOTE: log-probability density for the full model is obtained from summing the contribution of the log-density\n    for the individual distributions corresponding to each jump parameter.\n\n    For further information about the transition, see also self.birth_proposal().\n\n    Args:\n        current_state (dict): dictionary with current parameter values.\n\n    Returns:\n        prop_state (dict): dictionary updated with proposed state.\n        logp_pr_g_cr (float): transition probability for proposed state given current state.\n        logp_cr_g_pr (float): transition probability for current state given proposed state.\n\n    \"\"\"\n    prop_state = deepcopy(current_state)\n    prop_state[self.param] = prop_state[self.param] - 1\n    log_prop_density = 0\n    deletion_index = randint.rvs(low=0, high=current_state[self.param])\n    for associated_key in self.associated_params:\n        prop_state[associated_key] = np.delete(prop_state[associated_key], obj=deletion_index, axis=1)\n        log_prop_density += self.model[associated_key].log_p(current_state, by_observation=True)\n\n    if callable(self.state_death_function):\n        prop_state, logp_pr_g_cr, logp_cr_g_pr = self.state_death_function(\n            current_state, prop_state, deletion_index\n        )\n    else:\n        logp_pr_g_cr, logp_cr_g_pr = 0.0, 0.0\n    if self.matching_params is not None:\n        prop_state, logp_pr_g_cr, logp_cr_g_pr = self.matched_death_transition(\n            current_state, prop_state, logp_pr_g_cr, logp_cr_g_pr, deletion_index\n        )\n\n    p_birth, p_death = self.get_move_probabilities(current_state, False)\n    logp_pr_g_cr += np.log(p_death)\n    logp_cr_g_pr += np.log(p_birth) + log_prop_density[-1]\n\n    return prop_state, logp_pr_g_cr, logp_cr_g_pr\n</code></pre>"},{"location":"openmcmc/sampler/reversible_jump/#openmcmc.sampler.reversible_jump.ReversibleJump.matched_birth_transition","title":"<code>matched_birth_transition(current_state, prop_state, logp_pr_g_cr, logp_cr_g_pr)</code>","text":"<p>Generate a proposal for coefficients associated with a birth move, using the principle of matching the predictions before and after the move.</p> <p>The parameter vector in the proposed state is computed as: beta = F @ beta_aug, where: F = [G, 0      0', 1] G = (X' @ X)^{-1} @ (X' @ X) where X is the original basis matrix, and X* is the augmented basis matrix. For a detailed explanation of the approach, see: https://ygraigarw.github.io/ZnnEA1D19.pdf</p> <p>The basis matrix in the proposed state should already have been updated in self.state_birth_function(), before the call to this function (along with any other associated parameters that need to change shape).</p> <p>The following fields should be supplied as part of the self.matching_params dictionary:     - \"variable\" (str): reference to the coefficient parameter vector in the state.     - \"matrix\" (str): reference to the associated basis matrix in state.     - \"scale\" (float): scale of Gaussian noise added to proposal.     - \"limits\" (list): [lower, upper] limit for truncated Normal proposals.</p> The proposal for the additional basis parameter can be either from <ul> <li>a standard normal distribution (when self.matching_params[\"limits\"] is passed as None).</li> <li>a truncated normal distribution (when self.matching_params[\"limits\"] is a two-element list of the lower     and upper limits).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>current parameter state as dictionary.</p> required <code>prop_state</code> <code>dict</code> <p>proposed state dictionary, with updated basis matrix.</p> required <code>logp_pr_g_cr</code> <code>float</code> <p>transition probability for proposed state given current state.</p> required <code>logp_cr_g_pr</code> <code>float</code> <p>transition probability for current state given proposed state.</p> required <p>Returns:</p> Name Type Description <code>prop_state</code> <code>dict</code> <p>proposed state with updated parameter vector.</p> <code>logp_pr_g_cr</code> <code>float</code> <p>updated transition probability.</p> <code>logp_cr_g_pr</code> <code>float</code> <p>updated transition probability.</p> Source code in <code>src/openmcmc/sampler/reversible_jump.py</code> <pre><code>def matched_birth_transition(\n    self, current_state: dict, prop_state: dict, logp_pr_g_cr: float, logp_cr_g_pr: float\n) -&gt; Tuple[dict, float, float]:\n    \"\"\"Generate a proposal for coefficients associated with a birth move, using the principle of matching the predictions before and after the move.\n\n    The parameter vector in the proposed state is computed as: beta* = F @ beta_aug, where:\n    F = [G, 0\n         0', 1]\n    G = (X*' @ X*)^{-1} @ (X*' @ X)\n    where X is the original basis matrix, and X* is the augmented basis matrix. For a detailed explanation of the\n    approach, see: https://ygraigarw.github.io/ZnnEA1D19.pdf\n\n    The basis matrix in the proposed state should already have been updated in self.state_birth_function(), before\n    the call to this function (along with any other associated parameters that need to change shape).\n\n    The following fields should be supplied as part of the self.matching_params dictionary:\n        - \"variable\" (str): reference to the coefficient parameter vector in the state.\n        - \"matrix\" (str): reference to the associated basis matrix in state.\n        - \"scale\" (float): scale of Gaussian noise added to proposal.\n        - \"limits\" (list): [lower, upper] limit for truncated Normal proposals.\n\n    The proposal for the additional basis parameter can be either from:\n        - a standard normal distribution (when self.matching_params[\"limits\"] is passed as None).\n        - a truncated normal distribution (when self.matching_params[\"limits\"] is a two-element list of the lower\n            and upper limits).\n\n    Args:\n        current_state (dict): current parameter state as dictionary.\n        prop_state (dict): proposed state dictionary, with updated basis matrix.\n        logp_pr_g_cr (float): transition probability for proposed state given current state.\n        logp_cr_g_pr (float): transition probability for current state given proposed state.\n\n    Returns:\n        prop_state (dict): proposed state with updated parameter vector.\n        logp_pr_g_cr (float): updated transition probability.\n        logp_cr_g_pr (float): updated transition probability.\n\n    \"\"\"\n    vector = self.matching_params[\"variable\"]\n    matrix = self.matching_params[\"matrix\"]\n    proposal_scale = self.matching_params[\"scale\"]\n    proposal_limits = self.matching_params[\"limits\"]\n\n    current_basis = current_state[matrix]\n    prop_basis = prop_state[matrix]\n    G = np.linalg.solve(\n        prop_basis.T @ prop_basis + 1e-10 * np.eye(prop_basis.shape[1]), prop_basis.T @ current_basis\n    )\n    F = np.concatenate((G, np.eye(N=G.shape[0], M=1, k=-G.shape[0] + 1)), axis=1)\n    mu_star = G @ current_state[vector]\n    prop_state[vector] = deepcopy(mu_star)\n\n    if proposal_limits is not None:\n        prop_state[vector][-1] = gmrf.truncated_normal_rv(\n            mean=mu_star[-1], scale=proposal_scale, lower=proposal_limits[0], upper=proposal_limits[1], size=1\n        )\n        logp_pr_g_cr += gmrf.truncated_normal_log_pdf(\n            prop_state[vector][-1], mu_star[-1], proposal_scale, lower=proposal_limits[0], upper=proposal_limits[1]\n        )\n    else:\n        Q = np.array(1 / (proposal_scale**2), ndmin=2)\n        prop_state[vector][-1] = gmrf.sample_normal(mu=mu_star[-1], Q=Q, n=1)\n        logp_pr_g_cr += gmrf.multivariate_normal_pdf(x=prop_state[vector][-1], mu=mu_star[-1], Q=Q)\n\n    logp_cr_g_pr += np.log(np.linalg.det(F))\n\n    return prop_state, logp_pr_g_cr, logp_cr_g_pr\n</code></pre>"},{"location":"openmcmc/sampler/reversible_jump/#openmcmc.sampler.reversible_jump.ReversibleJump.matched_death_transition","title":"<code>matched_death_transition(current_state, prop_state, logp_pr_g_cr, logp_cr_g_pr, deletion_index)</code>","text":"<p>Generate a proposal for coefficients associated with a death move, as the reverse of the birth proposal in self.matched_birth_transition().</p> <p>See self.matched_birth_transition() for further details.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>current parameter state as dictionary.</p> required <code>prop_state</code> <code>dict</code> <p>proposed state dictionary, with updated basis matrix.</p> required <code>logp_pr_g_cr</code> <code>float</code> <p>transition probability for proposed state given current state.</p> required <code>logp_cr_g_pr</code> <code>float</code> <p>transition probability for current state given proposed state.</p> required <code>deletion_index</code> <code>int</code> <p>index of the basis element to be deleted</p> required <p>Returns:</p> Name Type Description <code>prop_state</code> <code>dict</code> <p>proposed state with updated parameter vector.</p> <code>logp_pr_g_cr</code> <code>float</code> <p>updated transition probability.</p> <code>logp_cr_g_pr</code> <code>float</code> <p>updated transition probability.</p> Source code in <code>src/openmcmc/sampler/reversible_jump.py</code> <pre><code>def matched_death_transition(\n    self, current_state: dict, prop_state: dict, logp_pr_g_cr: float, logp_cr_g_pr: float, deletion_index: int\n) -&gt; Tuple[dict, float, float]:\n    \"\"\"Generate a proposal for coefficients associated with a death move, as the reverse of the birth proposal in self.matched_birth_transition().\n\n    See self.matched_birth_transition() for further details.\n\n    Args:\n        current_state (dict): current parameter state as dictionary.\n        prop_state (dict): proposed state dictionary, with updated basis matrix.\n        logp_pr_g_cr (float): transition probability for proposed state given current state.\n        logp_cr_g_pr (float): transition probability for current state given proposed state.\n        deletion_index (int): index of the basis element to be deleted\n\n    Returns:\n        prop_state (dict): proposed state with updated parameter vector.\n        logp_pr_g_cr (float): updated transition probability.\n        logp_cr_g_pr (float): updated transition probability.\n\n    \"\"\"\n    vector = self.matching_params[\"variable\"]\n    matrix = self.matching_params[\"matrix\"]\n    proposal_scale = self.matching_params[\"scale\"]\n    proposal_limits = self.matching_params[\"limits\"]\n\n    current_basis = current_state[matrix]\n    prop_basis = prop_state[matrix]\n    G = np.linalg.solve(\n        current_basis.T @ current_basis + 1e-10 * np.eye(current_basis.shape[1]), current_basis.T @ prop_basis\n    )\n    F = np.insert(G, obj=deletion_index, values=np.eye(N=G.shape[0], M=1, k=-deletion_index).flatten(), axis=1)\n    mu_aug = np.linalg.solve(F, current_state[vector])\n    param_del = mu_aug[deletion_index]\n    prop_state[vector] = np.delete(mu_aug, obj=deletion_index, axis=0)\n\n    logp_pr_g_cr += np.log(np.linalg.det(F))\n    if proposal_limits is not None:\n        logp_cr_g_pr += gmrf.truncated_normal_log_pdf(\n            param_del, np.array(0), proposal_scale, lower=proposal_limits[0], upper=proposal_limits[1]\n        )\n    else:\n        logp_cr_g_pr += gmrf.multivariate_normal_pdf(\n            x=param_del, mu=np.array(0.0, ndmin=2), Q=np.array(1 / (proposal_scale**2), ndmin=2)\n        )\n\n    return prop_state, logp_pr_g_cr, logp_cr_g_pr\n</code></pre>"},{"location":"openmcmc/sampler/reversible_jump/#openmcmc.sampler.reversible_jump.ReversibleJump.get_move_type","title":"<code>get_move_type(current_state)</code>","text":"<p>Select the type of move (birth or death) to be made at the current iteration.</p> Logic for the choice of move is as follows <ul> <li>if state[self.param]=self.n_max, it is not possible to increase self.param, so a death move is chosen.</li> <li>if state[self.param]=1, it is not possible to decrease self.param, so a birth move is chosen.</li> <li>in any other state, a birth move is chosen with probability self.birth_probability, or a death move is     chosen with probability (1 - self.birth_probability).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary with current parameter values.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>if True, make a birth proposal; if False, make a death proposal.</p> Source code in <code>src/openmcmc/sampler/reversible_jump.py</code> <pre><code>def get_move_type(self, current_state: dict) -&gt; bool:\n    \"\"\"Select the type of move (birth or death) to be made at the current iteration.\n\n    Logic for the choice of move is as follows:\n        - if state[self.param]=self.n_max, it is not possible to increase self.param, so a death move is chosen.\n        - if state[self.param]=1, it is not possible to decrease self.param, so a birth move is chosen.\n        - in any other state, a birth move is chosen with probability self.birth_probability, or a death move is\n            chosen with probability (1 - self.birth_probability).\n\n    Args:\n        current_state (dict): dictionary with current parameter values.\n\n    Returns:\n        (bool): if True, make a birth proposal; if False, make a death proposal.\n\n    \"\"\"\n    if current_state[self.param] == self.n_max:\n        return False\n    if current_state[self.param] == 1:\n        return True\n\n    return uniform.rvs() &lt;= self.birth_probability\n</code></pre>"},{"location":"openmcmc/sampler/reversible_jump/#openmcmc.sampler.reversible_jump.ReversibleJump.get_move_probabilities","title":"<code>get_move_probabilities(current_state, birth)</code>","text":"<p>Get the state-dependent probabilities of the forward and reverse moves, accounting for edge cases.</p> <p>Returns a tuple of (p_birth, p_death), where these should be interpreted as follows:     Birth move: p_birth = probability of birth from CURRENT state.                 p_death = probability of death from PROPOSED state.     Death move: p_death = probability of death in CURRENT state.                 p_birth = probability of birth in PROPOSED state.</p> <p>In standard cases (away from the limits, assumed to be at [1, n_max]):     p_birth = q; p_death = 1 - q</p> <p>In edge cases (either where we are at one of the limits, or where our chosen move takes us into a limiting case), we adjust the probability of either the forward or the reverse move to account for this. E.g.: if n=2, q=0.5 and a death is proposed (i.e. proposed value n*=1), then p_death=0.5 (equal probabilities of birth/death in CURRENT state), and p_birth=1 (because death is not possible in PROPOSED state).</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary with current parameter values.</p> required <code>birth</code> <code>bool</code> <p>indicator for birth or death move.</p> required <p>Returns:</p> Name Type Description <code>p_birth</code> <code>float</code> <p>state-dependent probability of birth move.</p> <code>p_death</code> <code>float</code> <p>state-dependent probability of death move.</p> Source code in <code>src/openmcmc/sampler/reversible_jump.py</code> <pre><code>def get_move_probabilities(self, current_state: dict, birth: bool) -&gt; Tuple[float, float]:\n    \"\"\"Get the state-dependent probabilities of the forward and reverse moves, accounting for edge cases.\n\n    Returns a tuple of (p_birth, p_death), where these should be interpreted as follows:\n        Birth move: p_birth = probability of birth from CURRENT state.\n                    p_death = probability of death from PROPOSED state.\n        Death move: p_death = probability of death in CURRENT state.\n                    p_birth = probability of birth in PROPOSED state.\n\n    In standard cases (away from the limits, assumed to be at [1, n_max]):\n        p_birth = q; p_death = 1 - q\n\n    In edge cases (either where we are at one of the limits, or where our chosen move takes us into a limiting\n    case), we adjust the probability of either the forward or the reverse move to account for this. E.g.: if n=2,\n    q=0.5 and a death is proposed (i.e. proposed value n*=1), then p_death=0.5 (equal probabilities of birth/death\n    in CURRENT state), and p_birth=1 (because death is not possible in PROPOSED state).\n\n    Args:\n        current_state (dict): dictionary with current parameter values.\n        birth (bool): indicator for birth or death move.\n\n    Returns:\n        p_birth (float): state-dependent probability of birth move.\n        p_death (float): state-dependent probability of death move.\n\n    \"\"\"\n    p_birth = self.birth_probability\n    p_death = 1.0 - self.birth_probability\n\n    if current_state[self.param] == self.n_max:\n        p_death = 1.0\n    if current_state[self.param] == (self.n_max - 1) and birth:\n        p_death = 1.0\n\n    if current_state[self.param] == 1:\n        p_birth = 1.0\n    if current_state[self.param] == 2 and not birth:\n        p_birth = 1.0\n    return p_birth, p_death\n</code></pre>"},{"location":"openmcmc/sampler/sampler/","title":"Sampler","text":""},{"location":"openmcmc/sampler/sampler/#sampler","title":"Sampler","text":"<p>Collection of functions defining various MCMC samplers.</p> <p>MCMCSampler is a superclass for all MCMC sampler types. This file contains several conjugate MCMC sampling algorithms, which can be used for specific distribution combinations, and inherit directly from MCMCSampler.</p> <p>metropolis_hastings.py contains another set of algorithms, all of Metropolis-Hastings type, where a parameter proposal followed by an accept/reject step.</p> <p>reversible_jump.py contains a generic implementation of the reversible jump algorithm.</p>"},{"location":"openmcmc/sampler/sampler/#openmcmc.sampler.sampler.MCMCSampler","title":"<code>MCMCSampler</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for openMCMC sampling algorithms for a model parameter.</p> <p>Attributes:</p> Name Type Description <code>param</code> <code>str</code> <p>label of the parameter to be sampled.</p> <code>model</code> <code>Model</code> <p>sub-model of overall model, containing only distributions with some dependence on self.param.</p> <code>max_variable_size</code> <code>Union[int, tuple]</code> <p>(if required) maximum size for the variable. Only relevant in cases (e.g. RJMCMC) where variable dimension changes as a result of MCMC proposals.</p> Source code in <code>src/openmcmc/sampler/sampler.py</code> <pre><code>@dataclass\nclass MCMCSampler(ABC):\n    \"\"\"Abstract base class for openMCMC sampling algorithms for a model parameter.\n\n    Attributes:\n        param (str): label of the parameter to be sampled.\n        model (Model): sub-model of overall model, containing only distributions with some dependence on self.param.\n        max_variable_size (Union[int, tuple]): (if required) maximum size for the variable. Only relevant in cases\n            (e.g. RJMCMC) where variable dimension changes as a result of MCMC proposals.\n\n    \"\"\"\n\n    param: str\n    model: Model\n    max_variable_size: Union[int, tuple, None] = None\n\n    def __post_init__(self):\n        \"\"\"Extract the sub-model of distributions with some dependence on self.param.\"\"\"\n        self.model = self.model.conditional(self.param)\n\n    @abstractmethod\n    def sample(self, current_state: dict) -&gt; dict:\n        \"\"\"Generate the next sample in the chain.\n\n        Args:\n            current_state (dict): dictionary containing current parameter values.\n\n        Returns:\n            (dict): state with the value of self.param updated to a new sample.\n\n        \"\"\"\n\n    def init_store(self, current_state: dict, store: dict, n_iterations: int) -&gt; dict:\n        \"\"\"Initialise the field in the MCMC storage dictionary for self.param.\n\n        Args:\n            current_state (dict): dictionary containing current parameter values.\n            store (dict): dictionary to store all samples generated by the MCMC algorithm.\n            n_iterations (int): total number of MCMC iterations to be run.\n\n        Returns:\n            (dict): storage dictionary updated with field for self.param.\n\n        \"\"\"\n        if self.max_variable_size is None:\n            store[self.param] = np.full(shape=(np.size(current_state[self.param]), n_iterations), fill_value=np.nan)\n        elif isinstance(self.max_variable_size, tuple):\n            store[self.param] = np.full(shape=self.max_variable_size + (n_iterations,), fill_value=np.nan)\n        else:\n            store[self.param] = np.full(shape=(self.max_variable_size, n_iterations), fill_value=np.nan)\n        return store\n\n    def store(self, current_state: dict, store: dict, iteration: int) -&gt; dict:\n        \"\"\"Store the current state of the sampled variable in the MCMC storage dictionary.\n\n        If self.parameter is not initialised in the MCMC state, then the function generates a random sample from the\n        corresponding distribution in model.\n\n        Args:\n            current_state (dict): dictionary with current parameter values.\n            store (dict): storage dictionary for MCMC samples.\n            iteration (int): current MCMC iteration index.\n\n        Returns:\n            dict: storage dictionary updated with values from current iteration.\n\n        \"\"\"\n        current_param = current_state[self.param]\n\n        if self.max_variable_size is None:\n            store[self.param][:, [iteration]] = current_param\n        elif isinstance(self.max_variable_size, tuple):\n            index_list = []\n            for dim in range(current_param.ndim):\n                index_list.append(np.arange(current_param.shape[dim], dtype=int))\n            index_list.append(np.array([iteration]))\n\n            store[self.param][np.ix_(*index_list)] = current_param.reshape(current_param.shape + (1,))\n        else:\n            store[self.param][range(current_param.size), [iteration]] = current_param.flatten()\n\n        return store\n</code></pre>"},{"location":"openmcmc/sampler/sampler/#openmcmc.sampler.sampler.MCMCSampler.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Extract the sub-model of distributions with some dependence on self.param.</p> Source code in <code>src/openmcmc/sampler/sampler.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Extract the sub-model of distributions with some dependence on self.param.\"\"\"\n    self.model = self.model.conditional(self.param)\n</code></pre>"},{"location":"openmcmc/sampler/sampler/#openmcmc.sampler.sampler.MCMCSampler.sample","title":"<code>sample(current_state)</code>  <code>abstractmethod</code>","text":"<p>Generate the next sample in the chain.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary containing current parameter values.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>state with the value of self.param updated to a new sample.</p> Source code in <code>src/openmcmc/sampler/sampler.py</code> <pre><code>@abstractmethod\ndef sample(self, current_state: dict) -&gt; dict:\n    \"\"\"Generate the next sample in the chain.\n\n    Args:\n        current_state (dict): dictionary containing current parameter values.\n\n    Returns:\n        (dict): state with the value of self.param updated to a new sample.\n\n    \"\"\"\n</code></pre>"},{"location":"openmcmc/sampler/sampler/#openmcmc.sampler.sampler.MCMCSampler.init_store","title":"<code>init_store(current_state, store, n_iterations)</code>","text":"<p>Initialise the field in the MCMC storage dictionary for self.param.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary containing current parameter values.</p> required <code>store</code> <code>dict</code> <p>dictionary to store all samples generated by the MCMC algorithm.</p> required <code>n_iterations</code> <code>int</code> <p>total number of MCMC iterations to be run.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>storage dictionary updated with field for self.param.</p> Source code in <code>src/openmcmc/sampler/sampler.py</code> <pre><code>def init_store(self, current_state: dict, store: dict, n_iterations: int) -&gt; dict:\n    \"\"\"Initialise the field in the MCMC storage dictionary for self.param.\n\n    Args:\n        current_state (dict): dictionary containing current parameter values.\n        store (dict): dictionary to store all samples generated by the MCMC algorithm.\n        n_iterations (int): total number of MCMC iterations to be run.\n\n    Returns:\n        (dict): storage dictionary updated with field for self.param.\n\n    \"\"\"\n    if self.max_variable_size is None:\n        store[self.param] = np.full(shape=(np.size(current_state[self.param]), n_iterations), fill_value=np.nan)\n    elif isinstance(self.max_variable_size, tuple):\n        store[self.param] = np.full(shape=self.max_variable_size + (n_iterations,), fill_value=np.nan)\n    else:\n        store[self.param] = np.full(shape=(self.max_variable_size, n_iterations), fill_value=np.nan)\n    return store\n</code></pre>"},{"location":"openmcmc/sampler/sampler/#openmcmc.sampler.sampler.MCMCSampler.store","title":"<code>store(current_state, store, iteration)</code>","text":"<p>Store the current state of the sampled variable in the MCMC storage dictionary.</p> <p>If self.parameter is not initialised in the MCMC state, then the function generates a random sample from the corresponding distribution in model.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary with current parameter values.</p> required <code>store</code> <code>dict</code> <p>storage dictionary for MCMC samples.</p> required <code>iteration</code> <code>int</code> <p>current MCMC iteration index.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>storage dictionary updated with values from current iteration.</p> Source code in <code>src/openmcmc/sampler/sampler.py</code> <pre><code>def store(self, current_state: dict, store: dict, iteration: int) -&gt; dict:\n    \"\"\"Store the current state of the sampled variable in the MCMC storage dictionary.\n\n    If self.parameter is not initialised in the MCMC state, then the function generates a random sample from the\n    corresponding distribution in model.\n\n    Args:\n        current_state (dict): dictionary with current parameter values.\n        store (dict): storage dictionary for MCMC samples.\n        iteration (int): current MCMC iteration index.\n\n    Returns:\n        dict: storage dictionary updated with values from current iteration.\n\n    \"\"\"\n    current_param = current_state[self.param]\n\n    if self.max_variable_size is None:\n        store[self.param][:, [iteration]] = current_param\n    elif isinstance(self.max_variable_size, tuple):\n        index_list = []\n        for dim in range(current_param.ndim):\n            index_list.append(np.arange(current_param.shape[dim], dtype=int))\n        index_list.append(np.array([iteration]))\n\n        store[self.param][np.ix_(*index_list)] = current_param.reshape(current_param.shape + (1,))\n    else:\n        store[self.param][range(current_param.size), [iteration]] = current_param.flatten()\n\n    return store\n</code></pre>"},{"location":"openmcmc/sampler/sampler/#openmcmc.sampler.sampler.NormalNormal","title":"<code>NormalNormal</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MCMCSampler</code></p> <p>Normal-Normal conditional sampling (exploiting conjugacy).</p> <p>Sample from f(x|{b_k}) ~= [prod_k f(y_k|x)]f(x) where the components have the following form:     - Likelihoods: f(y_k|x) ~ N(y_k | d_k + A_k*a, W_k^{-1})     - Prior: f(x) ~ N(x |m, P^{-1}) The following features are assumed:     - There can be multiple likelihood/response distributions, but there is only one prior distribution.     - The mean of each of the response distributions must have a linear dependence on self.param.     - The prior Gaussian can be truncated (as long as the truncation points are constant), but the response         Gaussians cannot.</p> <p>If the prior Gaussian is truncated (i.e. it has specified domain limits), then those same domain limits will be used when generating the conditional sample.</p> <p>Attributes:</p> Name Type Description <code>_is_response</code> <code>dict</code> <p>dictionary containing boolean indicators for whether self.param is the response of the distribution. If self._is_response[key] is True, then self.param is the response of the distribution.</p> Source code in <code>src/openmcmc/sampler/sampler.py</code> <pre><code>@dataclass\nclass NormalNormal(MCMCSampler):\n    \"\"\"Normal-Normal conditional sampling (exploiting conjugacy).\n\n    Sample from f(x|{b_k}) ~= [prod_k f(y_k|x)]f(x) where the components have the following form:\n        - Likelihoods: f(y_k|x) ~ N(y_k | d_k + A_k*a, W_k^{-1})\n        - Prior: f(x) ~ N(x |m, P^{-1})\n    The following features are assumed:\n        - There can be multiple likelihood/response distributions, but there is only one prior distribution.\n        - The mean of each of the response distributions must have a linear dependence on self.param.\n        - The prior Gaussian can be truncated (as long as the truncation points are constant), but the response\n            Gaussians cannot.\n\n    If the prior Gaussian is truncated (i.e. it has specified domain limits), then those same domain limits will be\n    used when generating the conditional sample.\n\n    Attributes:\n        _is_response (dict): dictionary containing boolean indicators for whether self.param is the response of the\n            distribution. If self._is_response[key] is True, then self.param is the response of the distribution.\n\n    \"\"\"\n\n    def __post_init__(self):\n        \"\"\"Identify and extract the sub-model with a dependence on self.param.\n\n        Also identify whether self.param is the response for each of the pair of Gaussian distributions.\n\n        \"\"\"\n        super().__post_init__()\n        self._is_response = {}\n        for key in self.model.keys():\n            self._is_response[key] = key == self.param\n\n    def sample(self, current_state: dict) -&gt; dict:\n        \"\"\"Generate a sample from a Gaussian-Gaussian conditional distribution.\n\n        For a Gaussian-Gaussian conditional distribution, the parameters are as follows:\n            Conditional precision:\n                Q = P + sum_k [A_k'*W_k*A_k]\n            Conditional mean:\n                b = P*m + sum_k [A_k'*W_k*(y_k - d_k)]\n                mu = Q^{-1} * b\n        Where the parameters are as defined in the class docstring.\n\n        If the supplied response parameter has a second dimension, these are interpreted as repeated draws from the same\n        distribution, and are thus summed. The multiplication of the precision matrix by num_replicates is handled by\n        the grad_log_p() function of the corresponding distribution.\n\n        Args:\n            current_state (dict): dictionary containing the current sampler state.\n\n        Returns:\n            (dict): state with updated value for self.param.\n\n        \"\"\"\n        n_param = current_state[self.param].shape[0]\n        Q = sparse.csc_matrix((n_param, n_param))\n        b = np.zeros(shape=(n_param, 1))\n        for key, dist in self.model.items():\n            Q_rsp = dist.precision.predictor(current_state)\n            if self._is_response[key]:\n                Q += Q_rsp\n                b += Q_rsp @ dist.mean.predictor(current_state)\n            else:\n                _, Q_dist = dist.grad_log_p(current_state, self.param)\n                Q += Q_dist\n                if isinstance(dist.mean, Identity):\n                    b += Q_rsp @ np.sum(current_state[key], axis=1, keepdims=True)\n                else:\n                    predictor_exclude = dist.mean.predictor_conditional(current_state, term_to_exclude=self.param)\n                    A = current_state[dist.mean.form[self.param]]\n                    b += A.T @ Q_rsp @ (current_state[key] - predictor_exclude)\n\n        dist_param = self.model[self.param]\n\n        if dist_param.domain_response_lower is None and dist_param.domain_response_upper is None:\n            current_state[self.param] = gmrf.sample_normal_canonical(b, Q)\n        else:\n            current_state[self.param] = gmrf.gibbs_canonical_truncated_normal(\n                b,\n                Q,\n                x=current_state[self.param],\n                lower=dist_param.domain_response_lower,\n                upper=dist_param.domain_response_upper,\n            )\n\n        return current_state\n</code></pre>"},{"location":"openmcmc/sampler/sampler/#openmcmc.sampler.sampler.NormalNormal.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Identify and extract the sub-model with a dependence on self.param.</p> <p>Also identify whether self.param is the response for each of the pair of Gaussian distributions.</p> Source code in <code>src/openmcmc/sampler/sampler.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Identify and extract the sub-model with a dependence on self.param.\n\n    Also identify whether self.param is the response for each of the pair of Gaussian distributions.\n\n    \"\"\"\n    super().__post_init__()\n    self._is_response = {}\n    for key in self.model.keys():\n        self._is_response[key] = key == self.param\n</code></pre>"},{"location":"openmcmc/sampler/sampler/#openmcmc.sampler.sampler.NormalNormal.sample","title":"<code>sample(current_state)</code>","text":"<p>Generate a sample from a Gaussian-Gaussian conditional distribution.</p> <p>For a Gaussian-Gaussian conditional distribution, the parameters are as follows:     Conditional precision:         Q = P + sum_k [A_k'W_kA_k]     Conditional mean:         b = Pm + sum_k [A_k'W_k*(y_k - d_k)]         mu = Q^{-1} * b Where the parameters are as defined in the class docstring.</p> <p>If the supplied response parameter has a second dimension, these are interpreted as repeated draws from the same distribution, and are thus summed. The multiplication of the precision matrix by num_replicates is handled by the grad_log_p() function of the corresponding distribution.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary containing the current sampler state.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>state with updated value for self.param.</p> Source code in <code>src/openmcmc/sampler/sampler.py</code> <pre><code>def sample(self, current_state: dict) -&gt; dict:\n    \"\"\"Generate a sample from a Gaussian-Gaussian conditional distribution.\n\n    For a Gaussian-Gaussian conditional distribution, the parameters are as follows:\n        Conditional precision:\n            Q = P + sum_k [A_k'*W_k*A_k]\n        Conditional mean:\n            b = P*m + sum_k [A_k'*W_k*(y_k - d_k)]\n            mu = Q^{-1} * b\n    Where the parameters are as defined in the class docstring.\n\n    If the supplied response parameter has a second dimension, these are interpreted as repeated draws from the same\n    distribution, and are thus summed. The multiplication of the precision matrix by num_replicates is handled by\n    the grad_log_p() function of the corresponding distribution.\n\n    Args:\n        current_state (dict): dictionary containing the current sampler state.\n\n    Returns:\n        (dict): state with updated value for self.param.\n\n    \"\"\"\n    n_param = current_state[self.param].shape[0]\n    Q = sparse.csc_matrix((n_param, n_param))\n    b = np.zeros(shape=(n_param, 1))\n    for key, dist in self.model.items():\n        Q_rsp = dist.precision.predictor(current_state)\n        if self._is_response[key]:\n            Q += Q_rsp\n            b += Q_rsp @ dist.mean.predictor(current_state)\n        else:\n            _, Q_dist = dist.grad_log_p(current_state, self.param)\n            Q += Q_dist\n            if isinstance(dist.mean, Identity):\n                b += Q_rsp @ np.sum(current_state[key], axis=1, keepdims=True)\n            else:\n                predictor_exclude = dist.mean.predictor_conditional(current_state, term_to_exclude=self.param)\n                A = current_state[dist.mean.form[self.param]]\n                b += A.T @ Q_rsp @ (current_state[key] - predictor_exclude)\n\n    dist_param = self.model[self.param]\n\n    if dist_param.domain_response_lower is None and dist_param.domain_response_upper is None:\n        current_state[self.param] = gmrf.sample_normal_canonical(b, Q)\n    else:\n        current_state[self.param] = gmrf.gibbs_canonical_truncated_normal(\n            b,\n            Q,\n            x=current_state[self.param],\n            lower=dist_param.domain_response_lower,\n            upper=dist_param.domain_response_upper,\n        )\n\n    return current_state\n</code></pre>"},{"location":"openmcmc/sampler/sampler/#openmcmc.sampler.sampler.NormalGamma","title":"<code>NormalGamma</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MCMCSampler</code></p> <p>Normal-gamma conditional sampling (exploiting conjugacy).</p> <p>Assumes that self.param is the precision parameter for a Gaussian response distribution, and that it has a gamma prior distribution.</p> <p>Allows for the possibility that a single Gaussian response distribution might be associated with a number of different precision parameters, through use of a MixtureParameterMatrix precision parameters. These parameters are sampled in a loop within the sample function.</p> <p>This class samples from f(lam|y, a, b) ~ prod_k [f(y_k|lam)]f(lam|a,b) where     - Likelihoods: f(y_k|lam) ~ N(mu_k, 1/lam)     - Prior: f(lam|a, b) ~ G(a, b) Note that it is also possible to use a more complex (e.g. dense) precision matrix scaled by a single precision parameter, this does not fundamentally change the approach.</p> <p>Attributes:</p> Name Type Description <code>param</code> <code>str</code> <p>label of parameter name to be sampled</p> <code>normal_param</code> <code>str</code> <p>label of corresponding normal parameter</p> <code>model</code> <code>Model</code> <p>conditional model with distributions related to param only</p> Source code in <code>src/openmcmc/sampler/sampler.py</code> <pre><code>@dataclass\nclass NormalGamma(MCMCSampler):\n    \"\"\"Normal-gamma conditional sampling (exploiting conjugacy).\n\n    Assumes that self.param is the precision parameter for a Gaussian response distribution, and that it has a gamma\n    prior distribution.\n\n    Allows for the possibility that a single Gaussian response distribution might be associated with a number of\n    different precision parameters, through use of a MixtureParameterMatrix precision parameters. These parameters\n    are sampled in a loop within the sample function.\n\n    This class samples from f(lam|y, a, b) ~ prod_k [f(y_k|lam)]f(lam|a,b) where\n        - Likelihoods: f(y_k|lam) ~ N(mu_k, 1/lam)\n        - Prior: f(lam|a, b) ~ G(a, b)\n    Note that it is also possible to use a more complex (e.g. dense) precision matrix scaled by a single precision\n    parameter, this does not fundamentally change the approach.\n\n    Attributes:\n        param (str): label of parameter name to be sampled\n        normal_param (str): label of corresponding normal parameter\n        model (Model): conditional model with distributions related to param only\n\n    \"\"\"\n\n    def __post_init__(self):\n        \"\"\"Complete initialization of sampler.\n\n        Identifies the gamma distribution and its conjugate normal distribution, and attaches copies to the sampler\n        object.\n\n        \"\"\"\n        super().__post_init__()\n\n        nrm_prm = list(self.model.keys())\n        nrm_prm.remove(self.param)\n        self.normal_param = nrm_prm[0]\n\n        precision = self.model[self.normal_param].precision\n\n        if not isinstance(precision, (Identity, ScaledMatrix, MixtureParameterMatrix)):\n            raise TypeError(\"precision must be either Identity, ScaledMatrix or MixtureParameterMatrix\")\n\n    def sample(self, current_state: dict) -&gt; dict:\n        \"\"\"Generate a sample from a (series of) Gaussian-Gamma conditional distribution.\n\n        The conditional distribution for an individual parameter is:\n            G(a*, b*)\n        where:\n            a* = a_0 + n/2\n            b* = b_0 + (y - y_hat)' * P * (y - y_hat)\n        where n = [dimension of normal response], P = [un-scaled precision matrix].\n\n        It is assumed that the precision parameter of the Gaussian distribution has a predictor_unscaled() method,\n        which can be used to identify the subset of the full precision matrix which is dependent on the parameter being\n        sampled: this is returned un-scaled by the precision parameter.\n\n        Args:\n            current_state (dict): dictionary containing the current sampler state.\n\n        Returns:\n            (dict): state with updated value for self.param.\n\n        \"\"\"\n        precision = self.model[self.normal_param].precision\n        mean = self.model[self.normal_param].mean\n        y = current_state[self.model[self.normal_param].response]\n        residual = y - mean.predictor(current_state)\n\n        a = deepcopy(self.model[self.param].shape.predictor(current_state))\n        b = deepcopy(self.model[self.param].rate.predictor(current_state))\n\n        for k in range(current_state[self.param].shape[0]):\n            precision_unscaled = precision.precision_unscaled(current_state, k)\n            a[k] += np.sum(precision_unscaled.diagonal() &gt; 0) / 2\n            b[k] += (residual.T @ precision_unscaled @ residual).item() / 2\n        no_warning_b = np.where(b == 0, np.inf, b)\n        no_warning_scale = np.where(b == 0, np.inf, 1 / no_warning_b)\n        current_state[self.param] = gamma.rvs(a, scale=no_warning_scale).reshape(current_state[self.param].shape)\n        return current_state\n</code></pre>"},{"location":"openmcmc/sampler/sampler/#openmcmc.sampler.sampler.NormalGamma.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Complete initialization of sampler.</p> <p>Identifies the gamma distribution and its conjugate normal distribution, and attaches copies to the sampler object.</p> Source code in <code>src/openmcmc/sampler/sampler.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Complete initialization of sampler.\n\n    Identifies the gamma distribution and its conjugate normal distribution, and attaches copies to the sampler\n    object.\n\n    \"\"\"\n    super().__post_init__()\n\n    nrm_prm = list(self.model.keys())\n    nrm_prm.remove(self.param)\n    self.normal_param = nrm_prm[0]\n\n    precision = self.model[self.normal_param].precision\n\n    if not isinstance(precision, (Identity, ScaledMatrix, MixtureParameterMatrix)):\n        raise TypeError(\"precision must be either Identity, ScaledMatrix or MixtureParameterMatrix\")\n</code></pre>"},{"location":"openmcmc/sampler/sampler/#openmcmc.sampler.sampler.NormalGamma.sample","title":"<code>sample(current_state)</code>","text":"<p>Generate a sample from a (series of) Gaussian-Gamma conditional distribution.</p> The conditional distribution for an individual parameter is <p>G(a, b)</p> <p>where n = [dimension of normal response], P = [un-scaled precision matrix].</p> <p>It is assumed that the precision parameter of the Gaussian distribution has a predictor_unscaled() method, which can be used to identify the subset of the full precision matrix which is dependent on the parameter being sampled: this is returned un-scaled by the precision parameter.</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary containing the current sampler state.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>state with updated value for self.param.</p> Source code in <code>src/openmcmc/sampler/sampler.py</code> <pre><code>def sample(self, current_state: dict) -&gt; dict:\n    \"\"\"Generate a sample from a (series of) Gaussian-Gamma conditional distribution.\n\n    The conditional distribution for an individual parameter is:\n        G(a*, b*)\n    where:\n        a* = a_0 + n/2\n        b* = b_0 + (y - y_hat)' * P * (y - y_hat)\n    where n = [dimension of normal response], P = [un-scaled precision matrix].\n\n    It is assumed that the precision parameter of the Gaussian distribution has a predictor_unscaled() method,\n    which can be used to identify the subset of the full precision matrix which is dependent on the parameter being\n    sampled: this is returned un-scaled by the precision parameter.\n\n    Args:\n        current_state (dict): dictionary containing the current sampler state.\n\n    Returns:\n        (dict): state with updated value for self.param.\n\n    \"\"\"\n    precision = self.model[self.normal_param].precision\n    mean = self.model[self.normal_param].mean\n    y = current_state[self.model[self.normal_param].response]\n    residual = y - mean.predictor(current_state)\n\n    a = deepcopy(self.model[self.param].shape.predictor(current_state))\n    b = deepcopy(self.model[self.param].rate.predictor(current_state))\n\n    for k in range(current_state[self.param].shape[0]):\n        precision_unscaled = precision.precision_unscaled(current_state, k)\n        a[k] += np.sum(precision_unscaled.diagonal() &gt; 0) / 2\n        b[k] += (residual.T @ precision_unscaled @ residual).item() / 2\n    no_warning_b = np.where(b == 0, np.inf, b)\n    no_warning_scale = np.where(b == 0, np.inf, 1 / no_warning_b)\n    current_state[self.param] = gamma.rvs(a, scale=no_warning_scale).reshape(current_state[self.param].shape)\n    return current_state\n</code></pre>"},{"location":"openmcmc/sampler/sampler/#openmcmc.sampler.sampler.MixtureAllocation","title":"<code>MixtureAllocation</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MCMCSampler</code></p> <p>Conditional conjugate sampling of the allocation in a mixture distribution. Can be used with any kind of mixture distribution.</p> This class samples from <p>f(z|y, lam, tht) ~ f(y|z, lam)f(z|tht)</p> <p>Attributes:</p> Name Type Description <code>response_param</code> <code>str</code> <p>name of the response parameter associated with the allocation parameter.</p> Source code in <code>src/openmcmc/sampler/sampler.py</code> <pre><code>@dataclass\nclass MixtureAllocation(MCMCSampler):\n    \"\"\"Conditional conjugate sampling of the allocation in a mixture distribution. Can be used with any kind of mixture distribution.\n\n    This class samples from:\n        f(z|y, lam, tht) ~ f(y|z, lam)f(z|tht)\n    where:\n        - Likelihood: f(y|z, lam); Gaussian distribution, with different characteristics depending on z.\n        - Prior: f(z|tht); categorical distribution, with prior allocation probabilities tht.\n\n    Attributes:\n        response_param (str): name of the response parameter associated with the allocation parameter.\n\n    \"\"\"\n\n    response_param: Union[str, None] = None\n\n    def __post_init__(self):\n        \"\"\"Subset only model elements relevant for this sampler.\"\"\"\n        self.model = Model([self.model[self.param], self.model[self.response_param]])\n\n        if not isinstance(self.model[self.response_param], Normal):\n            raise TypeError(\"Mixture model currently only implemented for Normal case\")\n\n        if not isinstance(self.model[self.response_param].mean, MixtureParameterVector):\n            raise TypeError(\"Mean must be of type MixtureParameterVector\")\n\n        if not isinstance(self.model[self.response_param].precision, MixtureParameterMatrix):\n            raise TypeError(\"Mean must be of type MixtureParameterMatrix\")\n\n    def sample(self, current_state: dict) -&gt; dict:\n        \"\"\"Generate sample of a parameter allocation given current state of the sampler.\n\n        Computes the conditional allocation probability for each element of the response to each component of the\n        mixture, then samples an allocation based on the probabilities.\n\n        The conditional distribution is:\n            Cat([gam_1, gam_2,..., gam_m])\n        where:\n            gam_k = f(y|z_k, lam) * tht_k / W\n            W = sum_k [f(y|z_k, lam) * tht_k / Z]\n\n        Args:\n            current_state (dict): dictionary containing the current sampler state.\n\n        Returns:\n            (dict): state with updated value for self.param.\n\n        \"\"\"\n        allocation_prior = self.model[self.param].prob.predictor(current_state)\n        n_response = current_state[self.response_param].shape[0]\n        component_mean = current_state[self.model[self.response_param].mean.param]\n        component_precision = current_state[self.model[self.response_param].precision.param]\n\n        allocation_prob = np.empty((n_response, allocation_prior.shape[1]))\n        for k in range(allocation_prior.shape[1]):\n            allocation_prob[:, [k]] = allocation_prior[:, [k]] * norm.pdf(\n                current_state[self.response_param], loc=component_mean[k], scale=1 / np.sqrt(component_precision[k])\n            )\n\n        allocation_prob = allocation_prob / np.sum(allocation_prob, axis=1).reshape((allocation_prob.shape[0], 1))\n        U = np.random.rand(n_response, 1)\n        current_state[self.param] = np.atleast_2d(np.sum(U &gt; np.cumsum(allocation_prob, axis=1), axis=1)).T\n\n        return current_state\n</code></pre>"},{"location":"openmcmc/sampler/sampler/#openmcmc.sampler.sampler.MixtureAllocation.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Subset only model elements relevant for this sampler.</p> Source code in <code>src/openmcmc/sampler/sampler.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Subset only model elements relevant for this sampler.\"\"\"\n    self.model = Model([self.model[self.param], self.model[self.response_param]])\n\n    if not isinstance(self.model[self.response_param], Normal):\n        raise TypeError(\"Mixture model currently only implemented for Normal case\")\n\n    if not isinstance(self.model[self.response_param].mean, MixtureParameterVector):\n        raise TypeError(\"Mean must be of type MixtureParameterVector\")\n\n    if not isinstance(self.model[self.response_param].precision, MixtureParameterMatrix):\n        raise TypeError(\"Mean must be of type MixtureParameterMatrix\")\n</code></pre>"},{"location":"openmcmc/sampler/sampler/#openmcmc.sampler.sampler.MixtureAllocation.sample","title":"<code>sample(current_state)</code>","text":"<p>Generate sample of a parameter allocation given current state of the sampler.</p> <p>Computes the conditional allocation probability for each element of the response to each component of the mixture, then samples an allocation based on the probabilities.</p> The conditional distribution is <p>Cat([gam_1, gam_2,..., gam_m])</p> <p>Parameters:</p> Name Type Description Default <code>current_state</code> <code>dict</code> <p>dictionary containing the current sampler state.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>state with updated value for self.param.</p> Source code in <code>src/openmcmc/sampler/sampler.py</code> <pre><code>def sample(self, current_state: dict) -&gt; dict:\n    \"\"\"Generate sample of a parameter allocation given current state of the sampler.\n\n    Computes the conditional allocation probability for each element of the response to each component of the\n    mixture, then samples an allocation based on the probabilities.\n\n    The conditional distribution is:\n        Cat([gam_1, gam_2,..., gam_m])\n    where:\n        gam_k = f(y|z_k, lam) * tht_k / W\n        W = sum_k [f(y|z_k, lam) * tht_k / Z]\n\n    Args:\n        current_state (dict): dictionary containing the current sampler state.\n\n    Returns:\n        (dict): state with updated value for self.param.\n\n    \"\"\"\n    allocation_prior = self.model[self.param].prob.predictor(current_state)\n    n_response = current_state[self.response_param].shape[0]\n    component_mean = current_state[self.model[self.response_param].mean.param]\n    component_precision = current_state[self.model[self.response_param].precision.param]\n\n    allocation_prob = np.empty((n_response, allocation_prior.shape[1]))\n    for k in range(allocation_prior.shape[1]):\n        allocation_prob[:, [k]] = allocation_prior[:, [k]] * norm.pdf(\n            current_state[self.response_param], loc=component_mean[k], scale=1 / np.sqrt(component_precision[k])\n        )\n\n    allocation_prob = allocation_prob / np.sum(allocation_prob, axis=1).reshape((allocation_prob.shape[0], 1))\n    U = np.random.rand(n_response, 1)\n    current_state[self.param] = np.atleast_2d(np.sum(U &gt; np.cumsum(allocation_prob, axis=1), axis=1)).T\n\n    return current_state\n</code></pre>"}]}